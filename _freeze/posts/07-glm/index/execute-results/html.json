{
  "hash": "7b6a4f6fb91e083813760b21efa58c9c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tutorial on Generalized Linear Model from scratch\"\nsubtitle: \"Dogmatic statistics for fun series\"\ndescription: \"Finally understand what a link function actually does instead of just copy-pasting glm()\"\ndate: \"2025-11-23\"\nimage: \"image.png\"\ncategories: [R, Python, statistics, regression, machine-learning]\nformat:\n    html:\n        toc: true\n        toc-float: true\n        toc-depth: 3\n        number-sections: true\n        code-fold: false\n        code-tools: false\n        theme: default\n        highlight-style: tango\n        fig-width: 10\n        fig-height: 6\n        fig-cap-location: bottom\nexecute:\n    echo: true\n    message: false\n    warning: false\n    fig-align: center\n    fig-width: 10\n    fig-height: 6\nfilters: \n    - social-share\n    # - collapse-output\nshare:\n    permalink: \"https://joshuamarie.com/posts/07-glm\"\n    description: \"Tutorial on Generalized Linear Model - Joshua Marie\"\n    twitter: true\n    facebook: true\n    reddit: true\n    stumble: true\n    tumblr: true\n    linkedin: true\n    email: true\n    mastodon: true\n    bsky: true\n    location: \"before-body\"\nengine: knitr\n---\n\nStop asking:\n\n- \"What test do I need to use?\"\n- \"Do my data need to be normally distributed?\"\n- \"Can I just use linear regression on counts lol?\"\n\n![](spongebob-brain-on-fire.gif)\n\nKidding~\n\nI have no such problem of you guys asking these questions. This type of question, however, is a type of question that I heard many times already. I mean, I understand why there's so many misconceptions, and it could be they are not properly taught or...I don't know.\n\nI have no problems with tutorials and blogs online about teaching statistics, like predicting what species of iris is gonna be, or how likely you'll get ebola. There are ten thousand blog posts teaching you how to predict iris species with `glm()`, or Python's `sklearn.linear_model.LogisticRegression()`\n\nToday we’re going **full math gremlin mode** and actually understanding why GLMs exist, why linear regression is a lie for most real data, and why the link function is the most genius hack in 20th-century statistics.\n\n## Let's start with an overview\n\n![](lr.jpg)\n\nWhat did you observe on the mathematical model formula of linear regression and generalized linear model (GLM)? \n\nTake a look:\n\n***Linear Regression:***\n\n$$Y = \\mathbf{X}\\beta\\ +\\ \\epsilon$$\n\nwhere $\\mathbf{X}\\beta$ expands to:\n$$\\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_3\\beta_3 + \\dots + x_n\\beta_n$$\n\nHow about on GLM?\n\n$$\ng(\\mu_i) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta} = \\eta_i \\quad \\text{(linear predictor)}\n$$\n\n$$g(\\mu)=\\eta=\\mathbf{X}\\beta$$\n\nboth are taking the account of the linear model, where the response have linear relationship with the predictors, taken account by $\\mathbf{X}\\beta$. \n\nThere are few exceptions:\n\n1.  GLMs can take handle of the range not only lies on the real number, but with some strict intervals, unlike Linear regression who has always a range lies on the real number. \n\n2.  GLMs doesn't have an error term. Why is that? GLMs takes considerations on the *expected value* of the linear model, and when you take the expected value like this: \n\n    $$E(Y)=E(\\mathbf{X}\\beta\\ + \\ \\epsilon)$$\n\n    The expected values $E(\\mathbf{X}\\beta) = \\mathbf{X}\\beta$ (assuming no randomness in both terms here) and $E(\\epsilon) = 0$. Hence: \n\n    $$E(Y)=\\mathbf{X}\\beta$$\n\n    and GLMs only care about the expected value\n    \n\n### But wait, what's this $g(\\mu)$ doing here?\n\nYes, it's odd, but hey, it just works. \n\nThere's this thing called **link function**, and it's the reason GLMs exist. Let me break down why we need this weird little function:\n\nIn linear regression, the mean of the model would be:\n$$\\mu = E(Y) = \\mathbf{X}\\beta$$\n\nOkay, this definitely feels like \"Hey, it works, okay?\", but that's my intuition about GLMs. I hope you understood a little in this part. Anyways, that's the mean of the linear regression that we know of, and this works great when $y$ can be any real number. But, I am asking you a question:\n\nWhat if your outcome isn’t allowed to be just any real number?\n\n-   What if $Y$ is a *count* $(0, 1, 2, 3, ...)$? Let's say *number of kids in a family* — $\\mathbf{X}\\beta$ could give you -3.7 customers\n-   What if $Y$ is a *probability* (0 to 1)? Probability of rain? Between 0 and 1, inclusive, you monster\n-   What if $Y$ is *binary* (0 or 1)? Let's say \"whether someone clicks your ad\" —  $\\mathbf{X}\\beta$ could give you 0.3, and it can't be 1.337, you know. \n\nThis strange thing-y in GLMs, link function $g(\\cdot)$, solves this by transforming the mean so that the linear predictor $\\eta = \\mathbf{X}\\beta$ can be any real number, while $\\mu$ stays in the valid range. \n\nUnderstand? No? Alright, let's say, the output of $\\eta$, which is an equivalent of $\\mathbf{X}\\beta$, should be at least close to the value in $g(\\mu)$, which is an equivalent of $g(E(y))$. \n\nMathematically:\n$$g(\\mu) = \\eta = \\mathbf{X}\\beta$$\n\nSo the inverse is:\n$$\\mu = g^{-1}(\\eta) = g^{-1}(\\mathbf{X}\\beta)$$\n\nAgain, to understand, we fit the model in \"transformed space\" where everything is linear, then transform back to get predictions in the correct range.\n\n**List of common link functions:**\n\n| Link | $g(\\mu)$ | $\\mu = g^{-1}(\\eta)$ | Used for | Why |\n|------|----------|---------------------|----------|-----|\n| Identity | $\\mu$ | $\\eta$ | Normal/Gaussian | No transformation needed |\n| Logit | $\\log\\left(\\frac{\\mu}{1-\\mu}\\right)$ | $\\frac{1}{1+e^{-\\eta}}$ | Binomial/Binary | Maps $(0,1) \\to (-\\infty, \\infty)$ |\n| Log | $\\log(\\mu)$ | $e^{\\eta}$ | Poisson/Counts | Maps $(0, \\infty) \\to (-\\infty, \\infty)$ |\n| Probit | $\\Phi^{-1}(\\mu)$ | $\\Phi(\\eta)$ | Binomial | For masochists |\n\n### Why Can't We Just Minimize Squared Errors?\n\nIn linear regression, we minimize the sum of squared residuals (SSR):\n$$SSR = \\sum_{i}(Y_i - \\hat{Y}_i)^2 = \\sum_{i}(Y_i - \\mathbf{X}\\beta)^2$$\n\nThis has a nice closed-form solution and works because:\n\n-   The errors are approximately normally distributed\n-   The variance is constant (a.k.a. homoscedasticity)\n-   Minimizing SSR is equivalent to maximum likelihood estimation\n\nBut in GLMs, these assumptions break down:\n\n-   Binomial data has variance $\\mu(1-\\mu)$, which depends on the mean\n-   Poisson data has variance equal to the mean $\\mu$\n-   The response isn't always in real number or approximately normally distributed (even with the presence of CLT)\n\nSo we need the alternative approach: **Maximum Likelihood Estimation** — this is the alternative estimation method for Linear regression. \n\nIn GLM, it will be proven difficult to estimate the optimal solution as the use case will become different. You see the table above? There are actually plenty of them. \n\n### The Three Sacred Components of GLM\n\nEvery GLM has exactly three components. Forget one at your dissertation defense, I dare you:\n\n1.  Random Component (Distribution Family)\n\n    Your response $Y$ has a likelihood function, expressed in an **exponential family distribution**. The fancy math looks like:\n    $$f(Y; \\theta, \\phi) = \\exp\\left\\{\\frac{Y\\theta - b(\\theta)}{a(\\phi)} + c(Y, \\phi)\\right\\}$$\n    \n    Translation: Normal, Binomial, Poisson, Gamma — they're all in this family because they can be written in this form. \n    \n    Why do we care? Because exponential family distributions have nice properties:\n    \n    -   $E[Y] = b'(\\theta)$ (the mean is the derivative of that $b$ function)\n    -   $\\text{Var}(Y) = b''(\\theta) \\cdot a(\\phi)$ (variance is also derived from $b$)\n\n2.  Systematic Component (Linear Predictor)\n\n    $$\\eta = \\mathbf{X}\\beta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots$$\n    \n    This is just your boring linear combination of predictors. Nothing fancy. We're still doing regression.\n\n3.  Link Function\n\n    $$g(\\mu) = \\eta$$\n    \n    Connects your linear predictor $\\eta$ (which lives in $(-\\infty, \\infty)$) to your mean response $\\mu$ (which might live in $(0, 1)$ or $[0, \\infty)$ or wherever your response is allowed to be).\n\n### Why Linear Regression is Just a Special Case\n\nNotice that if you:\n\n-   Use the **Normal distribution** (random component)\n-   Its *identity link* $g(\\mu) = \\mu$ (link function)\n-   Keep the linear predictor $\\eta = \\mathbf{X}\\beta$ (systematic component)\n\nYou get just it:\n\n$$\\mu = \\mathbf{X}\\beta$$\n\nwhich itself is equivalent to $E(y)$\n\nWhich is just...your good old ordinary regression: \n\n::: {.callout-note collapse=\"false\"}\n\n#### In R\n\n``` r\nlm(y ~ x, data = data)\n```\n\n:::\n\n::: {.callout-note collapse=\"false\"}\n\n#### In Python\n\n``` python \nimport statsmodels.formula.api as smf\n\nsmf. \\ \n    ols(\"y ~ x\", data = data). \\\n    fit(). \\ \n    summary()\n```\n\n::: \n\nLinear regression is a GLM with no identity, literally and figuratively. \n\n---\n\n## In-depth mathematics and computation {#math}\n\n![](math.gif)\n\n*What mathematics do I need to know more?*\n\nNow, you know it takes a lot to optimize the GLM. What are the things you need to know? \n\nFirst, in Linear regression, we have two ways to estimate the coefficients: MLE and ordinary least squares (OLS), which also can be derived and closed form from the maximum likelihood of the normal distribution. In GLM, we cannot use OLS, thus no closed form solution and [MLE](#mle) is the only main solution. \n\nThen for the extra part, which we talk about the [deviance](#deviance). Imagine this as the residual sum of squares in GLMs. \n\n### Maximum Likelihood Estimation {#mle}\n\nBut how do we actually estimate $\\beta$? We use *maximum likelihood estimation* (MLE), but considering there are many forms of likelihood functions, derived from several probability distributions came from exponential family, and so there's several link functions to be derived.\n\nThe likelihood for a GLM is:\n\n$$L(\\beta) = \\prod_{i=1}^n f(y_i; \\theta_i, \\phi)=\\prod_{i=1}^n\\exp\\left\\{\\frac{y_i\\theta_i - b(\\theta_i)}{a(\\phi)} + c(y_i, \\phi)\\right\\}$$\n\nFancy, isn't it? \n\nSIKE!! \n\n![](sike.gif)\n\nYou don't want this, trust me. This thing is so hard to calculate, even the computer starts to act crazy if you really direcly executed a program for this. Direct maximization of this product is computationally difficult. Taking logarithms simplifies things enormously (thanks, John Napier!). \n\nA sane person would only like what's the easier to eat — The log-likelihood function for the GLM instead:\n\n$$\\ell(\\beta) = \\sum_{i=1}^n \\left\\{\\frac{y_i\\theta_i - b(\\theta_i)}{a(\\phi)} + c(y_i, \\phi)\\right\\}$$\n\nNow, this is much easier to work with because:\n\n-   Products become sums\n-   It's numerically more stable\n-   The maximum is the same (logarithm is monotonic, after all)\n\n### Fisher Scoring / IRLS Algorithm (The Actual Work) {#irls}\n\nSince, there's no actual closed form to calculate the coefficients, we can't solve $U(\\beta) = 0$ directly. There's a method we can use called **Iteratively Reweighted Least Squares (IRLS)**, which is just Newton-Raphson but with the Fisher Information Matrix, denoted as $W$.\n\nBefore starting, we need to assign a variable that arbitrarily assigns design matrix `X`, the response vector `y`, and then determines the number of predictors `p`, the number of observations `n`, the maximum iteration `max_iter`, and the tolerance `eps`. \n\n:::: panel-tabset\n\n#### In R\n\n``` r\nn = nrow(X)\np = ncol(X)\nmax_iter = 100\neps = 1e-7\n```\n\n#### In Python\n\n``` python\nn = X.shape[0]\np = X.shape[1]\nmax_iter = 100\neps = 1e-7  \n```\n\n::::\n\nHere are the steps:\n\n#### 1.  **The Algorithm:** \n\nStart with initial values $\\beta^{(0)}$ (usually from an unweighted regression or just zeros).\n\nThis is how you program it in R and Python: \n\n:::: panel-tabset\n\n##### R\n\n``` r\nbeta = matrix(0, nrow = p, ncol = 1)\n```\n\n##### Python\n\n``` python\nimport numpy as np\n\nbeta = np.zeros((p, 1))\n```\n\n::::\n\n#### 2.  **Repeat until convergence:** \n\nIn this step, the extra steps, i.e.: \n\n-   Calculate linear predictor: $\\eta^{(t)} = \\mathbf{X}\\beta^{(t)}$\n-   Calculate fitted values: $\\mu^{(t)} = g^{-1}(\\eta^{(t)})$\n-   Calculate weights: \n\n    $$w_i^{(t)} = \\frac{1}{\\text{Var}(Y_i)} \\left(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\right)^2$$\n\n-   Calculate working response:\n\n    $$z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)}) \\frac{\\partial \\eta_i}{\\partial \\mu_i}$$\n\n-   Update coefficients by solving weighted least squares:\n\n    $$\\beta^{(t+1)} = (\\mathbf{X}^T W^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T W^{(t)} z^{(t)}$$\n\n-   Check convergence: If $|\\beta^{(t+1)} - \\beta^{(t)}| < \\epsilon$, stop. Else, go back to step 1.\n\n:::: panel-tabset\n\n##### R\n\n``` r\nfor (i in 1:max_iter) {\n    # 1. Calculate linear predictor\n    eta = X %*% beta\n    \n    # 2. Calculate fitted values\n    mu = family$linkinv(eta)\n    \n    # 3. Calculate weights\n    V = as.vector(family$variance(mu))\n    gradient = as.vector(family$mu.eta(eta))\n    w_vec = (gradient^2) / V\n    \n    # 4. Working response\n    z = as.vector(eta) + (as.vector(y) - mu) / gradient\n    \n    # 5. Update coefficients by solving weighted least squares\n    W = diag(as.vector(w_vec), n, n)\n    beta_new = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% z\n    \n    # 6. Check convergence\n    if (max(abs(beta_new - beta)) < tol) {\n        beta = beta_new\n        converged = TRUE\n        break\n    }\n    \n    # Final beta\n    beta = beta_new\n}\n```\n\n##### Python\n\n``` python\nfor i in range(max_iter):\n    # 1. Calculate linear predictor\n    eta = X @ beta\n    \n    # 2. Calculate fitted values\n    mu = family.linkinv(eta)\n    \n    # 3. Calculate weights\n    V = family.variance(mu).flatten()\n    gradient = family.mu_eta(eta).flatten()\n    w_vec = (gradient ** 2) / V\n    \n    # 4. Working response\n    z = eta + (y - mu) / gp\n    \n    # 5. Update coefficients by solving weighted least squares\n    W = np.diag(w_vec)\n    beta_new = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ z)\n    \n    # 6. Check convergence\n    if np.max(np.abs(beta_new - beta)) < tol:\n        beta = beta_new\n        converged = True\n        break\n    \n    # Final beta\n    beta = beta_new\n```\n\n::::\n\n### Be deviant: Deviance (GLM's Version of Sum of Squares) {#deviance}\n\nTo be frank, I am not planning to include this, but I was thinking if anyone is curious on how the fitness in GLMs is being measured. In Linear regression, we measure model fit by summarizing the residuals using Residual Sum of Squares (RSS). It is so odd for GLM to use other way to measure (well, the key term is \"Generalized\", after all) — Instead, GLMs use **deviance**:\n\n$$D = 2[\\ell(\\text{saturated model}) - \\ell(\\text{fitted model})]$$\n\nThe saturated model has $\\hat{\\mu}_i = y_i$ (perfect fit). Deviance measures how much worse your model is than this.\n\n**For specific families:**\n\n- **Normal / Gaussian**: $D = \\sum(y_i - \\hat{\\mu}_i)^2$ (literally just RSS)\n- **Poisson**: $D = 2\\sum\\left[y_i\\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) - (y_i - \\hat{\\mu}_i)\\right]$\n- **Binomial**: $D = 2\\sum\\left[y_i\\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) + (n_i-y_i)\\log\\left(\\frac{n_i-y_i}{n_i-\\hat{\\mu}_i}\\right)\\right]$\n\nLower deviance = better fit.\n\n## Full Function implementation\n\nThe main programming languages are still R and Python. The functions I implemented expects to be brittle and not applicable for some cases.\n\n*The functions are implemented in this [source code](https://github.com/joshuamarie/joshuamarie/tree/main/posts/07-glm)*.\n\n--- \n\n## Show Me the Money: mtcars Logistic Regression Showdown\n\n*Okay, enough theory — show me the money!*\n\nNow, we can implement a full function only from [*In-depth mathematics and computation*](#math). That is, if you only need the coefficients. Use any dataset you want, but on our example, we'll use the `mtcars` dataset for the simpletons.\n\nWhy `mtcars`? Because every R tutorial on earth has used it at least once, and I’m not about to break tradition, am I? We’ll predict whether a car has an automatic (0) or manual (1) transmission (`am`) using horsepower (`hp`), and weight (`wt`). Classic binary outcome -> logistic regression -> GLM with binomial family and logit link.\n\nWe’ll do it three ways so you can flex at parties:\n\n1. The lazy way (`stats::glm()` in R / `{statsmodels}` in Python)\n2. The \"I Implemented IRLS at 3 AM\" way (calling our own IRLS function in this [part](#irls))\n\n### Tier 1: The lazy way (everyone does this)\n\nThis is the \"why should I bother making myself one if there's one existed already\" part. R has built-in `glm()` from `{stats}` package, while Python has `{statsmodels}` package, ideally under `statsmodels.api`, not under `statsmodels.formula.api`, but for our example, we can use that instead. \n\n:::: panel-tabset\n\n#### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_r = glm(am ~ hp + wt, data = mtcars, family = binomial(link = \"logit\"))\nsummary(fit_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = am ~ hp + wt, family = binomial(link = \"logit\"), \n    data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)   \n(Intercept) 18.86630    7.44356   2.535  0.01126 * \nhp           0.03626    0.01773   2.044  0.04091 * \nwt          -8.08348    3.06868  -2.634  0.00843 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 10.059  on 29  degrees of freedom\nAIC: 16.059\n\nNumber of Fisher Scoring iterations: 8\n```\n\n\n:::\n:::\n\n\n#### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.api as sm\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.genmod.families.links import Logit\nimport statsmodels.formula.api as smf\n\n# mtcars = sm.datasets.get_rdataset(\"mtcars\").data\nmtcars = r.mtcars\n\nfit_py = smf \\\n    .glm(\n        \"am ~ hp + wt\",\n        data=mtcars,\n        family=Binomial(link=Logit())\n    ) \\\n    .fit()\n\nprint(fit_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                     am   No. Observations:                   32\nModel:                            GLM   Df Residuals:                       29\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -5.0296\nDate:                Mon, 01 Dec 2025   Deviance:                       10.059\nTime:                        17:18:29   Pearson chi2:                     15.0\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.6453\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     18.8663      7.444      2.535      0.011       4.277      33.455\nhp             0.0363      0.018      2.044      0.041       0.001       0.071\nwt            -8.0835      3.069     -2.634      0.008     -14.098      -2.069\n==============================================================================\n```\n\n\n:::\n:::\n\n\n::::\n\n### Tier 2: The “I Implemented IRLS at 3 AM” way\n\n:::: panel-tabset\n\n#### R\n\n> View the [source code](https://github.com/joshuamarie/joshuamarie/tree/main/posts/07-glm/glm.r)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox::use(\n    ./glm[glm_custom]\n)\n\nglm_custom(am ~ hp + wt, data = mtcars, family = binomial())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$coefficients\n[1] 18.8662987  0.0362556 -8.0834752\n\n$converged\n[1] TRUE\n\n$iterations\n[1] 9\n```\n\n\n:::\n:::\n\n\n#### Python\n\n> View the [source code](https://github.com/joshuamarie/joshuamarie/tree/main/posts/07-glm/glm.py)\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom glm import glm_custom\nimport glm\n\nprint(glm_custom('am ~ hp + wt', data = mtcars, family = glm.Binomial()))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{'coefficients': {'Intercept': np.float64(18.866298717203943), 'hp': np.float64(0.03625559608221654), 'wt': np.float64(-8.083475182444579)}, 'converged': True, 'iterations': 9, 'beta_vector': array([[18.86629872],\n       [ 0.0362556 ],\n       [-8.08347518]])}\n```\n\n\n:::\n:::\n\n\n::::\n\n---\n\n## When Things Go Wrong (Trust me, they Will)\n\n### \"My model won't converge!\"\n\n-   Check for complete separation in logistic regression\n-   Try scaling your predictors\n-   Check for perfect multicollinearity\n-   Or just accept that your data is cursed\n\n### \"My coefficients are huge!\"\n\n-   You forgot to scale your variables\n-   Or you have a separation issue\n-   Or your data really is that dramatic\n\n### \"The deviance is gigantic!\"\n\n-   Maybe your model actually sucks?\n-   Or you have outliers\n-   Check if you're using the right family\n-   Consider adding interaction terms or polynomial terms\n\n### \"I'm getting warnings about fitted probabilities of 0 or 1\"\n\n-   This is actually okay sometimes! It means you have very confident predictions\n-   But if you're getting tons of these, you might have separation issues\n-   Check your data for extreme values\n\n---\n\n## Final Boss Wisdom\n\n- Linear regression is a GLM that forgot it was a GLM\n- The link function is the reason you’re not predicting negative counts\n- IRLS is just weighted least squares that updates itself until it stops crying\n- Deviance is RSS for adults\n\nGo forth and model responsibly.\n\n## Some good references\n\n- McCullagh & Nelder (1989) – The Bible. Dense. Bring coffee and a therapist.\n- Dobson & Barnett – Actually readable introduction\n- Faraway – Practical R examples, minimal pain\n- Nelder & Wedderburn (1972) – The original paper. Short, brilliant, life-changing\n- The UCLA stats page on GLMs – Still the best free resource in 2025\n\nAnd remember:\n\n> “All models are wrong. But GLMs are wrong in the most elegant way possible.”\n\nNow go write some proper models, you beautiful statistician.\n\n## Appendix — Cheat Sheet\n\n*Which GLM Should I Use?*\n\n| Your Y looks like... | Distribution | Link | R Code | Python Code |\n|---------------------|--------------|------|---------|-------------|\n| 0, 1, 2, 3... (counts) | Poisson | log | `family = poisson()` | `sm.families.Poisson()` |\n| Yes/No (binary) | Binomial | logit | `family = binomial()` | `sm.families.Binomial()` |\n| 0.2, 0.7, 0.4... (proportions) | Binomial | logit | `family = binomial()` | `sm.families.Binomial()` |\n| Always positive, right-skewed | Gamma | log | `family = Gamma()` | `sm.families.Gamma()` |\n| Any real number | Gaussian | identity | `family = gaussian()` or `lm()` | `sm.families.Gaussian()` |\n\n*Oh wait, the overdispersion in count data? R pre-installs `{MASS}` anyways, so bother use `glm.nb()` yourself. *\n\n**Pro tip**: When in doubt, plot your residuals. If they look like a crime scene, you chose wrong.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}