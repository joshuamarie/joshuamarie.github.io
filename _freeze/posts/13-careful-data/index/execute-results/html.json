{
  "hash": "b25667c064b30ec13fffa4eced7637a6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"A 'careful' and 'small' guide to data science with 'tidyverse'\"\nsubtitle: \"\"\ndate: \"2025-12-28\"\ncategories: [R, data-science, analytics, tidyverse]\ndescription: \"Teaching you the things you can take advantage of, and things weren't taught by some tutorials you (potentially) know\"\nformat:\n    html:\n        toc: true\n        toc-float: true\n        toc-depth: 3\n        number-sections: true\n        code-fold: false\n        code-tools: false\n        theme: default\n        fig-width: 10\n        fig-height: 6\n        fig-cap-location: bottom\n        code-annotations: hover\nexecute:\n    echo: true\n    warning: false\n    message: false\nfilters:\n    - social-share\n    - collapse-output\nshare:\n    permalink: \"https://joshuamarie.com/posts/13-careful-data\"\n    description: \"Careful data science with tidyverse — Joshua Marie\"\n    twitter: true\n    facebook: true\n    reddit: true\n    stumble: true\n    tumblr: true\n    linkedin: true\n    email: true\n    mastodon: true\n    bsky: true\n    location: \"before-body\"\nengine: knitr\n---\n\n# Why \"careful\"? \n\nAs you read in the title, why \"careful\"? Using `{tidyverse}`, or any frameworks for data manipulation out there, is fine. Most introductory `{tidyverse}` tutorials teach you how to write code with it. Very few teach you when the nice-looking code becomes dangerous or unnecessarily painful six months later. \n\nThis short guide focuses on the second part — the things that experienced people wish they had known earlier.\n\nBefore we start, here are the packages I'll be using in this post:\n\n1.  Packages that are `{tidyverse}`. This includes the following specifics:\n\n    -   `{dplyr}`\n    -   `{tidyr}`\n    -   `{readr}`\n    -   `{lubridate}`\n    -   `{purrr}`\n\n2.  Packages that are not `{tidyverse}`. This includes the following specifics:\n\n    -   `{box}`\n    -   `{numberize}`\n    -   `{broom}`\n    -   `{e1071}`\n\nAnd they will be used much later on. \n\n# Tidyverse is not just unquoted column names\n\nYou thought the reason why `{tidyverse}` framework differ from other frameworks, e.g. Pandas, Polars, or `{data.table}`, comes from accepting \"columns\" argument not being quoted? I mean, yes, and `{data.table}` does this as well, although the syntax and APIs are not completely the same. That's not what I am trying to convey here. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This fails R CMD check in a package\nmtcars |> dplyr::mutate(new = mpg / wt) |> head(5)\n#>>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb      new\n#>> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 8.015267\n#>> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 7.304348\n#>> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 9.827586\n#>> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 6.656299\n#>> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 5.436047\n\n# `R CMD CHECK` friendly\nmtcars |> dplyr::mutate(new = .data$mpg / .data$wt) |> head(5)\n#>>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb      new\n#>> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 8.015267\n#>> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 7.304348\n#>> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 9.827586\n#>> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 6.656299\n#>> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 5.436047\n```\n:::\n\n\nWhy did I tell you this? Let's say you're planning to use this package as one of your package dependencies. Unfortunately, `R CMD CHECK` will fail and will complain if you are using bare column names, so instead use `.data$colname`. \n\nAnother method is the use of `{{ name }}` when used in a different context. The most optimal application is when you’re writing functions that takes column names as arguments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_summarise1 = function(data, col) {\n    data |> \n        dplyr::summarise(mean_col := mean({{ col }}, na.rm = TRUE))\n}\n\niris |> my_summarise1(Sepal.Length)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mean_col\n1 5.843333\n```\n\n\n:::\n:::\n\n\nHere's more complex as it uses `!!` (pronounced as \"bang-bang\") operator: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_summarise2 = function(data, col) {\n    box::use(\n        dplyr[summarise],\n        glue[glue],\n        rlang[ensym, as_string]\n    )\n    \n    capt_col = ensym(col)\n    name_col = glue(\"mean_{as_string(capt_col)}\")\n    \n    data |> \n        summarise(!!name_col := mean({{ col }}, na.rm = TRUE))\n}\n\niris |> my_summarise2(Sepal.Length)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mean_Sepal.Length\n1          5.843333\n```\n\n\n:::\n:::\n\n\nClick this [link](https://dplyr.tidyverse.org/articles/programming.html#indirection) for more details. \n\n# Don't just straight data imports\n\nDo not just import data (not limited to CSVs), most especially when the data wasn't collected in one pattern. For example, a CSV data that has \"date\" column(s), where it doesn't follow the right format, e.g. `%Y-%m-%d` (translated as \"2005-01-29\" for example). This is probably the #1 source of silent bugs in production data pipelines, especially if overlooked.\n\nBut first, let me tell you something: \n\n::: callout-note\n\n## When CSV file contains messy format in general\n\n-   If the data contains messy format in general, set all the columns into string type first\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox::use(readr[read_csv, cols, col_character])\n\n# Dangerous (very common in real life)\nread_csv(\"data.csv\")\n\n# Much safer minimum\nread_csv(\n    \"data.csv\",\n    col_types = cols(\n        # force everything as text first\n        .default = col_character(),           \n    )\n)\n```\n:::\n\n\nAuto-guessing fails when:\n\n1.  First 1000 rows look clean, row 1001 doesn't\n2.  Excel exported dates as text in weird formats\n3.  IDs like \"001234\" get parsed as numbers (losing leading zeros)\n4.  Mix of `\"N/A\"`, `\"null\"`, `\"NULL\"`, `\"-\"`, `\"nan\"` all mean missing\n\n:::\n\n## Synthetic example: Dealing with dates\n\nHere, imagine you have this kind of data in a certain CSV, where it contains filedate/datetime columns that come:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_csv = \"\ndate, readings, x, y, z\\n\n\\\"Mar.23,2005\\\",0.02,3.3,1.1,5.7\\n\n\\\"Feb.26,2005\\\",0.15,4.5,5.0,1.9\\n\n\\\"Apr.5,2005\\\",0.2,7.2,2.8,5.2\\n\n\\\"May.12,2005\\\", 0.5,4.9,1.3,6.8\n\"\n```\n:::\n\n\nIf you directly import this CSV data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreadr::read_csv(I(sample_csv))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  date        readings     x     y     z\n  <chr>          <dbl> <dbl> <dbl> <dbl>\n1 Mar.23,2005     0.02   3.3   1.1   5.7\n2 Feb.26,2005     0.15   4.5   5     1.9\n3 Apr.5,2005      0.2    7.2   2.8   5.2\n4 May.12,2005     0.5    4.9   1.3   6.8\n```\n\n\n:::\n:::\n\n\nThe problem arises when `readr::read_csv()` fails to parse the `\"date\"` column as an actual `date` type as you expect, instead it is parsed as a string type. \n\nThe `{readr}` package provides an API that can explicitly judge the column type. \n\nHere's how you do it: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox::use(\n    readr[read_csv, cols, col_date]\n)\n\nread_csv(\n    I(sample_csv),\n    col_types = cols(\n        date = col_date(\"%b.%d,%Y\")\n    )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  date       readings     x     y     z\n  <date>        <dbl> <dbl> <dbl> <dbl>\n1 2005-03-23     0.02   3.3   1.1   5.7\n2 2005-02-26     0.15   4.5   5     1.9\n3 2005-04-05     0.2    7.2   2.8   5.2\n4 2005-05-12     0.5    4.9   1.3   6.8\n```\n\n\n:::\n:::\n\n\n## Synthetic example: Multiple messy date formats in one column\n\nBut, what if the `'date'` column doesn't follow 1 pattern? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox::use(\n    readr[read_csv, cols, col_character],\n    dplyr[mutate], \n    lubridate[parse_date_time, as_date]\n)\n\nsample_csv = \"\ndate, readings, x, y, z\\n\n\\\"Mar.23,2005\\\",0.02,3.3,1.1,5.7\\n\n\\\"Feb 26, 2005\\\",0.15,4.5,5.0,1.9\\n\n\\\"04-5-2005\\\",0.2,7.2,2.8,5.2\\n\n\\\"May12,05\\\", 0.5,4.9,1.3,6.8\\n\n\\\"Aug.17,2005\\\", 0.17,9.5,0.8,4.2\n\"\n\nread_csv(\n    I(sample_csv),\n    col_types = cols(\n        date = col_character()\n    )\n) |> \n    mutate(\n        date = \n            parse_date_time(\n                date,\n                orders = c(\n                    \"b.d,Y\",\n                    \"b d, Y\",\n                    \"m-d-Y\",\n                    \"bd,y\"\n                )\n            ) |> \n            as_date()\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 5\n  date       readings     x     y     z\n  <date>        <dbl> <dbl> <dbl> <dbl>\n1 2005-03-23     0.02   3.3   1.1   5.7\n2 2005-02-26     0.15   4.5   5     1.9\n3 2005-04-05     0.2    7.2   2.8   5.2\n4 2005-05-12     0.5    4.9   1.3   6.8\n5 2005-08-17     0.17   9.5   0.8   4.2\n```\n\n\n:::\n:::\n\n\nUnfortunately, right now, `readr::read_csv()` doesn't know how to parse CSV files with different data format, so I have to take some roundabouts to properly parse the `\"date\"` column. \n\n# Use tidyverse aggressively for serious data cleaning\n\nNow you know how easy it get when importing the data with messed up pattern structure. Use `{tidyverse}` for data cleaning for f sake, trust me. Seriously, if you can do data cleaning from other tools, e.g. Excel or Python-Pandas (I know some companies you are working with will choose them), `{tidyverse}` makes things much easier, conventional, readable, and maintainable (arguable). Some practitioners actually use `{tidyverse}` for modeling and resort to base R loops or hacky solutions for cleaning. Don't be that person.\n\nLet us demonstrate with the [Messy HR Data](https://raw.githubusercontent.com/eyowhite/Messy-dataset/main/messy_HR_data.csv\") dataset from this [repository](https://github.com/eyowhite/Messy-dataset). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhr_data = readr::read_csv(\n    \"https://raw.githubusercontent.com/eyowhite/Messy-dataset/main/messy_HR_data.csv\"\n)\n```\n:::\n\n\nIf you glimpse this data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhr_data |> dplyr::glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,000\nColumns: 10\n$ Name                <chr> \"grace\", \"david\", \"hannah\", \"eve\", \"grace\", \"jack\"…\n$ Age                 <chr> \"25\", \"nan\", \"35\", \"nan\", \"nan\", \"nan\", \"nan\", \"40…\n$ Salary              <chr> \"50000\", \"65000\", \"SIXTY THOUSAND\", \"50000\", \"NAN\"…\n$ Gender              <chr> \"Male\", \"Female\", \"Female\", \"Female\", \"Female\", \"O…\n$ Department          <chr> \"HR\", \"Finance\", \"Sales\", \"IT\", \"Finance\", \"Market…\n$ Position            <chr> \"Manager\", \"Director\", \"Director\", \"Manager\", \"Man…\n$ `Joining Date`      <chr> \"April 5, 2018\", \"2020/02/20\", \"01/15/2020\", \"Apri…\n$ `Performance Score` <chr> \"D\", \"F\", \"C\", \"A\", \"F\", \"F\", \"B\", \"C\", \"C\", \"A\", …\n$ Email               <chr> \"email@example.com\", \"user@domain.com\", \"email@exa…\n$ `Phone Number`      <chr> \"nan\", \"123-456-7890\", \"098-765-4321\", NA, \"098-76…\n```\n\n\n:::\n:::\n\n\nYou see how bad it is, even if you said it's not \"dirty enough\" or similar. The reason why `{tidyverse}` makes things \"conventional\" because in R, missing values has its own unique representation value, denoted by `NA`, not through `null`, `NULL`, or `NaN` / `nan`. \n\nThe data cleaning in R with `{tidyverse}` is much easier than you thought\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox::use(\n    dplyr[mutate, across, everything, if_else, case_when],\n    stringr[detect = str_detect, capitalize = str_to_sentence],\n    lubridate[parse_date_time, as_date]\n)\n\nhr_data |> \n    mutate(\n        across(\n            everything(), \n            \\(col) if_else(detect(col, \"^(?i)nan$\"), NA, col)\n        ), \n        `Joining Date` = parse_date_time(\n            `Joining Date`, c(\n                \"%B %d, %Y\",\n                \"%Y/%m/%d\", \n                \"%m-%d-%Y\",\n                \"%Y.%m.%d\"\n            )\n        ) |> as_date(), \n        Salary = numberize::numberize(Salary),\n        Name = capitalize(Name),\n        Age = numberize::numberize(Age)\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,000 × 10\n   Name      Age Salary Gender Department Position  `Joining Date`\n   <chr>   <dbl>  <dbl> <chr>  <chr>      <chr>     <date>        \n 1 Grace      25  50000 Male   HR         Manager   2018-04-05    \n 2 David      NA  65000 Female Finance    Director  2020-02-20    \n 3 Hannah     35  60000 Female Sales      Director  2020-01-15    \n 4 Eve        NA  50000 Female IT         Manager   2018-04-05    \n 5 Grace      NA     NA Female Finance    Manager   2020-01-15    \n 6 Jack       NA  65000 Other  Marketing  Director  2019-03-25    \n 7 Charlie    NA  50000 Male   Marketing  Clerk     2019-12-01    \n 8 Grace      40  50000 Other  HR         Director  2019-03-25    \n 9 Hannah     40  60000 Female Marketing  Manager   2020-01-15    \n10 Eve        30     NA Other  Finance    Assistant 2020-02-20    \n# ℹ 990 more rows\n# ℹ 3 more variables: `Performance Score` <chr>, Email <chr>,\n#   `Phone Number` <chr>\n```\n\n\n:::\n:::\n\n\nYou see how easy is this? The fact that `dplyr::mutate()` accepts bare expressions together with its bare column, you can easily use other functions, like `numberize::numberize()` to translate bare word value, i.e. `\"thirty\"` in `Age` column and `\"SIXTY THOUSAND\"` in `Salary` column. \n\n# Use across() + <tidy-select> for safe type / role-based transformations\n\nYou see how I clean the data above, right? I use one of `{tidyverse}`'s most underrated features: `across()` combined with `<tidy-select>` helpers, commonly used in \"data masking\" functions, where the operation is applied _across_ multiple columns. This lets you apply transformations to multiple columns based on their type or name pattern, making your code both DRY and maintainable.\n\n## Clean all text columns at once\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox::use(\n    dplyr[mutate, across, where],\n    stringr[str_trim, str_to_upper]\n)\n\nsurvey = tibble::tibble(\n    resp_id = c(1, 2, 3),\n    name = c(\"  alice  \", \"BOB\", \"  Charlie  \"),\n    city = c(\"New York  \", \"  Boston\", \"Chicago  \"),\n    rating = c(4.2, 3.8, 4.5)\n)\n\nsurvey |>\n    mutate(\n        across(where(is.character), str_trim),\n        across(where(is.character) & !resp_id, str_to_upper)\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  resp_id name    city     rating\n    <dbl> <chr>   <chr>     <dbl>\n1       1 ALICE   NEW YORK    4.2\n2       2 BOB     BOSTON      3.8\n3       3 CHARLIE CHICAGO     4.5\n```\n\n\n:::\n:::\n\n\nThe second `across()` shows combining conditions: all character columns *except* the ID field.\n\n## Scale financial columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox::use(dplyr[mutate, across, starts_with])\n\nfinancials = tibble::tibble(\n    company = c(\"TechCo\", \"RetailCorp\"),\n    revenue_q1 = c(1200000, 850000),\n    revenue_q2 = c(1350000, 920000),\n    costs_q1 = c(800000, 600000),\n    costs_q2 = c(850000, 640000)\n)\n\nfinancials |>\n    mutate(\n        across(\n            starts_with(\"revenue\"),\n            \\(x) x / 1e6,\n            .names = \"{.col}_M\"\n        )\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n  company    revenue_q1 revenue_q2 costs_q1 costs_q2 revenue_q1_M revenue_q2_M\n  <chr>           <dbl>      <dbl>    <dbl>    <dbl>        <dbl>        <dbl>\n1 TechCo        1200000    1350000   800000   850000         1.2          1.35\n2 RetailCorp     850000     920000   600000   640000         0.85         0.92\n```\n\n\n:::\n:::\n\n\nThe `.names` argument controls output column names. When you add `revenue_q3` later, it automatically gets converted too.\n\n## Conditional rounding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox::use(dplyr[mutate, across, where, any_of])\n\nmeasurements = tibble::tibble(\n    sample_id = 1:4,\n    temperature = c(98.6234, 99.1456, 97.8921, 98.4567),\n    pressure = c(120.456, 118.234, 122.789, 119.123),\n    ph_level = c(7.4123, 7.3987, 7.4256, 7.4089)\n)\n\nmeasurements |>\n    mutate(\n        across(\n            where(is.numeric) & !any_of(\"sample_id\"),\n            round,\n            digits = 2\n        )\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n  sample_id temperature pressure ph_level\n      <int>       <dbl>    <dbl>    <dbl>\n1         1        98.6     120.     7.41\n2         2        99.2     118.     7.4 \n3         3        97.9     123.     7.43\n4         4        98.5     119.     7.41\n```\n\n\n:::\n:::\n\n\nThis pattern scales: add new numeric columns and they're automatically rounded. No need to update the rounding code.\n\n## Multiple functions and their \"names\"\n\nThe `.fns` parameter from `dplyr::across()` function allows multiple functions, stored in a list, and name them (I strongly recommend naming them). They could be a known function, a lambda / anonymous function (written through `function()` keyword or its short-hand `\\()`), a purrr-style lambda function, or a mix of them — as long as they are \"function\" objects, nothing else.  \n\nMy most used scenario is when you want to calculate the descriptive statistics for all numeric columns: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox::use(\n    tidyselect[where, matches], \n    dplyr[summarise, across],\n    tidyr[long = pivot_longer, wide = pivot_wider],\n    purrr[compose, partial], \n    stats[sd, quantile],\n    e1071[skewness, kurtosis]\n)\n\niris |> \n    summarise(\n        across(\n            where(is.numeric), \n            list(\n                n_missing = compose(sum, is.na), # Since compose() returns a function\n                mean = ~ mean(., na.rm = TRUE), \n                sd = \\(col) sd(col, na.rm = TRUE), \n                min = min, \n                q1 = ~ quantile(., prob = 0.25, na.rm = TRUE), \n                md = \\(col) median(col, na.rm = TRUE), \n                q3 = ~ quantile(., prob = 0.75, na.rm = TRUE), \n                max = \\(col) max(col, na.rm = TRUE),\n                skew = skewness,\n                kurt = partial(kurtosis, na.rm = TRUE) # Since partial() returns a function\n            ), \n            \n            .names = \"{.col}..{.fn}\"\n        )\n    ) |> \n    long(\n        cols = matches(\"\\\\.\\\\.\"), \n        names_pattern = \"(.+)\\\\.\\\\.(.+)\",  \n        names_to = c(\"variable\", \"statistic\"),\n        values_to = \"est\"\n    ) |> \n    wide(\n        names_from = statistic,\n        values_from = est\n    ) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 11\n  variable     n_missing  mean    sd   min    q1    md    q3   max   skew   kurt\n  <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>\n1 Sepal.Length         0  5.84 0.828   4.3   5.1  5.8    6.4   7.9  0.309 -0.606\n2 Sepal.Width          0  3.06 0.436   2     2.8  3      3.3   4.4  0.313  0.139\n3 Petal.Length         0  3.76 1.77    1     1.6  4.35   5.1   6.9 -0.269 -1.42 \n4 Petal.Width          0  1.20 0.762   0.1   0.3  1.3    1.8   2.5 -0.101 -1.36 \n```\n\n\n:::\n:::\n\n\n# Joins flavor in dplyr\n\nSince the stable version (1.1.x) release of `{dplyr}`, specification of joins is now more flexible, more explicit, and more \"controlled\" using `dplyr::join_by()`. While the new API provides clearer complex \"join\" tasks, this also makes the operation less error-prone.\n\n## Inequality joins for binning\n\nAssign customer segments based on purchase totals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox::use(dplyr[left_join, join_by, group_by, slice_max, ungroup])\n\npurchases = tibble::tibble(\n    customer_id = 1:8,\n    total_spent = c(45, 280, 520, 95, 1200, 180, 650, 35)\n)\n\nsegments = tibble::tibble(\n    segment = c(\"Bronze\", \"Silver\", \"Gold\", \"Platinum\"),\n    min_spend = c(0, 100, 500, 1000)\n)\n\npurchases |>\n    left_join(segments, by = join_by(total_spent >= min_spend)) |>\n    group_by(customer_id) |>\n    slice_max(min_spend, n = 1, with_ties = FALSE) |>\n    ungroup()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 × 4\n  customer_id total_spent segment  min_spend\n        <int>       <dbl> <chr>        <dbl>\n1           1          45 Bronze           0\n2           2         280 Silver         100\n3           3         520 Gold           500\n4           4          95 Bronze           0\n5           5        1200 Platinum      1000\n6           6         180 Silver         100\n7           7         650 Gold           500\n8           8          35 Bronze           0\n```\n\n\n:::\n:::\n\n\nThe \"greater than or equal to (`>` + `=`)\" operator finds all matching tiers, then we keep the highest one. Much clearer than manual conditional logic.\n\n## Range joins for time-based matching\n\nMatch transactions to active promotions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox::use(\n    dplyr[tbl = tibble, inner_join, join_by, between, transmute],\n    lubridate[as_date]\n)\n\ntransactions = tbl(\n    tx_id = 1:6,\n    tx_date = as_date(c(\n        \"2024-01-20\", \"2024-02-15\", \"2024-03-25\",\n        \"2024-04-10\", \"2024-05-15\", \"2024-06-05\"\n    )),\n    amount = c(150, 200, 180, 220, 175, 190)\n)\n\npromos = tbl(\n    promo = c(\"New Year\", \"Spring Sale\", \"Summer Blast\"),\n    start = as_date(c(\"2024-01-01\", \"2024-03-01\", \"2024-05-01\")),\n    end = as_date(c(\"2024-01-31\", \"2024-04-30\", \"2024-06-30\")),\n    discount = c(0.15, 0.10, 0.20)\n)\n\ntransactions |>\n    inner_join(\n        promos,\n        by = join_by(between(tx_date, start, end))\n    ) |>\n    transmute(\n        tx_id,\n        tx_date,\n        promo, \n        amount,\n        discount, \n        savings = amount * discount\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 6\n  tx_id tx_date    promo        amount discount savings\n  <int> <date>     <chr>         <dbl>    <dbl>   <dbl>\n1     1 2024-01-20 New Year        150     0.15    22.5\n2     3 2024-03-25 Spring Sale     180     0.1     18  \n3     4 2024-04-10 Spring Sale     220     0.1     22  \n4     5 2024-05-15 Summer Blast    175     0.2     35  \n5     6 2024-06-05 Summer Blast    190     0.2     38  \n```\n\n\n:::\n:::\n\n\n::: {.callout-note title=\"SQL Code Equivalent\" collapse=\"true\"}\nHere's the equivalent SQL code:\n\n``` sql\nSELECT\n    tx_id,\n    tx_date,\n    promo,\n    amount,\n    discount,\n    amount * discount AS savings\nFROM (\n    SELECT t.*, p.*\n    FROM transactions AS t\n    INNER JOIN promos AS p\n        ON (\n            t.tx_date >= p.start AND\n            t.tx_date <= p.end\n        )\n) AS new_table\n```\n:::\n\nThis beats manually checking date ranges with `filter()`. The join expresses intent directly.\n\n# Advanced tidyverse Tricks for Grouped Modeling\n\nWhat's so great about `{tidyverse}` is while easy to use, there are a lot of things you can take advantage of, such as the code below — it's far more flexible than you thought. In the example below, I take advantage of `dplyr::reframe()` to generate a new data frame with a variable number of rows directly from grouped operations on the original data.\n\nIn this case, we're exploring the linear relationship between vehicle weight (`wt`) and fuel efficiency (`mpg`) in the classic `{mtcars}` dataset, stratified by the number of cylinders (`cyl`). The code below computes key summary statistics — including regression coefficients, Pearson correlation, R-squared, and adjusted R-squared—for each cylinder group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox::use(dplyr[group_by, reframe])\n\nmtcars |> \n    group_by(cyl) |> \n    reframe(\n        {         \n            box::use(\n                stats[linear_reg = lm, coef, pearson_r = cor], \n                purrr[imap_dfc, set_names], \n                tibble[tbl = tibble]\n            )\n            \n            model = linear_reg(mpg ~ wt)\n            coefs = coef(model)\n            coef_table = imap_dfc(coefs, \\(bi, nm) {\n                result = tbl(bi)\n                set_names(result, nm)\n            })\n            \n            corr = pearson_r(wt, mpg)\n            \n            test = summary(model)\n            tbl(\n                coef_table, \n                corr = corr, \n                rsq = test$r.squared,\n                adj_rsq = test$adj.r.squared\n            )\n        }\n        \n        # , .by = cyl\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n    cyl `(Intercept)`    wt   corr   rsq adj_rsq\n  <dbl>         <dbl> <dbl>  <dbl> <dbl>   <dbl>\n1     4          39.6 -5.65 -0.713 0.509   0.454\n2     6          28.4 -2.78 -0.682 0.465   0.357\n3     8          23.9 -2.19 -0.650 0.423   0.375\n```\n\n\n:::\n:::\n\n\nThe code cleverly uses `dplyr::reframe()` with an anonymous block `{} `to run a complete mini-analysis per cylinder group in `mtcars` dataset. Inside the block, it loads just the needed functions via `box::use()`, fits a linear model of `mpg ~ wt` on the current group, extracts coefficients into a tidy one-row table, computes Pearson correlation, and grabs $R^2$ and adjusted $R^2$ from the model summary. Everything is then combined into a single row per group. This compact, namespace-safe pattern lets you perform fairly complex grouped modeling without cluttering the environment with temporary objects.\n\n::: {.callout-note collapse=\"true\"}\n## Beyond p-Values: Why Effect Size and Power Matter\n\nI chose this scenario to somehow demonstrate the flaw of NHST, at least part of this \"careful\" data science guide with `{tidyverse}`. Here are some insights of the results above why I chose this scenario: \n\n-   When investigating relationships between variables, it's tempting to rely solely on p-values from null hypothesis significance testing (NHST). For instance, a simple linear regression on the full `mtcars` dataset shows a highly significant relationship between `wt` and `mpg`:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    model = lm(mpg ~ wt, data = mtcars)\n    broom::tidy(model)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    # A tibble: 2 × 5\n      term        estimate std.error statistic  p.value\n      <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n    1 (Intercept)    37.3      1.88      19.9  8.24e-19\n    2 wt             -5.34     0.559     -9.56 1.29e-10\n    ```\n    \n    \n    :::\n    \n    ```{.r .cell-code}\n    broom::glance(model)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    # A tibble: 1 × 12\n      r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n          <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n    1     0.753         0.745  3.05      91.4 1.29e-10     1  -80.0  166.  170.\n    # ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n    ```\n    \n    \n    :::\n    :::\n\n\n-   After running both linear regression and correlation test at the same time, you can say it rejects the null hypothesis of the test, implying that there are strong linear relationship between `wt` and `mpg`. Keep in mind that directly using \"Null Hypothesis Significant Testing\" (NHST) is a direct flaw for reproducibility. The low p-value (< 0.001) and strong correlation suggest a clear negative linear association—higher weight predicts lower mileage.\n\n-   However, stratifying by cylinder count reveals a more nuanced picture. The 6-cylinder group with only 7 observations fails to reject the null hypothesis (p = 0.092), despite having a similar effect size to the other groups. This isn't because the relationship doesn't exist – it's because the sample size is too small to detect the effect reliably.\n\n    This demonstrates why:\n    \n    -   **Sample size matters**: The 6-cylinder group lacks statistical power\n    -   **Effect sizes differ**: The slope varies substantially across groups (-2.19 to -5.65), suggesting the relationship isn't uniform\n\nThat's why when doing an analysis, it is better to run simulation before conducting the analysis and the sample size should be large enough. \n:::\n\n<!-- # When using parallel mapping from {purrr} package -->\n\n<!-- Keep in mind that the `mutate()` function accepts bare expressions, and this is true if the expression you made allows vectorization — true for all the rows. What if it is not vectorized for all the rows?  -->\n\n<!-- Well, operations in `dplyr::rowwise()` does this, but my solution is, although verbose, it is more explicit, logical, and maintainable. Since it allows bare expression, directly using `purrr::pmap_*()` variants with `dplyr::pick()` are also valid within `mutate()`.  -->\n\n# Closing Remarks and Resources\n\nThose are all my \"simple annotation\" on your fussy data work, potentially (hoping) will help in your future jobs. \n\n-   [R for Data Science](https://r4ds.hadley.nz/)\n-   [Box: Placing module system into R](https://modules-in-r.joshuamarie.com/)\n-   [box: Write Reusable, Composable and Modular R Code](https://klmr.me/box/)\n-   [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/)\n-   [dplyr 1.1.0 blog post](https://www.tidyverse.org/blog/2023/01/dplyr-1-1-0-joins/) - Deep dive on `join_by()`\n-   [Programming with dplyr](https://dplyr.tidyverse.org/articles/programming.html) - When to use `.data$` and `{{ }}`\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}