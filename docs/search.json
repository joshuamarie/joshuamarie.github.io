[
  {
    "objectID": "services.html#firm",
    "href": "services.html#firm",
    "title": "Work with Me",
    "section": "Consultation Firm",
    "text": "Consultation Firm\n\n\n\n\n\n\nImportantTerms and Condition\n\n\n\nRead terms & and conditions first before booking.\n\n\n\n\n\nWhat I Offer\nMathematics & statistics foundation\n\nCalculus\nLinear Algebra\nProbability and statistics theory\n\nStatistical modelling & Machine Learning 1\n\nStatistical inference and simulation\nBayesian inference with Stan/CmdStan\nCustom model development and validation\n\nStatistical modelling & Machine Learning 2\n\nExperimental design and causal inference\nLongitudinal study and time series analysis & forecasting\nApplications with Python and R\n\n\n\nResearch & Methods Consultation\n\nStudy design and methodology review\nStatistical analysis plan development\nLongitudinal studies and panel data\nPower analysis and sample size calculation\nManuscript review (methods & results sections)\n\n\n\n\nWhat I Offer\nData Science\n\nData Visualization ({ggplot2}, {altair})\nShiny app development\nData manipulation and relational algebra with SQL and {tidyverse}\nGeospatial data analysis\n\nPackage Development\n\nR/Python package development\nAPI design and documentation\nTesting frameworks\nOpen-source maintenance strategies\n\n\n\n\n\n\n\nRates, Inquiry & Engagement\n\n\nRetainer packages available for ongoing work\nBook the consultation firm in Calendly\nOr email me at: joshua.marie.k@gmail.com"
  },
  {
    "objectID": "services.html#course-tutorial",
    "href": "services.html#course-tutorial",
    "title": "Work with Me",
    "section": "Course Tutorial",
    "text": "Course Tutorial\nThis is a live session with me, prepared with interactive slides.\n\n\n\n\n\n\nImportantTerms and Condition\n\n\n\nRead terms & and conditions first before booking.\n\n\n\nAvailable Now\n\nCore of R Programming\n\nCourse section 1: Functional programming\nCourse section 2: Performance optimization\nCourse section 3: Metaprogramming\n\n\n\n\nComing Soon\n\n\n\n\n\nBayesian Inference with Stan\n\nCourse section 1: Probabilistic programming fundamentals\nCourse section 2: Stan mechanics\nCourse section 3: Model building and diagnostics\n\n\n\nA/B Testing\n\nCourse section 1: Fundamentals of Experimental Design\nCourse section 2: Statistical Methods\nCourse section 3: Advanced Topics & Real-World Implementation\nPrimary softwares: \n  \nBack-ups:"
  },
  {
    "objectID": "services.html#prods",
    "href": "services.html#prods",
    "title": "Work with Me",
    "section": "Products",
    "text": "Products\n\n&lt; Not available for now &gt;"
  },
  {
    "objectID": "services.html#tac",
    "href": "services.html#tac",
    "title": "Work with Me",
    "section": "Terms & Conditions",
    "text": "Terms & Conditions\n\n\n\n\n\n\nNoteEngagement & Pricing\n\n\n\n\n\nAll service fees are determined based on project scope, complexity, and timeline. Rates will be clearly communicated and agreed upon in writing before any work commences. Custom quotes are provided following an initial consultation or discovery call.\nAcademic Discount: Graduate students and non-profit research institutions qualify for a 20% discount on consulting services.\n\n\n\n\n\n\n\n\n\nNotePayment Terms\n\n\n\n\n\nConsulting & Development: A 50% deposit is required before project initiation, with the remaining balance due upon completion. Hourly consulting can be billed monthly for ongoing retainers.\nCourses: Full payment is required at registration. Course materials include a 14-day satisfaction guarantee from the first session date.\nFreelance Projects: Payment terms are negotiated per project, typically 30-50% upfront with milestone-based payments for larger engagements.\n\n\n\n\n\n\n\n\n\nNotePayment Methods\n\n\n\n\n\nCalendly Bookings: For consultation sessions and course registrations, payment is processed directly through Calendly at the time of booking.\nPayPal: International clients can pay via PayPal for added convenience. Link provided in invoice.\nWise (TransferWise): Preferred for international transfers with lower fees. Details shared upon request.\nInvoicing: All project-based work is formally invoiced via email with payment instructions and due dates.\n\n\n\n\n\n\n\n\n\nNoteScope & Revisions\n\n\n\n\n\nEach project includes reasonable revisions as outlined in the initial agreement. Minor adjustments and clarifications are provided at no additional charge. Significant scope changes, additional analyses, or new deliverables will require a revised quote and written approval before proceeding.\n\n\n\n\n\n\n\n\n\nNoteConfidentiality & Data Security\n\n\n\n\n\nAll client data, project details, and proprietary information are treated with strict confidentiality. I adhere to industry-standard data protection practices and will not disclose any information without explicit written consent. Upon request, a Non-Disclosure Agreement (NDA) can be executed prior to engagement.\n\n\n\n\n\n\n\n\n\nNoteProject Timelines\n\n\n\n\n\nTimelines are established during the initial consultation and confirmed in the project agreement. I commit to meeting agreed-upon deadlines and will communicate proactively if unforeseen circumstances arise.\nClient Responsibilities: Timely provision of data, feedback, and necessary resources is essential. Delays in client-side deliverables may impact project timelines and will be addressed collaboratively.\n\n\n\n\n\n\n\n\n\nNoteIntellectual Property\n\n\n\n\n\nClient Work: All custom deliverables, analyses, and reports created specifically for a client become the client’s property upon full payment.\nReusable Components: I retain the right to reuse general methodologies, code frameworks, and non-proprietary techniques developed during projects.\nOpen Source: For package development projects, licensing terms (MIT, GPL, etc.) will be agreed upon in advance.\n\n\n\n\n\n\n\n\n\nNoteCancellation Policy\n\n\n\n\n\nConsulting Sessions: Cancellations with less than 24 hours notice will be charged at 50% of the session fee. No-shows will be charged the full amount.\nCourses: Cancellations made 7+ days before the first session receive a full refund. Cancellations within 7 days are non-refundable but can be transferred to a future cohort.\nProject Work: Cancellation terms are negotiated per project. Completed work up to the cancellation point will be invoiced.\n\n\n\n\n\n\n\n\n\nNoteLimitation of Liability\n\n\n\n\n\nServices are provided with professional care and expertise. However, I cannot guarantee specific outcomes or results. Liability is limited to the total fees paid for the specific engagement. Clients are responsible for validating all analyses and recommendations within their specific context before implementation.\n\n\n\nQuestions about these terms? Contact me at joshua.marie.k@gmail.com"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Check out my blogs",
    "section": "",
    "text": "Get notified when I publish new posts. No spam, unsubscribe anytime"
  },
  {
    "objectID": "posts/index.html#subscribe-to-my-newsletter",
    "href": "posts/index.html#subscribe-to-my-newsletter",
    "title": "Check out my blogs",
    "section": "",
    "text": "Get notified when I publish new posts. No spam, unsubscribe anytime"
  },
  {
    "objectID": "posts/11-composing-r-function/index.html",
    "href": "posts/11-composing-r-function/index.html",
    "title": "Three levels to compose R functions",
    "section": "",
    "text": "Maybe few people who use R have forgotten already that R is functional by heart. R has Python dogma OO system, thanks to Reference Class (RC) and R6. R functions can be treated as Lisp’s macros, where it can let you meddle the function and abstract syntax tree (AST) of the function call.\nR has few ways to compose a function, divided by three (3) levels:"
  },
  {
    "objectID": "posts/11-composing-r-function/index.html#function-composition-with-purrrcompose",
    "href": "posts/11-composing-r-function/index.html#function-composition-with-purrrcompose",
    "title": "Three levels to compose R functions",
    "section": "\n2.1 Function Composition with purrr::compose()",
    "text": "2.1 Function Composition with purrr::compose()\nThe nice thing about this is that it takes multiple functions and creates a single new function that applies them in sequence (the default direction is backwards).\nSo, instead of writing this manually:\n\ntransform = function(x) {\n    sqrt(mean(log(x)))\n}\n\ntransform(1:5)\n\n[1] 0.9785184\n\n\nWe can compose it through purrr::compose():\n\npurrr::compose(sqrt, mean, log)(1:5)\n\n[1] 0.9785184\n\n\nFurthermore, this is also (almost) as readable as using pipe:\n\n1:5 |&gt; log() |&gt; mean() |&gt; sqrt()\n\n[1] 0.9785184\n\n\n\nI have a different blog mentioning on how bad can the nested function call go.\n\nThis function can be read from left to right (by default, it is read vice-versa):\n\npurrr::compose(sqrt, mean, log, .dir = \"forward\")(1:5)\n\n[1] 0.5166883\n\n\nWhich is an equivalent of:\n\n1:5 |&gt; sqrt() |&gt; mean() |&gt; log()\n\n[1] 0.5166883\n\n\nThis function can take the inverse: can be read from left to right (by default, it is read vice-versa):\n\ntransform_forward = purrr::compose(sqrt, mean, log, .dir = \"forward\")\ntransform_forward(1:5)\n\n[1] 0.5166883\n\n\nWhich is an equivalent of:\n\n1:5 |&gt; sqrt() |&gt; mean() |&gt; log()\n\n[1] 0.5166883"
  },
  {
    "objectID": "posts/11-composing-r-function/index.html#partial-application-with-purrrpartial",
    "href": "posts/11-composing-r-function/index.html#partial-application-with-purrrpartial",
    "title": "Three levels to compose R functions",
    "section": "\n2.2 Partial Application with purrr::partial()",
    "text": "2.2 Partial Application with purrr::partial()\nAnother function from purrr is the purrr::partial(), where the application is where you “pre-fill” some arguments of a function, creating a new function with fewer parameters. This is incredibly useful for creating specialized versions of general functions.\nConsider this function, where a number argument x is divided by 100:\n\ndivide_by_100 = function(x) {\n    x / 100\n}\n\ndivide_by_100(500)\n\n[1] 5\n\n\nWe can write as:\n\ndivide_by_100 = purrr::partial(`/`, e2 = 100)\ndivide_by_100(500)\n\n[1] 5\n\n\nIf you ask “what’s the point of partial application”, this can be particularly useful in conjunction with functionals and other function operators, and also supports rlang and NSE APIs — which means you can write it without “typing too much”."
  },
  {
    "objectID": "posts/11-composing-r-function/index.html#function-operators-in-general",
    "href": "posts/11-composing-r-function/index.html#function-operators-in-general",
    "title": "Three levels to compose R functions",
    "section": "\n2.3 Function Operators in general",
    "text": "2.3 Function Operators in general\nIn case you don’t know, decorators in Python are just “function operators” in general, except it is pretty much tied into Python and coated with syntactic sugars.\nFunction operators take functions as input and return modified functions as output. They’re like functions that operate on the “function space” rather than the data space.\n\n2.3.1 Example 1:\nFunction operators can add behavior without changing the core logic:\n\nwith_logging = function(f) {\n    function(...) {\n        cat(\"Calling function with args:\", paste(list(...), collapse = \", \"), \"\\n\")\n        res = f(...)\n        cat(\"Result:\", res, \"\\n\")\n        res\n    }\n}\n\nlogged_sqrt = with_logging(sqrt)\nlogged_sqrt(16)\n\nCalling function with args: 16 \nResult: 4 \n\n\n[1] 4\n\n\nIf you want to do this with Python, here take a look:\n\ndef with_logging(f):\n    def call(*args):\n        args_str = \", \".join(str(a) for a in args)\n        print(f\"Calling function with args: {args_str}\")\n        res = f(*args)\n        print(f\"Result: {res}\")\n        return res\n    \n    return call\n\n\n@with_logging\ndef logged_sqrt(x):\n    return x ** 0.5\n    \nprint(logged_sqrt(16))\n\nCalling function with args: 16\nResult: 4.0\n4.0\n\n\n\n2.3.2 Example 2: Creating a Memoization Operator\nMemoization caches function results to speed up repeated calls:\n\nmemoize = function(f) {\n    cache = new.env(parent = emptyenv())\n    \n    function(...) {\n        key = paste(list(...), collapse = \"_\")\n        if (!exists(key, envir = cache)) {\n            cache[[key]] = f(...)\n        }\n        cache[[key]]\n    }\n}\n\nslow_function = function(x) {\n    Sys.sleep(1) \n    x^2\n}\n\nfast_function = memoize(slow_function)\nsystem.time(fast_function(5))\n\n   user  system elapsed \n   0.00    0.00    1.01"
  },
  {
    "objectID": "posts/11-composing-r-function/index.html#start-with-the-basic-first",
    "href": "posts/11-composing-r-function/index.html#start-with-the-basic-first",
    "title": "Three levels to compose R functions",
    "section": "\n3.1 Start with the basic first",
    "text": "3.1 Start with the basic first\nThe new_function() function takes three arguments:\n\n\nargs: The formal arguments under that function\n\nbody: An expression that takes the function body\n\nenv: If supplied, this will be the parent environment of the function.\n\nLet’s start with a function that squares the number:\n\nbox::use(\n    rlang[new_function, expr, caller_env, call2]\n)\n\nsquare = new_function(\n    args = list(x = expr()), \n    body = call2(\"{\", quote(x^2)), \n    env = caller_env()\n)\n\nsquare(5)\n\n[1] 25\n\n\nOr instead of list() in args parameter, how about using pairlist2() instead?\n\nbox::use(rlang[pairlist2])\n\nsquare2 = new_function(\n    args = pairlist2(x = ), \n    body = expr(x^2), \n    env = caller_env()\n)\n\nsquare2(5)\n\n[1] 25\n\n\nAlright, you might be asking: What’s the point of using new_function() if we can just use function() instead? The purpose of this approach is that you can programmatically generate functions based on data or configuration.\nIn fact, this is how I automatically generate torch::nn_module() expression used in my new R package called kindling:\n\nbox::use(kindling[ffnn_generator, act_funs, args])\n\nffnn_generator(\n    nn_name = \"MyFFNN2\",\n    hd_neurons = c(128, 64, 32, 15),\n    no_x = 20,\n    no_y = 5,\n    activations = act_funs(\n        relu,\n        selu,\n        softshrink = args(lambd = 0.5),\n        \"celu(alpha = 0.5)\"\n    )\n)\n\ntorch::nn_module(\"MyFFNN2\", initialize = function () \n{\n    self$fc1 = torch::nn_linear(20, 128, bias = TRUE)\n    self$fc2 = torch::nn_linear(128, 64, bias = TRUE)\n    self$fc3 = torch::nn_linear(64, 32, bias = TRUE)\n    self$fc4 = torch::nn_linear(32, 15, bias = TRUE)\n    self$out = torch::nn_linear(15, 5, bias = TRUE)\n}, forward = function (x) \n{\n    x = self$fc1(x)\n    x = torch::nnf_relu(x)\n    x = self$fc2(x)\n    x = torch::nnf_selu(x)\n    x = self$fc3(x)\n    x = torch::nnf_softshrink(x, lambd = 0.5)\n    x = self$fc4(x)\n    x = torch::nnf_celu(x, alpha = 0.5)\n    x = self$out(x)\n    x\n})\n\n\nIn which, this function is a reference from this blog post of mine quite a while ago, based on what I learned in Advanced R to automatically generate torch::nn_module()."
  },
  {
    "objectID": "posts/09-ggplot2/index.html",
    "href": "posts/09-ggplot2/index.html",
    "title": "Things you may or you may not know in ggplot2",
    "section": "",
    "text": "1 Brief Introduction\nThe ggplot2 package is now over 18 years old (released way back in 2007) and has 2.2 million downloads per month, based on cranlogs. Thank you Hadley Wickham and co. for adapting “The Grammar of Graphics” by Leland Wilkinson into R programming language.\nThe things to be enumerated here are some brand new features and some hidden in plain sight. Here are my compilations of things you may or you may not know in ggplot2. But first, let us load the package through box:\n\nbox::use(\n    ggplot2[...]\n)\n\n\n2 1. Starting from 4.0.0, S7 is the new imported OO system\nThe 4.0.0 release of ggplot2 is now on CRAN few months ago (followed by 4.0.1 — published 18 days ago on November 14, 2025). This release introduces several significant updates, including a major internal shift: ggplot2 now imports the S7 object-oriented system, a new OO system, developed by the Object-Oriented Programming Working Group, R Consortium.\nWith this transition, several core internal objects, such as ggplot() and layer(), are now implemented using S7. However, the public-facing API remains largely S3-based, meaning that most users will experience the package exactly as before, with no changes required in existing code.\n\nclass({\n    ggplot(mtcars, aes(x = mpg, y = hp)) +\n        geom_point()\n})\nCheck out its class\n[1] \"ggplot2::ggplot\" \"ggplot\"          \"ggplot2::gg\"     \"S7_object\"      \n[5] \"gg\"             \n\n\n\nIf you’re upgrading old projects, expect a few things to break (especially if you relied on internal structure). Lock your version with renv or groundhog until you’re ready.\nThis move to S7 represents the most substantial internal redesign of ggplot2 since the introduction of ggproto in 2015.\n\n3 2. It has its own OO system: ggproto\nEven with the latest 4.0.0 release, the actual layers, scales, coords, etc. are still held by ggproto objects. This is what makes ggplot2 so extensible. This OO system allows reference semantics and inheritance method — a classic OO programming scheme.\n\nclass(ggplot2::GeomPoint)\nCheck out its class\n[1] \"GeomPoint\" \"Geom\"      \"ggproto\"   \"gg\"       \n\n\n\nWanna know how geom_smooth() secretly uses loess or gam? Just look at ggplot2:::StatSmooth$compute_group.\n\n4 3. It has 5 ggplot2-based interactive viz\nThere are R packages that makes ggplot2 interactive, at least several:\n\n\n\n\n\n\n\n\nPackage\nStrength\nFidelity to static ggplot\nLearning curve (verdict)\n\n\n\nplotly\nDynamic and one-liner using ggplotly()\n\nMedium (some features lost)\nLow (1 star / easy to use)\n\n\nggiraph\nGiving full control over tooltip / hover / click\nPerfect\nVery High (4 stars / difficult for non-native ggplot2 users)\n\n\nggiraphExtra\nShiny-integrated version of ggiraph\nHigh\nFairly Low (2 stars / quick interactive)\n\n\ngginteractive\nMinimal code changes, HTMLwidget backend\nHigh\nFairly Low (2 stars / not actively maintained)\n\n\nggvis (now a legacy)\nOnce the future, now archived\nNULL (I don’t use it)\nNULL (I don’t use it)\n\n\nvegafacet (2025)\nVega-Lite\nFairly Low (not even ggplot2, but close)\nSeriously high (5 stars / still new)\n\n\n\nA quick showcase of ggiraph:\n\nClick to see the codebox::use(\n    ggiraph[geom_point_interactive, girafe, opts_hover, opts_hover_inv], \n    tibble[rownames_to_column], \n    glue[glue]\n)\n\np = \n    mtcars |&gt; \n    rownames_to_column(var = \"car\") |&gt; \n    ggplot(aes(wt, mpg, color = factor(cyl))) +\n    geom_point_interactive(\n        aes(\n            tooltip = glue(\"{car} (wt: {wt}; mpg: {mpg})\"), \n            data_id = car\n        ), \n        size = 4\n    ) +\n    theme_minimal(base_size = 14)\n\ngirafe(\n    ggobj = p,\n    width_svg = 10,\n    height_svg = 6,\n    options = list(\n        opts_hover(css = \"fill:yellow;stroke:black;stroke-width:2;cursor:pointer;\"),\n        opts_hover_inv(css = \"opacity:0.3;\")\n    )\n)\n\n\n\n\n\n\n5 4. Customized fonts\nFont styles to be used in your plot is not limited to the fonts you had in your system — this includes the font found in the web, such as Google fonts.\nThanks to sysfonts, you can do:\n\nsysfonts::font_add_google(\"Lobster\", \"lobster\")\nsysfonts::font_add_google(\"Roboto Condensed\", \"roboto\")\nshowtext::showtext_auto()\n\nThen use it as the base font family of the whole plot:\n\nClick to see the codeggplot(mtcars, aes(mpg, hp)) +\n    geom_point(color = \"#C70039\", size = 3) +\n    labs(\n        title = \"This title is in Lobster font\",\n        subtitle = \"And the rest of the text too if you want\"\n    ) +\n    theme_minimal(base_size = 20, base_family = \"lobster\") +\n    theme(\n        plot.subtitle = element_text(color = \"grey40\", size = 13),\n        panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(color = \"grey90\", linewidth = 0.3),\n        axis.title = element_text(face = \"bold\"),\n        axis.text = element_text(color = \"grey30\")\n    )\n\n\n\n\n\n\n\n\n6 5. ggplot2 works perfectly without a data frame\nR’s metaprogramming and its environment semantics is so handy but sometimes quirky: they’re so noble in data analysis and statistical modelling, but sometimes they’re so hard to predict. In many cases when it comes to working with data in R, NSE or the non-standard evaluation is used and one of the core features in tidyverse.\nMany tutorials and documentations show how to use ggplot2 with data frames, but do you know that it is still fine to use ggplot2 without providing a data frame?\nThe demo:\n\nClick to see the codeset.seed(123)\nx = rnorm(100)\ny = rnorm(100) + 0.5 * x\n\nggplot() + # This is always required! No need to put a data frame\n    aes(x, y) + \n    geom_point(\n        color = \"#002455\",\n        size = 2.5,\n        alpha = 0.7,\n        shape = 16\n    ) +\n    geom_smooth(\n        method = \"lm\", \n        se = TRUE,  \n        color = \"#E7DEAF\",\n        fill = \"#E7DEAF\",\n        alpha = 0.2,\n        linewidth = 1.2\n    ) +\n    labs(\n        x = \"X-axis\",\n        y = \"Y-axis\",\n        title = \"Scatter Plot with Linear Regression\",\n        subtitle = paste0(\n            \"Pearson's r = \", round(cor(x, y), 3), \", \", \n            \"n = \", length(x)\n        )\n    ) + \n    scale_x_continuous(\n        breaks = seq(-3, 3, 1),\n        limits = c(-3, 3)\n    ) +\n    scale_y_continuous(\n        breaks = seq(-3, 3, 1)\n    ) +\n    theme_minimal(base_size = 12, base_family = \"lobster\") +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(color = \"grey40\", size = 10),\n        panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(color = \"grey90\", linewidth = 0.3),\n        axis.title = element_text(face = \"bold\"),\n        axis.text = element_text(color = \"grey30\"),\n        plot.margin = margin(15, 15, 15, 15)\n    )\n\n\n\n\n\n\n\nAlthough I still recommend providing a data frame since that’s how ggplot2 is designed to be used — especially in many tutorials and documentations.\n\n7 6. The art of delay\nYou may haven’t encounter this but there are 3 particular functions that manipulates aesthetic evaluation:\n\n\nstage() lets you delay the evaluation of aesthetics until the plot is drawn.\n\n\n\n\n\n\nTipSimple demo\n\n\n\n\n\n\nggplot(mpg, aes(x = displ)) +\n    geom_bar(\n        aes(\n            y = stage(\n                start = hwy,             \n                after_scale = y * 2      \n            )\n        ),\n        stat = \"identity\",\n        fill = \"#4A90E2\",\n        alpha = 0.7\n    ) +\n    labs(\n        title = \"Using stage() to modify values after scaling\",\n        subtitle = \"Y-axis values are doubled after ggplot2's internal scaling\",\n        x = \"Engine Displacement (L)\",\n        y = \"Highway MPG (scaled × 2)\"\n    ) +\n    theme_minimal(base_family = \"roboto\", base_size = 13) +\n    theme(\n        plot.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank()\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nafter_stat() enables you to modify aesthetics based on computed statistics after they have been calculated.\n\n\n\n\n\n\nTipSimple demo\n\n\n\n\n\n\nggplot(mpg, aes(x = hwy)) +\n    geom_histogram(\n        aes(\n            y = after_stat(density),  \n            fill = after_stat(count)  \n        ),\n        bins = 30,\n        color = \"white\",\n        linewidth = 0.2\n    ) +\n    scale_fill_viridis_c(option = \"plasma\") +\n    labs(\n        title = \"Histogram with after_stat()\",\n        subtitle = \"Y-axis shows density, fill color shows count\",\n        x = \"Highway MPG\",\n        y = \"Density\",\n        fill = \"Count\"\n    ) +\n    theme_minimal(base_size = 14, base_family = \"roboto\") +\n    theme(\n        plot.title = element_text(face = \"bold\"),\n        legend.position = \"right\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nafter_scale() allows you to modify aesthetics after they have been scaled but before they are rendered.\n\n\n\n\n\n\nTipSimple demo\n\n\n\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n    geom_point(\n        aes(\n            color = hp,\n            alpha = after_scale(scales::rescale(as.numeric(factor(color)), to = c(0.3, 1)))\n        ),\n        size = 5\n    ) +\n    scale_color_viridis_c(option = \"magma\", end = 0.9) +\n    labs(\n        title = \"Using after_scale() for dynamic transparency\",\n        subtitle = \"Point transparency adjusted based on scaled color values\",\n        x = \"Weight (1000 lbs)\",\n        y = \"Miles per Gallon\",\n        color = \"Horsepower\"\n    ) +\n    theme_minimal(base_size = 14, base_family = \"roboto\") +\n    theme(\n        plot.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank()\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nThese are particularly useful when you want to create plots that depend on dynamic data or when you want to modify aesthetics based on user input in interactive applications.\nThis feature in ggplot2 is available in version 3.5.0 and later.\n\n8 7. Markdown within {ggplot2} text elements\nWhat I like about ggtext is the ability to use markdown syntax within ggplot2 text elements, such as titles, subtitles, captions, and axis labels. This means you can easily format your text to include bold, italic, code, and even links directly within your plots.\n\nClick to see the codebox::use(\n    ggtext[element_markdown], \n    dplyr[mutate], \n    tidyr[long = pivot_longer], \n    stringr[replace_all = str_replace_all, to_title = str_to_title]\n)\n\niris_colors = c(\n    setosa = \"#D83F87\",   \n    versicolor = \"#36E2A3\",   \n    virginica = \"#7B68EE\"    \n)\n\niris |&gt;\n    mutate(\n        Species = factor(\n            Species,\n            levels = c(\"setosa\", \"versicolor\", \"virginica\"))\n    ) |&gt;\n    long(\n        cols = where(is.numeric),\n        names_to  = \"features\",\n        values_to = \"sizes\"\n    ) |&gt;\n    mutate(\n        features = to_title(replace_all(features, \"\\\\.\", \" \"))\n    ) |&gt;\n    ggplot(aes(x = Species, y = sizes, color = Species, fill = Species)) +\n    geom_violin(alpha = 0.65, linewidth = 0.6) +                  \n    geom_boxplot(\n        width = 0.25, \n        outlier.alpha = 0,\n        linewidth = 0.6, \n        fill = \"white\"\n    ) +     \n    ggforce::geom_sina(\n        alpha = 0.5, \n        size = 2.2, \n        shape = 21,\n        color = \"gray20\", \n        stroke = 0.4\n    ) +\n    scale_color_manual(values = iris_colors) +\n    scale_fill_manual(values = iris_colors) +\n    scale_y_continuous(labels = scales::label_number(accuracy = 0.1)) +\n    facet_wrap(~ features, scales = \"free_y\", ncol = 2) +\n    labs(\n        title    = \"&lt;b&gt;Iris Dataset – Distribution of Measurements by Species&lt;/b&gt;\",\n        subtitle = paste0(\n            \"Species: &lt;span style='color:#D83F87;'&gt;**setosa**&lt;/span&gt; | \",\n            \"&lt;span style='color:#36E2A3;'&gt;**versicolor**&lt;/span&gt; | \",\n            \"&lt;span style='color:#7B68EE;'&gt;**virginica**&lt;/span&gt;\"\n        ),\n        x = NULL,\n        y = NULL\n    ) +\n    theme_classic(base_family = \"lobster\") +\n    theme(\n        plot.title = element_markdown(hjust = 0.5, size = 18),\n        plot.subtitle = element_markdown(\n            hjust = 0.5, size = 13, margin = margin(b = 20)\n        ),\n        strip.text = element_text(size = 13, face = \"bold\", color = \"gray30\"),\n        strip.background = element_rect(fill = \"gray95\", color = NA),\n        axis.text.x = element_blank(), # element_text(size = 12)\n        axis.text.y = element_text(size = 11),\n        axis.ticks.x = element_blank(),\n        panel.grid.major.y = element_line(color = \"gray88\", linewidth = 0.4),  \n        # panel.grid.minor.y = element_line(color = \"gray94\", linewidth = 0.25), \n        legend.position = \"none\"\n    )\n\n\n\n\n\n\n\nThe element_markdown() function replaces standard theme text elements and parses markdown/HTML. You can use:\n\nMarkdown: **bold**, *italic*, `code`\n\nHTML tags: &lt;span&gt;, &lt;br&gt;, &lt;sup&gt;, &lt;sub&gt;, and more\nInline CSS: &lt;span style='color:red;'&gt;colored text&lt;/span&gt;\n\n\n9 Resources\n\nggplot2 Official Documentation\nggplot2 Book by Hadley Wickham\nggplot2 GitHub Repository\nR Graphics Cookbook by Winston Chang\nshowtext Package Documentation\nggpattern Package\nggiraph Package\nS7 OOP System\nggplot2 Extensions Gallery\nCedric Scherer’s ggplot2 Tutorials"
  },
  {
    "objectID": "posts/07-glm/index.html",
    "href": "posts/07-glm/index.html",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "",
    "text": "Stop asking:\nKidding~\nI have no such problem of you guys asking these questions. This type of question, however, is a type of question that I heard many times already. I mean, I understand why there’s so many misconceptions, and it could be they are not properly taught or…I don’t know.\nI have no problems with tutorials and blogs online about teaching statistics, like predicting what species of iris is gonna be, or how likely you’ll get ebola. There are ten thousand blog posts teaching you how to predict iris species with glm(), or Python’s sklearn.linear_model.LogisticRegression()\nToday we’re going full math gremlin mode and actually understanding why GLMs exist, why linear regression is a lie for most real data, and why the link function is the most genius hack in 20th-century statistics."
  },
  {
    "objectID": "posts/07-glm/index.html#lets-start-with-an-overview",
    "href": "posts/07-glm/index.html#lets-start-with-an-overview",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n1 Let’s start with an overview",
    "text": "1 Let’s start with an overview\n\nWhat did you observe on the mathematical model formula of linear regression and generalized linear model (GLM)?\nTake a look:\nLinear Regression:\n\\[Y = \\mathbf{X}\\beta\\ +\\ \\epsilon\\]\nwhere \\(\\mathbf{X}\\beta\\) expands to: \\[\\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_3\\beta_3 + \\dots + x_n\\beta_n\\]\nHow about on GLM?\n\\[\ng(\\mu_i) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta} = \\eta_i \\quad \\text{(linear predictor)}\n\\]\n\\[g(\\mu)=\\eta=\\mathbf{X}\\beta\\]\nboth are taking the account of the linear model, where the response have linear relationship with the predictors, taken account by \\(\\mathbf{X}\\beta\\).\nThere are few exceptions:\n\nGLMs can take handle of the range not only lies on the real number, but with some strict intervals, unlike Linear regression who has always a range lies on the real number.\n\nGLMs doesn’t have an error term. Why is that? GLMs takes considerations on the expected value of the linear model, and when you take the expected value like this:\n\\[E(Y)=E(\\mathbf{X}\\beta\\ + \\ \\epsilon)\\]\nThe expected values \\(E(\\mathbf{X}\\beta) = \\mathbf{X}\\beta\\) (assuming no randomness in both terms here) and \\(E(\\epsilon) = 0\\). Hence:\n\\[E(Y)=\\mathbf{X}\\beta\\]\nand GLMs only care about the expected value\n\n\n\n1.1 But wait, what’s this \\(g(\\mu)\\) doing here?\nYes, it’s odd, but hey, it just works.\nThere’s this thing called link function, and it’s the reason GLMs exist. Let me break down why we need this weird little function:\nIn linear regression, the mean of the model would be: \\[\\mu = E(Y) = \\mathbf{X}\\beta\\]\nOkay, this definitely feels like “Hey, it works, okay?”, but that’s my intuition about GLMs. I hope you understood a little in this part. Anyways, that’s the mean of the linear regression that we know of, and this works great when \\(y\\) can be any real number. But, I am asking you a question:\nWhat if your outcome isn’t allowed to be just any real number?\n\nWhat if \\(Y\\) is a count \\((0, 1, 2, 3, ...)\\)? Let’s say number of kids in a family — \\(\\mathbf{X}\\beta\\) could give you -3.7 customers\nWhat if \\(Y\\) is a probability (0 to 1)? Probability of rain? Between 0 and 1, inclusive, you monster\nWhat if \\(Y\\) is binary (0 or 1)? Let’s say “whether someone clicks your ad” — \\(\\mathbf{X}\\beta\\) could give you 0.3, and it can’t be 1.337, you know.\n\nThis strange thing-y in GLMs, link function \\(g(\\cdot)\\), solves this by transforming the mean so that the linear predictor \\(\\eta = \\mathbf{X}\\beta\\) can be any real number, while \\(\\mu\\) stays in the valid range.\nUnderstand? No? Alright, let’s say, the output of \\(\\eta\\), which is an equivalent of \\(\\mathbf{X}\\beta\\), should be at least close to the value in \\(g(\\mu)\\), which is an equivalent of \\(g(E(y))\\).\nMathematically: \\[g(\\mu) = \\eta = \\mathbf{X}\\beta\\]\nSo the inverse is: \\[\\mu = g^{-1}(\\eta) = g^{-1}(\\mathbf{X}\\beta)\\]\nAgain, to understand, we fit the model in “transformed space” where everything is linear, then transform back to get predictions in the correct range.\nList of common link functions:\n\n\n\n\n\n\n\n\n\nLink\n\\(g(\\mu)\\)\n\\(\\mu = g^{-1}(\\eta)\\)\nUsed for\nWhy\n\n\n\nIdentity\n\\(\\mu\\)\n\\(\\eta\\)\nNormal/Gaussian\nNo transformation needed\n\n\nLogit\n\\(\\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\)\n\\(\\frac{1}{1+e^{-\\eta}}\\)\nBinomial/Binary\nMaps \\((0,1) \\to (-\\infty, \\infty)\\)\n\n\n\nLog\n\\(\\log(\\mu)\\)\n\\(e^{\\eta}\\)\nPoisson/Counts\nMaps \\((0, \\infty) \\to (-\\infty, \\infty)\\)\n\n\n\nProbit\n\\(\\Phi^{-1}(\\mu)\\)\n\\(\\Phi(\\eta)\\)\nBinomial\nFor masochists\n\n\n\n1.2 Why Can’t We Just Minimize Squared Errors?\nIn linear regression, we minimize the sum of squared residuals (SSR): \\[SSR = \\sum_{i}(Y_i - \\hat{Y}_i)^2 = \\sum_{i}(Y_i - \\mathbf{X}\\beta)^2\\]\nThis has a nice closed-form solution and works because:\n\nThe errors are approximately normally distributed\nThe variance is constant (a.k.a. homoscedasticity)\nMinimizing SSR is equivalent to maximum likelihood estimation\n\nBut in GLMs, these assumptions break down:\n\nBinomial data has variance \\(\\mu(1-\\mu)\\), which depends on the mean\nPoisson data has variance equal to the mean \\(\\mu\\)\n\nThe response isn’t always in real number or approximately normally distributed (even with the presence of CLT)\n\nSo we need the alternative approach: Maximum Likelihood Estimation — this is the alternative estimation method for Linear regression.\nIn GLM, it will be proven difficult to estimate the optimal solution as the use case will become different. You see the table above? There are actually plenty of them.\n\n1.3 The Three Sacred Components of GLM\nEvery GLM has exactly three components. Forget one at your dissertation defense, I dare you:\n\n\nRandom Component (Distribution Family)\nYour response \\(Y\\) has a likelihood function, expressed in an exponential family distribution. The fancy math looks like: \\[f(Y; \\theta, \\phi) = \\exp\\left\\{\\frac{Y\\theta - b(\\theta)}{a(\\phi)} + c(Y, \\phi)\\right\\}\\]\nTranslation: Normal, Binomial, Poisson, Gamma — they’re all in this family because they can be written in this form.\nWhy do we care? Because exponential family distributions have nice properties:\n\n\n\\(E[Y] = b'(\\theta)\\) (the mean is the derivative of that \\(b\\) function)\n\n\\(\\text{Var}(Y) = b''(\\theta) \\cdot a(\\phi)\\) (variance is also derived from \\(b\\))\n\n\n\nSystematic Component (Linear Predictor)\n\\[\\eta = \\mathbf{X}\\beta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots\\]\nThis is just your boring linear combination of predictors. Nothing fancy. We’re still doing regression.\n\n\nLink Function\n\\[g(\\mu) = \\eta\\]\nConnects your linear predictor \\(\\eta\\) (which lives in \\((-\\infty, \\infty)\\)) to your mean response \\(\\mu\\) (which might live in \\((0, 1)\\) or \\([0, \\infty)\\) or wherever your response is allowed to be).\n\n\n1.4 Why Linear Regression is Just a Special Case\nNotice that if you:\n\nUse the Normal distribution (random component)\nIts identity link \\(g(\\mu) = \\mu\\) (link function)\nKeep the linear predictor \\(\\eta = \\mathbf{X}\\beta\\) (systematic component)\n\nYou get just it:\n\\[\\mu = \\mathbf{X}\\beta\\]\nwhich itself is equivalent to \\(E(y)\\)\nWhich is just…your good old ordinary regression:\n\n\n\n\n\n\nNoteIn R\n\n\n\n\n\nlm(y ~ x, data = data)\n\n\n\n\n\n\n\n\n\nNoteIn Python\n\n\n\n\n\nimport statsmodels.formula.api as smf\n\nsmf. \\ \n    ols(\"y ~ x\", data = data). \\\n    fit(). \\ \n    summary()\n\n\n\nLinear regression is a GLM with no identity, literally and figuratively."
  },
  {
    "objectID": "posts/07-glm/index.html#math",
    "href": "posts/07-glm/index.html#math",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n2 In-depth mathematics and computation",
    "text": "2 In-depth mathematics and computation\n\nWhat mathematics do I need to know more?\nNow, you know it takes a lot to optimize the GLM. What are the things you need to know?\nFirst, in Linear regression, we have two ways to estimate the coefficients: MLE and ordinary least squares (OLS), which also can be derived and closed form from the maximum likelihood of the normal distribution. In GLM, we cannot use OLS, thus no closed form solution and MLE is the only main solution.\nThen for the extra part, which we talk about the deviance. Imagine this as the residual sum of squares in GLMs.\n\n2.1 Maximum Likelihood Estimation\nBut how do we actually estimate \\(\\beta\\)? We use maximum likelihood estimation (MLE), but considering there are many forms of likelihood functions, derived from several probability distributions came from exponential family, and so there’s several link functions to be derived.\nThe likelihood for a GLM is:\n\\[L(\\beta) = \\prod_{i=1}^n f(y_i; \\theta_i, \\phi)=\\prod_{i=1}^n\\exp\\left\\{\\frac{y_i\\theta_i - b(\\theta_i)}{a(\\phi)} + c(y_i, \\phi)\\right\\}\\]\nFancy, isn’t it?\nSIKE!!\n\nYou don’t want this, trust me. This thing is so hard to calculate, even the computer starts to act crazy if you really direcly executed a program for this. Direct maximization of this product is computationally difficult. Taking logarithms simplifies things enormously (thanks, John Napier!).\nA sane person would only like what’s the easier to eat — The log-likelihood function for the GLM instead:\n\\[\\ell(\\beta) = \\sum_{i=1}^n \\left\\{\\frac{y_i\\theta_i - b(\\theta_i)}{a(\\phi)} + c(y_i, \\phi)\\right\\}\\]\nNow, this is much easier to work with because:\n\nProducts become sums\nIt’s numerically more stable\nThe maximum is the same (logarithm is monotonic, after all)\n\n2.2 Fisher Scoring / IRLS Algorithm (The Actual Work)\nSince, there’s no actual closed form to calculate the coefficients, we can’t solve \\(U(\\beta) = 0\\) directly. There’s a method we can use called Iteratively Reweighted Least Squares (IRLS), which is just Newton-Raphson but with the Fisher Information Matrix, denoted as \\(W\\).\nBefore starting, we need to assign a variable that arbitrarily assigns design matrix X, the response vector y, and then determines the number of predictors p, the number of observations n, the maximum iteration max_iter, and the tolerance eps.\n\n\nIn R\nIn Python\n\n\n\nn = nrow(X)\np = ncol(X)\nmax_iter = 100\neps = 1e-7\n\n\nn = X.shape[0]\np = X.shape[1]\nmax_iter = 100\neps = 1e-7  \n\n\n\nHere are the steps:\n\n2.2.1 1. The Algorithm:\n\nStart with initial values \\(\\beta^{(0)}\\) (usually from an unweighted regression or just zeros).\nThis is how you program it in R and Python:\n\n\nR\nPython\n\n\n\nbeta = matrix(0, nrow = p, ncol = 1)\n\n\nimport numpy as np\n\nbeta = np.zeros((p, 1))\n\n\n\n\n2.2.2 2. Repeat until convergence:\n\nIn this step, the extra steps, i.e.:\n\nCalculate linear predictor: \\(\\eta^{(t)} = \\mathbf{X}\\beta^{(t)}\\)\nCalculate fitted values: \\(\\mu^{(t)} = g^{-1}(\\eta^{(t)})\\)\n\nCalculate weights:\n\\[w_i^{(t)} = \\frac{1}{\\text{Var}(Y_i)} \\left(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\right)^2\\]\n\n\nCalculate working response:\n\\[z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)}) \\frac{\\partial \\eta_i}{\\partial \\mu_i}\\]\n\n\nUpdate coefficients by solving weighted least squares:\n\\[\\beta^{(t+1)} = (\\mathbf{X}^T W^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T W^{(t)} z^{(t)}\\]\n\nCheck convergence: If \\(|\\beta^{(t+1)} - \\beta^{(t)}| &lt; \\epsilon\\), stop. Else, go back to step 1.\n\n\n\nR\nPython\n\n\n\nfor (i in 1:max_iter) {\n    # 1. Calculate linear predictor\n    eta = X %*% beta\n    \n    # 2. Calculate fitted values\n    mu = family$linkinv(eta)\n    \n    # 3. Calculate weights\n    V = as.vector(family$variance(mu))\n    gradient = as.vector(family$mu.eta(eta))\n    w_vec = (gradient^2) / V\n    \n    # 4. Working response\n    z = as.vector(eta) + (as.vector(y) - mu) / gradient\n    \n    # 5. Update coefficients by solving weighted least squares\n    W = diag(as.vector(w_vec), n, n)\n    beta_new = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% z\n    \n    # 6. Check convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n        beta = beta_new\n        converged = TRUE\n        break\n    }\n    \n    # Final beta\n    beta = beta_new\n}\n\n\nfor i in range(max_iter):\n    # 1. Calculate linear predictor\n    eta = X @ beta\n    \n    # 2. Calculate fitted values\n    mu = family.linkinv(eta)\n    \n    # 3. Calculate weights\n    V = family.variance(mu).flatten()\n    gradient = family.mu_eta(eta).flatten()\n    w_vec = (gradient ** 2) / V\n    \n    # 4. Working response\n    z = eta + (y - mu) / gp\n    \n    # 5. Update coefficients by solving weighted least squares\n    W = np.diag(w_vec)\n    beta_new = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ z)\n    \n    # 6. Check convergence\n    if np.max(np.abs(beta_new - beta)) &lt; tol:\n        beta = beta_new\n        converged = True\n        break\n    \n    # Final beta\n    beta = beta_new\n\n\n\n\n2.3 Be deviant: Deviance (GLM’s Version of Sum of Squares)\nTo be frank, I am not planning to include this, but I was thinking if anyone is curious on how the fitness in GLMs is being measured. In Linear regression, we measure model fit by summarizing the residuals using Residual Sum of Squares (RSS). It is so odd for GLM to use other way to measure (well, the key term is “Generalized”, after all) — Instead, GLMs use deviance:\n\\[D = 2[\\ell(\\text{saturated model}) - \\ell(\\text{fitted model})]\\]\nThe saturated model has \\(\\hat{\\mu}_i = y_i\\) (perfect fit). Deviance measures how much worse your model is than this.\nFor specific families:\n\n\nNormal / Gaussian: \\(D = \\sum(y_i - \\hat{\\mu}_i)^2\\) (literally just RSS)\n\nPoisson: \\(D = 2\\sum\\left[y_i\\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) - (y_i - \\hat{\\mu}_i)\\right]\\)\n\n\nBinomial: \\(D = 2\\sum\\left[y_i\\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) + (n_i-y_i)\\log\\left(\\frac{n_i-y_i}{n_i-\\hat{\\mu}_i}\\right)\\right]\\)\n\n\nLower deviance = better fit."
  },
  {
    "objectID": "posts/07-glm/index.html#full-function-implementation",
    "href": "posts/07-glm/index.html#full-function-implementation",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n3 Full Function implementation",
    "text": "3 Full Function implementation\nThe main programming languages are still R and Python. The functions I implemented expects to be brittle and not applicable for some cases.\nThe functions are implemented in this source code."
  },
  {
    "objectID": "posts/07-glm/index.html#show-me-the-money-mtcars-logistic-regression-showdown",
    "href": "posts/07-glm/index.html#show-me-the-money-mtcars-logistic-regression-showdown",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n4 Show Me the Money: mtcars Logistic Regression Showdown",
    "text": "4 Show Me the Money: mtcars Logistic Regression Showdown\nOkay, enough theory — show me the money!\nNow, we can implement a full function only from In-depth mathematics and computation. That is, if you only need the coefficients. Use any dataset you want, but on our example, we’ll use the mtcars dataset for the simpletons.\nWhy mtcars? Because every R tutorial on earth has used it at least once, and I’m not about to break tradition, am I? We’ll predict whether a car has an automatic (0) or manual (1) transmission (am) using horsepower (hp), and weight (wt). Classic binary outcome -&gt; logistic regression -&gt; GLM with binomial family and logit link.\nWe’ll do it three ways so you can flex at parties:\n\nThe lazy way (stats::glm() in R / {statsmodels} in Python)\nThe “I Implemented IRLS at 3 AM” way (calling our own IRLS function in this part)\n\n\n4.1 Tier 1: The lazy way (everyone does this)\nThis is the “why should I bother making myself one if there’s one existed already” part. R has built-in glm() from {stats} package, while Python has {statsmodels} package, ideally under statsmodels.api, not under statsmodels.formula.api, but for our example, we can use that instead.\n\n\nR\nPython\n\n\n\n\nfit_r = glm(am ~ hp + wt, data = mtcars, family = binomial(link = \"logit\"))\nsummary(fit_r)\n\n\nCall:\nglm(formula = am ~ hp + wt, family = binomial(link = \"logit\"), \n    data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) 18.86630    7.44356   2.535  0.01126 * \nhp           0.03626    0.01773   2.044  0.04091 * \nwt          -8.08348    3.06868  -2.634  0.00843 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 10.059  on 29  degrees of freedom\nAIC: 16.059\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.genmod.families.links import Logit\nimport statsmodels.formula.api as smf\n\n# mtcars = sm.datasets.get_rdataset(\"mtcars\").data\nmtcars = r.mtcars\n\nfit_py = smf \\\n    .glm(\n        \"am ~ hp + wt\",\n        data=mtcars,\n        family=Binomial(link=Logit())\n    ) \\\n    .fit()\n\nprint(fit_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                     am   No. Observations:                   32\nModel:                            GLM   Df Residuals:                       29\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -5.0296\nDate:                Thu, 11 Dec 2025   Deviance:                       10.059\nTime:                        14:35:33   Pearson chi2:                     15.0\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.6453\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     18.8663      7.444      2.535      0.011       4.277      33.455\nhp             0.0363      0.018      2.044      0.041       0.001       0.071\nwt            -8.0835      3.069     -2.634      0.008     -14.098      -2.069\n==============================================================================\n\n\n\n\n\n\n4.2 Tier 2: The “I Implemented IRLS at 3 AM” way\n\n\nR\nPython\n\n\n\n\nView the source code\n\n\nbox::use(\n    ./glm[glm_custom]\n)\n\nglm_custom(am ~ hp + wt, data = mtcars, family = binomial())\n\n$coefficients\n[1] 18.8662987  0.0362556 -8.0834752\n\n$converged\n[1] TRUE\n\n$iterations\n[1] 9\n\n\n\n\n\nView the source code\n\n\nfrom glm import glm_custom\nimport glm\n\nprint(glm_custom('am ~ hp + wt', data = mtcars, family = glm.Binomial()))\n\n{'coefficients': {'Intercept': np.float64(18.866298717203943), 'hp': np.float64(0.03625559608221654), 'wt': np.float64(-8.083475182444579)}, 'converged': True, 'iterations': 9, 'beta_vector': array([[18.86629872],\n       [ 0.0362556 ],\n       [-8.08347518]])}"
  },
  {
    "objectID": "posts/07-glm/index.html#when-things-go-wrong-trust-me-they-will",
    "href": "posts/07-glm/index.html#when-things-go-wrong-trust-me-they-will",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n5 When Things Go Wrong (Trust me, they Will)",
    "text": "5 When Things Go Wrong (Trust me, they Will)\n\n5.1 “My model won’t converge!”\n\nCheck for complete separation in logistic regression\nTry scaling your predictors\nCheck for perfect multicollinearity\nOr just accept that your data is cursed\n\n5.2 “My coefficients are huge!”\n\nYou forgot to scale your variables\nOr you have a separation issue\nOr your data really is that dramatic\n\n5.3 “The deviance is gigantic!”\n\nMaybe your model actually sucks?\nOr you have outliers\nCheck if you’re using the right family\nConsider adding interaction terms or polynomial terms\n\n5.4 “I’m getting warnings about fitted probabilities of 0 or 1”\n\nThis is actually okay sometimes! It means you have very confident predictions\nBut if you’re getting tons of these, you might have separation issues\nCheck your data for extreme values"
  },
  {
    "objectID": "posts/07-glm/index.html#final-boss-wisdom",
    "href": "posts/07-glm/index.html#final-boss-wisdom",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n6 Final Boss Wisdom",
    "text": "6 Final Boss Wisdom\n\nLinear regression is a GLM that forgot it was a GLM\nThe link function is the reason you’re not predicting negative counts\nIRLS is just weighted least squares that updates itself until it stops crying\nDeviance is RSS for adults\n\nGo forth and model responsibly."
  },
  {
    "objectID": "posts/07-glm/index.html#some-good-references",
    "href": "posts/07-glm/index.html#some-good-references",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n7 Some good references",
    "text": "7 Some good references\n\nMcCullagh & Nelder (1989) – The Bible. Dense. Bring coffee and a therapist.\nDobson & Barnett – Actually readable introduction\nFaraway – Practical R examples, minimal pain\nNelder & Wedderburn (1972) – The original paper. Short, brilliant, life-changing\nThe UCLA stats page on GLMs – Still the best free resource in 2025\n\nAnd remember:\n\n“All models are wrong. But GLMs are wrong in the most elegant way possible.”\n\nNow go write some proper models, you beautiful statistician."
  },
  {
    "objectID": "posts/07-glm/index.html#appendix-cheat-sheet",
    "href": "posts/07-glm/index.html#appendix-cheat-sheet",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n8 Appendix — Cheat Sheet",
    "text": "8 Appendix — Cheat Sheet\nWhich GLM Should I Use?\n\n\n\n\n\n\n\n\n\nYour Y looks like…\nDistribution\nLink\nR Code\nPython Code\n\n\n\n0, 1, 2, 3… (counts)\nPoisson\nlog\nfamily = poisson()\nsm.families.Poisson()\n\n\nYes/No (binary)\nBinomial\nlogit\nfamily = binomial()\nsm.families.Binomial()\n\n\n0.2, 0.7, 0.4… (proportions)\nBinomial\nlogit\nfamily = binomial()\nsm.families.Binomial()\n\n\nAlways positive, right-skewed\nGamma\nlog\nfamily = Gamma()\nsm.families.Gamma()\n\n\nAny real number\nGaussian\nidentity\n\nfamily = gaussian() or lm()\n\nsm.families.Gaussian()\n\n\n\nOh wait, the overdispersion in count data? R pre-installs MASS anyways, so bother use glm.nb() yourself. \nPro tip: When in doubt, plot your residuals. If they look like a crime scene, you chose wrong."
  },
  {
    "objectID": "posts/05-pipes/index.html#the-piper-pioneer-2013",
    "href": "posts/05-pipes/index.html#the-piper-pioneer-2013",
    "title": "How much do you know about pipes?",
    "section": "\n4.1 1. The {pipeR} Pioneer (2013)",
    "text": "4.1 1. The {pipeR} Pioneer (2013)\nThe pipeR package by Kun Ren was one of the earliest pipe implementations in R, introducing the %&gt;&gt;% operator.\n\nbox::use(pipeR[`%&gt;&gt;%`])\n\n1:10 %&gt;&gt;%\n    mean() %&gt;&gt;%\n    round(2)\n\n[1] 5.5\n\n\nHere’s the cool part:\n\nLambda expressions with parentheses:\n\n\n1:10 %&gt;&gt;%\n    (mean(.) * 2) %&gt;&gt;%\n    round(2)\n\n[1] 11\n\n\n\nSide effects with continued piping:\n\n\nset.seed(123)\nrnorm(100) %&gt;&gt;%\n    (~ plot(., main = \"Random Normal Values\")) %&gt;&gt;%  # Side effect\n    mean() %&gt;&gt;%\n    round(2)\n\n\n\n\n\n\n\n[1] 0.09\n\n\n\n\n\n\n\n\nNoteWhy it faded\n\n\n\nIt’s not like it vanished from the existence, more like it is superseded by magrittr and took over."
  },
  {
    "objectID": "posts/05-pipes/index.html#the-game-changer-magrittr-pipe-2014",
    "href": "posts/05-pipes/index.html#the-game-changer-magrittr-pipe-2014",
    "title": "How much do you know about pipes?",
    "section": "\n4.2 2. The Game Changer: {magrittr} Pipe (2014)",
    "text": "4.2 2. The Game Changer: {magrittr} Pipe (2014)\nThe magrittr package, created by Stefan Milton Bache and later maintained by Lionel Henry at Posit (formerly RStudio), became the most popular pipe implementation. It was inspired by F#’s pipe-forward operator and Unix pipes.\n\nbox::use(magrittr[`%&gt;%`, `%&lt;&gt;%`, `%T&gt;%`, `%$%`, `%!&gt;%`])\n\nc(1, 2, 3, 4, 5) %&gt;%\n    mean() %&gt;%\n    round(2)\n\n[1] 3\n\n\nDo you know? There are plenty pipe operators in magrittr package, consists of at least 5 operators. Here are the special features:\n\n\nThe classic\nAssignment pipe\nTee pipe\nExposition pipe\nEager pipe\n\n\n\nThe %&gt;% is magrittr’s standard and “lazy” pipe - it doesn’t evaluate arguments until needed, which can affect behavior with certain functions. Lazy evaluation means that the RHS is only computed when its value is required, which optimizes performance but can lead to surprises with side-effect-heavy code.\nTo understand better how %&gt;% works, let’s give a demonstration by applying dot placeholder for non-first arguments:\n\nmtcars %&gt;%\n    lm(mpg ~ cyl, data = .)\n\n\nCall:\nlm(formula = mpg ~ cyl, data = .)\n\nCoefficients:\n(Intercept)          cyl  \n     37.885       -2.876  \n\n\nThe dot (.) acts as a placeholder for the piped value, allowing it to be inserted into any argument position—not just the first. You can also apply multiple placeholders:\n\nmtcars %&gt;% \n    head(5) %&gt;% \n    split(., .$cyl)\n\n$`4`\n            mpg cyl disp hp drat   wt  qsec vs am gear carb\nDatsun 710 22.8   4  108 93 3.85 2.32 18.61  1  1    4    1\n\n$`6`\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n\n$`8`\n                   mpg cyl disp  hp drat   wt  qsec vs am gear carb\nHornet Sportabout 18.7   8  360 175 3.15 3.44 17.02  0  0    3    2\n\n\n\n\nThe %&lt;&gt;% operator is invoking reference semantics, where it pipes and assigns the result back to the original variable:\n\nx = 1:5\nx %&lt;&gt;% log() %&gt;% sum()\nx\n\n[1] 4.787492\n\n\nThis is equivalent to x = x %&gt;% log() %&gt;% sum() but more concise. What happened here is we created a side-effect of x. Some pointed it out why it is a problem.\n\n\nThe %T&gt;% “tee” pipe passes the left-hand side value forward, not the output of the right-hand side. Useful for side effects like plotting or printing, where you want to perform an action but continue with the original data:\n\nset.seed(123)\nrnorm(100) %T&gt;% \n    plot(main = \"Values before mean\") %&gt;% \n    mean() %&gt;%\n    round(2)\n\n\n\n\n\n\n\n[1] 0.09\n\n\nThis should be the equivalent:\n{\n    set.seed(123)\n    plot(rnorm(100), main = \"Values before mean\")\n    round(mean(rnorm(100)), 2)\n}\nSo, if you try the following:\n\n1:5 %T&gt;% \n    mean()\n\n[1] 1 2 3 4 5\n\n\nThe %T&gt;% operator discards the output of mean(1:5), and that’s because mean() doesn’t return a side-value effect.\nBy the way, the “tee” name comes from Unix’s tee command, which splits output streams.\n\n\nThe %$% “exposition” pipe exposes the names within the left-hand side object to the right-hand side expression:\n\nmtcars %$%\n    cor(mpg, cyl)\n\n[1] -0.852162\n\n\nThis is equivalent to:\ncor(mtcars$mpg, mtcars$cyl)\nThis is particularly useful with functions that don’t have a data argument.\n\n\n\n\n\n\nWarning\n\n\n\nDo not use %$% operator when LHS is not a a list or data frame with named elements.\n\n\n\n\nThe %!&gt;% operator is the “eager” version of %&gt;% that evaluates arguments immediately. This can matter for functions with non-standard evaluation:\n\n# Standard (lazy) pipe\niris %&gt;% \n    subset(Species == \"setosa\") %&gt;% \n    head(3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\n# Eager pipe (forces immediate evaluation)\niris %!&gt;% \n    subset(Species == \"setosa\") %!&gt;% \n    head(3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\n\nIn most cases, the difference is subtle, but it can matter for advanced programming.\nTo see the actual difference:\n\n\n%!&gt;%: cat(1) is immediately evaluated (it evaluates from left to right)\n\n\n0 %!&gt;% (\\(x) { cat(1); x }) %!&gt;% (\\(x) cat(2))\n\n12\n\n\n\n\n%&gt;%: Evaluates only cat(2) as the first result is never used\n\n\n0 %&gt;% (\\(x) { cat(1); x }) %&gt;% (\\(x) cat(2))  \n\n2\n\n\nSource: https://stackoverflow.com/questions/76326742/what-are-the-differences-and-use-cases-of-the-five-magrittr-pipes"
  },
  {
    "objectID": "posts/05-pipes/index.html#the-wrapr-dot-arrow-2017",
    "href": "posts/05-pipes/index.html#the-wrapr-dot-arrow-2017",
    "title": "How much do you know about pipes?",
    "section": "\n4.3 3. The {wrapr} Dot Arrow (2017)",
    "text": "4.3 3. The {wrapr} Dot Arrow (2017)\nJohn Mount’s wrapr package provides the %.&gt;% “dot arrow” pipe, a deliberate and explicit alternative to %&gt;%.\n\nbox::use(wrapr[`%.&gt;%`])\n\n1:10 %.&gt;%\n    mean(.) %.&gt;%\n    round(., 2)\n\n[1] 5.5\n\n\nI don’t know much about this pipe, to be honest. As what I can see, this pipe requires the dot to always be explicit, which, for me, it’s so good that it can prevent some subtle bugs and makes code intentions clearer."
  },
  {
    "objectID": "posts/05-pipes/index.html#the-bizarro-pipe-base-r-2017",
    "href": "posts/05-pipes/index.html#the-bizarro-pipe-base-r-2017",
    "title": "How much do you know about pipes?",
    "section": "\n4.4 4. The Bizarro Pipe (Base R, ~2017)",
    "text": "4.4 4. The Bizarro Pipe (Base R, ~2017)\nI am not sure when this operator released, but there’s a pipe operator (not categorically) in base R: the “Bizarro pipe” (-&gt;.;), that works like %&gt;% and %.&gt;%. It’s not a formal operator but an emergent behavior from combining existing R syntax.\n\n1:10 -&gt;.; \n    mean(.) -&gt;.; \n    round(., 2)\n\n[1] 5.5\n\n\nThe Bizarro pipe works by:\n\nUsing right assignment -&gt; to assign to . (this is done by typing - + &gt; + .)\nEnding each statement with ; to separate expressions\nThe next line uses . as input\n\nIt’s called “Bizarro” because it uses right-to-left assignment syntax (-&gt;) to create a left-to-right workflow.\nHowever, it has disadvantages (talked in this Stackoverflow discussion):\n\nCreates hidden side-effects (the persistent . variable)\nGoes against R style guides (right assignment and semicolons are discouraged)\nCan lead to subtle bugs if you forget to assign to . at some step\nThe . variable is hidden from ls() and IDE inspectors\nIt’s so pesky, it won’t auto-indent\n\nSeriously, I won’t recommend Bizarro pipe at all. It is still a nice touch as a temporary replacement of %&gt;% for chained R codes, and will not use it for production code."
  },
  {
    "objectID": "posts/05-pipes/index.html#the-native-pipe-r-v4.1-2021",
    "href": "posts/05-pipes/index.html#the-native-pipe-r-v4.1-2021",
    "title": "How much do you know about pipes?",
    "section": "\n4.5 5. The Native Pipe (R v4.1+, 2021)",
    "text": "4.5 5. The Native Pipe (R v4.1+, 2021)\nIn May 2021, R v4.1 introduced the native pipe operator |&gt; (type | and &gt;), bringing pipe functionality into base R without the need for external packages. This operator is the actual operator that was inspired by the pipe-forward operator in F# and the concept of Unix pipes.\n\nc(1, 2, 3, 4, 5) |&gt;\n    mean() |&gt;\n    round(2)\n\n[1] 3\n\n\nThis is too identical to %&gt;% from magrittr with some obvious differences.\n\n4.5.1 Common differences from {magrittr} pipe\nThe placeholder for |&gt; is now applied in R v4.2 and above. For the syntax, it rather uses _, not ..\n\nmtcars |&gt;\n    lm(mpg ~ cyl, data = _)\n\n\nCall:\nlm(formula = mpg ~ cyl, data = mtcars)\n\nCoefficients:\n(Intercept)          cyl  \n     37.885       -2.876  \n\n\nThe native pipe:\n\nIs slightly faster (negligible in often cases, this matters for some cases like running for-loop)\nDoes not support the tee (%T&gt;%), exposition (%$%), or assignment (%&lt;&gt;%) operators\nCannot be used with compound assignment\nIs more strict about valid syntax\n\n4.5.2 Performance comparison\nThe native pipe is clearly faster than the magrittr pipe because native pipe does not add more function calls within its implementation compared to the magrittr pipe.\n\nbench::mark(\n    \"magrittr pipe\" = replicate(10000, 1:100 %&gt;% sum()), \n    \"native R pipe\" = replicate(10000, 1:100 |&gt; sum())\n)\n\n# A tibble: 2 × 6\n  expression         min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;    &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 magrittr pipe   46.7ms   50.5ms      19.8     375KB     79.3\n2 native R pipe   11.8ms   13.9ms      71.2     369KB     30.9\n\n\n\n4.5.3 Pipe-bind operator\nAfter R v4.2, the pipe-bind operator =&gt; (type = + &gt;), or a pipe-binding syntax, allows you to bind the result of the left-hand side (LHS) to a name within the right-hand side (RHS) expression.\nThis feature is, however, disabled by default. You may want to enable it by running the following:\n\nSys.setenv(\"_R_USE_PIPEBIND_\" = TRUE)\n\nAnother options:\n\nPlace this command into .Renviron file (Hint: run usethis::edit_r_environ()):\n\n_R_USE_PIPEBIND_=true\n\nRun this in a command prompt or PowerShell\n\nsetx _R_USE_PIPEBIND_ true\nIf you are in Linux / macOS (bash / zsh):\nexport _R_USE_PIPEBIND_=true\nThen restart R.\nHere’s what it does:\n\nmtcars |&gt; \n    df =&gt; lm(mpg ~ wt, data = df)\n\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nmtcars |&gt; \n    df =&gt; split(df, df$cyl) |&gt; \n    lapply(\\(df) lm(mpg ~ wt, data = df)) |&gt; \n    vapply(\\(mod) summary(mod)$r.squared, numeric(1))\n\n        4         6         8 \n0.5086326 0.4645102 0.4229655 \n\n\nThe df name temporarily exists only inside that RHS expression — not in your global environment. I like this because this is more explicit than . in %&gt;% operator. You can name the LHS result and refer to it directly inside the RHS expression anything you like."
  },
  {
    "objectID": "posts/05-pipes/index.html#non-pipe-alternatives",
    "href": "posts/05-pipes/index.html#non-pipe-alternatives",
    "title": "How much do you know about pipes?",
    "section": "\n4.6 Non-Pipe Alternatives",
    "text": "4.6 Non-Pipe Alternatives\nWhile the above are true pipe operators, it’s worth mentioning that some packages achieve similar left-to-right workflows through different mechanisms.\n\n4.6.1 Chaining in {data.table} (2010)\ndata.table uses method chaining with [][] notation. It is NOT a pipe operator in a sense, but achieves a similar left-to-right flow. It behaves differently from the pipe operator — it chains operations within the same [.data.table method, and doesn’t pass values between functions, i.e. the use of placeholders.\nLet’s look at the basic data.table example:\n\nbox::use(data.table[as.data.table, `:=`])\n\ndt = as.data.table(mtcars)\ndt[cyl == 8][order(-mpg)][, .(mpg, cyl, hp)][1:5]\n\n     mpg   cyl    hp\n   &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:  19.2     8   175\n2:  18.7     8   175\n3:  17.3     8   180\n4:  16.4     8   180\n5:  15.8     8   264\n\n\nDeeper method chaining in data.table with grouping and aggregation:\n\ndt[, log_mpg := log(mpg)][,\n    .(\n        mpg_mean = mean(mpg, na.rm = TRUE), \n        log_mpg_mean = mean(log_mpg, na.rm = TRUE)\n    ), by = cyl\n][\n    order(-mpg_mean, -log_mpg_mean)\n]\n\n     cyl mpg_mean log_mpg_mean\n   &lt;num&gt;    &lt;num&gt;        &lt;num&gt;\n1:     4 26.66364     3.270454\n2:     6 19.74286     2.980439\n3:     8 15.10000     2.700171\n\n\nThis is method chaining, not piping—the key difference is that pipes pass values between different functions, while data.table chains operations within the same [ method.\n\n4.6.2 Multiple Assignment with {zeallot} (2018)\nThis is not exactly an operator that behaves like a pipe, where it passes LHS as an input for RHS, but I would like to point this one out. R lacks destructuring (also called “unpacking”) method, just like what you see in other languages, such as Python:\nx, y = 0, 1\nThe zeallot allows destructuring assignment with %&lt;-%. While not exactly a pipe operator to chain the commands, works well in pipe-like workflows.\n\nbox::use(zeallot[`%&lt;-%`])\n\n# Multiple assignment\nc(a, b) %&lt;-% c(1, 2)\nc(a, b)\n\n[1] 1 2\n\n\nDestructuring with computations:\n\nc(mean_val, sd_val, n) %&lt;-% local ({\n    set.seed(125)\n    x = rnorm(100)\n    c(mean(x), sd(x), length(x))\n})\n\ncat(glue::glue(\"Mean: {mean_val}, SD: {sd_val} , N: {n}\"), \"\\n\")\n\nMean: 0.100208694251594, SD: 1.06105719788861 , N: 100 \n\n\nWorks with pipe workflows:\n\nset.seed(123)\nc(m, s) %&lt;-% (rnorm(100) %&gt;% { c(mean(.), sd(.)) })\nc(m, s)\n\n[1] 0.09040591 0.91281588"
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#what-youll-find-here",
    "href": "posts/03-modules-in-r/index.html#what-youll-find-here",
    "title": "Box: Placing module system into R",
    "section": "1.1 What You’ll Find Here",
    "text": "1.1 What You’ll Find Here\nA definitive guide that walks you through:\n\nAlternative approach to import R codes (i.e. R packages, modules)\nModern approaches to compose and organize R codes\nStep-by-step tutorials for using the box package\nBest practices for maintainable R code"
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#why-this-book",
    "href": "posts/03-modules-in-r/index.html#why-this-book",
    "title": "Box: Placing module system into R",
    "section": "1.2 Why this book",
    "text": "1.2 Why this book\nOnly little to no books teach you to correctly write reusable, composable, and modular R codes :). Most of the books maybe teaches you about R, particularly application of R in different fields, but little to none explains one of the best practices. Most of them uses library() anyways, so you won’t certainly find similar book like this.\nConsequently, while R offers various ways to organize code, the box package manages to be superior among them (I am bias) by introducing a fresh, modern approach to module system that may significantly improve your R development workflow, similar to the workflow made by other developers, particularly Python devs. This book bridges the gap between basic R programming and professional-grade code organization."
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#how-to-use-this-guide",
    "href": "posts/03-modules-in-r/index.html#how-to-use-this-guide",
    "title": "Box: Placing module system into R",
    "section": "1.3 How to Use This Guide",
    "text": "1.3 How to Use This Guide\nClick here and it will send you to the actual book. \nThe content is structured progressively, building from foundational concepts to deep applications. For the best learning experience:\n\nStart with the introduction to understand the core concepts\nLearning the fundamentals of import system with {box} package\nLearning the structures and constructions of reusability of modules, treating them like R packages, and supplied with documentation\nLearning unit testing the modules"
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#contributing",
    "href": "posts/03-modules-in-r/index.html#contributing",
    "title": "Box: Placing module system into R",
    "section": "1.4 Contributing",
    "text": "1.4 Contributing\nContributions are welcome! If you have suggestions or improvements, please open an issue or submit a pull request on the GitHub repository."
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#license",
    "href": "posts/03-modules-in-r/index.html#license",
    "title": "Box: Placing module system into R",
    "section": "1.5 License",
    "text": "1.5 License\nThis book is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). See the LICENSE file for details."
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#credits",
    "href": "posts/03-modules-in-r/index.html#credits",
    "title": "Box: Placing module system into R",
    "section": "1.6 Credits",
    "text": "1.6 Credits\nThis book was created by Joshua Marie.\nSpecial thanks to:\n\nKonrad Rudolph for creating and maintaining the box package\nThe R community for their continued support and feedback\n\nThe book is built with Quarto, hosted on GitHub Pages."
  },
  {
    "objectID": "posts/01-meta-nn/index.html#introduction",
    "href": "posts/01-meta-nn/index.html#introduction",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "\n1.1 Introduction",
    "text": "1.1 Introduction\nThe create_nn_module() function dynamically generates torch neural network module definitions. Instead of manually writing out layer definitions and forward pass logic, this function builds the code expressions for you.\nKey benefits:\n\n\nFlexibility: Change network architecture with a single function call\n\nAutomation: Generate multiple network configurations programmatically\n\nExperimentation: Quickly test different architectures in hyperparameter searches\n\nThis is how it’s done:\n\nDefine the network architecture (input size, hidden layers, output size)\nSpecify activation functions for each layer\nProgrammatically generate the initialize method (layer definitions)\nProgrammatically generate the forward method (forward pass logic)\nReturn an nn_module expression ready to be evaluated\n\nThe packages used:\n\n\nrlang (v1.1.4) - For metaprogramming tools\n\npurrr (v1.0.2) - For functional programming\n\nglue (v1.7.0) - For string interpolation\n\nmagrittr - For pipe operator\n\nbox (v1.2.0) - For modular code organization"
  },
  {
    "objectID": "posts/01-meta-nn/index.html#the-complete-function",
    "href": "posts/01-meta-nn/index.html#the-complete-function",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "\n1.2 The Complete Function",
    "text": "1.2 The Complete Function\nI created create_nn_module() function a while ago and shared it on GitHub Gist. Here’s the function we’ll be analyzing:\n\nCodecreate_nn_module = function(nn_name = \"DeepFFN\", hd_neurons = c(20, 30, 20, 15), no_x = 10, no_y = 1, activations = NULL) {\n    box::use(\n        rlang[new_function, call2, caller_env, expr, exprs, sym, is_function, env_get_list],\n        purrr[map, map2, reduce, set_names, compact, map_if, keep, map_lgl], \n        glue[glue], \n        magrittr[`%&gt;%`]\n    )\n    \n    nodes = c(no_x, hd_neurons, no_y)\n    n_layers = length(nodes) - 1\n    \n    call_args = match.call()\n    activation_arg = call_args$activations\n    \n    if (is.null(activations)) {\n        activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n    } else if (length(activations) == 1 || is.function(activations)) {\n        single_activation = activations\n        activations = c(rep(list(single_activation), length(hd_neurons)), list(NA))\n    }\n    \n    activations = map2(activations, seq_along(activations), function(x, i) {\n        if (is.null(x)) {\n            NULL\n        } else if (is.function(x)) {\n            if(!is.null(activation_arg) && is.call(activation_arg) && activation_arg[[1]] == quote(c)) {\n                func_name = as.character(activation_arg[[i + 1]])\n                sym(func_name)\n            } else if(!is.null(activation_arg) && (is.symbol(activation_arg) || is.character(activation_arg))) {\n                func_name = as.character(activation_arg)\n                sym(func_name)\n            } else {\n                parent_env = parent.frame()\n                env_names = ls(envir = parent_env)\n                matching_names = env_names %&gt;%\n                    keep(~ {\n                        obj = env_get_list(parent_env, .x)[[1]]\n                        identical(obj, x)\n                    })\n                \n                if (length(matching_names) &gt; 0) {\n                    sym(matching_names[1])\n                } else {\n                    stop(\"Could not determine function name for activation function\")\n                }\n            }\n        } else if (is.character(x)) {\n            if (length(x) == 1 && is.na(x)) {\n                NULL\n            } else {\n                sym(x)\n            }\n        } else if (is.symbol(x)) {\n            x\n        } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n        }\n    })\n    \n    init_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), call2(\"nn_linear\", !!!dims))\n    })\n    \n    init = new_function(\n        args = list(), \n        body = call2(\"{\", !!!init_body)\n    )\n    \n    layer_calls = map(1:n_layers, function(i) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        activation_fn = if (i &lt;= length(activations)) activations[[i]] else NULL\n        \n        result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n        if (!is.null(activation_fn)) {\n            result = append(result, list(call2(activation_fn)))\n        }\n        result\n    }) |&gt; \n        unlist() |&gt; # recursive = FALSE is also valid\n        compact()\n    \n    forward_body = reduce(layer_calls, function(acc, call) {\n        expr(!!acc %&gt;% !!call)\n    }, .init = expr(x))\n    \n    forward = new_function(\n        args = list(x = expr()), \n        body = call2(\"{\", forward_body)\n    )\n    \n    call2(\"nn_module\", nn_name, initialize = init, forward = forward)\n}\n\n\n\n\n1.2.1 Why box?\nYou’ll notice that I’ve been using another approach to load namespace in R. But, why ‘box’? You need to check out my mini book dedicated on modular programming in R.\n\n1.2.2 But why load dependencies using box::use() inside a function?\n\nWell, a function, or a function call, creates an environment, which encloses the objects and operations within it. In other words, we create a closure. This is actually a good practice for several reasons:\n\nNamespace isolation: Dependencies loaded inside the function will not make pollution the global environment, or conflicts with any packages loaded. When you load packages required with library(), inside a function or not, it attaches those packages to your search path, and will mask functions from other packages. With box::use() inside a function, the imports are scoped only to that function’s or call’s environment.\nExplicit dependencies: Anyone reading the function immediately knows what external tools it uses. You don’t have to scroll to the top of a script to see what’s loaded.\nReproducibility: The function becomes self-contained. If you share just this function, others know exactly what packages they need less hunting through documentation.\nAvoiding conflicts: Different functions can use different versions or implementations without conflicts. For example, one function might use dplyr::filter() while another uses stats::filter(), and they won’t interfere with each other.\nLazy loading: The packages are only loaded when the function is actually called, not when it’s defined. This can improve script startup time if you have many functions but only use a few.\n\n\n\n\n\n\n\nNote\n\n\n\nIn a nutshell: The ‘box’ package provides explicit, granular imports, making it transparent which namespace to be imported from which packages. It’s like having a well-organized toolbox where each tool is labeled."
  },
  {
    "objectID": "posts/01-meta-nn/index.html#explanations",
    "href": "posts/01-meta-nn/index.html#explanations",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "\n1.3 Explanations",
    "text": "1.3 Explanations\nI’ll be trying to be concise on explaining each layers of the function so that you’ll understand what I did\n\n1.3.1 Step 1: Loading Dependencies\nI use box::use() to load dependencies:\n\n\nrlang: Improvised Core R programming. One of the core R programming, metaprogramming which includes creating expressions and functions programmatically, are less painful than what base R have.\n\npurrr: Improvised Functional programming utilities.\n\nglue: R lacks Python’s f-string for string interpolation, although we have sprintf() and paste() for that. glue makes string interpolation more readable with glue(\"fc{i}\") instead of paste0(\"fc\", i) or sprintf(\"fc%d\", i).\n\nmagrittr: The pipe operator %&gt;% for chaining operations. This is optional, by the way — R 4.1+ has the native pipe |&gt;, but %&gt;% offers better flexibility with the dot placeholder.\n\n1.3.2 Step 2: Defining Network Architecture\nIn DFFNN architecture, it is defined by the input layer, the hidden layer, and the output layer.\n\n\nSource: https://medium.com/data-science/designing-your-neural-networks-a5e4617027ed\n\nThe number of nodes are defined by integers, except for input and output layer nodes which they are fixed and determined by the data you provided, and they are defined by no_x and no_y. The number of hidden layers is defined by the length of input in hd_neurons argument.\nCombine no_x, hd_neurons, no_y in order:\nnodes = c(no_x, hd_neurons, no_y)\nAnd then calculate the length of nodes, which is \\(1 + n_{\\text{hidden layres}} + 1\\), and then subtract it by 1 because the applied activation functions is invoked between each layer.\nn_layers = length(nodes) - 1\n\n1.3.2.1 Example\nWhen you have:\n\n10 predictors\n\n5 hidden layers, and for each layer:\n\n20 nodes\n30 nodes\n20 nodes\n15 nodes\n20 nodes\n\n\n1 response variable\n\nTotal number of layers: 7\nThis means we need 7 - 1 linear transformations, and here is my diagram:\n\\[10_{\\text{inputs}} \\rightarrow20_{\\text{hd1}} \\rightarrow30_{\\text{hd2}} \\rightarrow20_{\\text{hd3}} \\rightarrow15_{\\text{hd4}} \\rightarrow20_{\\text{hd5}} \\rightarrow1_{\\text{ouput}}\\]\n\n1.3.3 Step 3: Setting Activation Functions\nThe activations argument holds the account of the activation function. It could be a string, a literal function, or a mix of it in a vector of inputs.\nThen, set activations = NULL, where NULL is the default value, which leads to set ReLU (nnf_relu) as the activation function for all hidden layers\n\nCodeif (is.null(activations)) {\n    activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n}\n\n\nEvery activations will have NA as the last element because we need to ensure no activation function after the output. The output layer often doesn’t need an activation (for regression) or needs a specific one based on the task (softmax for classification, sigmoid for binary classification). By defaulting to NA, the user can decide.\n\n\n\n\n\n\nNoteLength of inputs\n\n\n\nTo provide values in activations argument, it needs to be equal to the size of hidden layers, or if you provide only 1 act. function, this will be the activation function across the transformations.\n\n\n\n\n\n\n\n\nNoteDefault\n\n\n\nThe default is NULL. That is, if activations is not provided, the activation function is set to ReLU function.\n\n\n\n\n\n\n\n\nTipInstead of NULL\n\n\n\nNow, if you’re asking “Why needs to set activations to \"nnf_relu\" instead of NULL”? Don’t worry, I did consider that, but this is just a pure demo.\n\n\n\n1.3.4 Step 4: Processing Activation Functions\nThis part (re)processes the activation function inputs in the activations argument. This kept tracks the argument you are putting, especially when you the input you are writing in activations argument has different types.\n\nCodecall_args = match.call()\nactivation_arg = call_args$activations\n\nactivations = map2(activations, seq_along(activations), function(x, i) {\n    if (is.null(x)) {\n        NULL\n    } else if (is.function(x)) {\n        if(!is.null(activation_arg) && is.call(activation_arg) && \n           activation_arg[[1]] == quote(c)) {\n            func_name = as.character(activation_arg[[i + 1]])\n            sym(func_name)\n        } else {\n            func_name = names(which(sapply(ls(envir = parent.frame()), \n                function(name) {\n                    identical(get(name, envir = parent.frame()), x)\n                })))[1]\n            if (!is.na(func_name)) {\n                sym(func_name)\n            } else {\n                stop(\"Could not determine function name for activation function\")\n            }\n        }\n    } else if (is.character(x)) {\n        if (length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            sym(x)\n        }\n    } else if (is.symbol(x)) {\n        x\n    } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n        NULL\n    } else {\n        stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n    }\n})\n\n\n\n1.3.5 Step 5: Building the initialize Method Body\nThe body I am referring in initialize method is the body of the function for the initialize implemented method. This part is a bit far from trivial. I named it init_body to keep track the expression I am trying to build.\n\n\n\n\n\n\nNoteIn reality\n\n\n\nKeep in mind that there’s no initialize and forward parameters within nn_module() torch namespace or whatsoever. However, it is expected you to create them to create a module inside nn_module(). These parameters are kept within the ... wildcard parameter.\n\n\n\n1.3.5.1 Creation of the expressions inside the body\nHere is the part I am tracking inside create_nn_module body expression:\n\nCodeinit_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), \n    function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), \n              call2(\"nn_linear\", !!!dims))\n    })\n\n\nWhat it does is it creates assignment expressions for each layer in the network.\nFor instance, c(20, 30, 20, 15, 20) is your argument for the activations:\n\n\nmap2(nodes[-length(nodes)], nodes[-1], c) pairs consecutive layer sizes:\n\nCodelist(\n    c(10, 20), \n    c(20, 30), \n    c(30, 20), \n    c(20, 15), \n    c(15, 20), \n    c(20, 1)\n)\n\n\n\n\nFor each pair, generates a layer assignment expression:\n\nLayer names: fc1, fc2, …, out (last layer)\nCreates: self$fc1 = nn_linear(10, 20)\n\n\n\n\nThis will be the generated expression:\nself$fc1 = nn_linear(10, 20)\nself$fc2 = nn_linear(20, 30)\nself$fc3 = nn_linear(30, 20)\nself$fc4 = nn_linear(20, 15)\nself$fc5 = nn_linear(15, 20)\nself$out = nn_linear(20, 1)\n\n\n\n\n\n\nNoteHow is it done?\n\n\n\nI need you to understand rlang::call2() a bit:\nThe call2() function is a glorified call() from base R that builds function call expressions.\nFrom what I did within init_body:\n\ncall2(\"$\", expr(self), sym(\"fc1\")) constructs self$fc1\n\ncall2(\"nn_linear\", !!!dims) is a bit complex:\n\nIt splices dims from what I created in map2(nodes[-length(nodes)], nodes[-1], c).\n\ncall2() function accepts rlang’s quasiquotation API, then splices the dimensions, i.e. call2(\"nn_linear\", !!!c(10, 20)) to call2(\"nn_linear\", 10, 20).\nThen finally constructs nn_linear(10, 20)\n\n\n\ncall2(\"=\", lhs, rhs) parses an expression: lhs = rhs. This part yields an expression I want: self$fc1 = nn_linear(10, 20).\n\nNote: You can use &lt;- if you want, instead of =. After all, = within call2()’s .fn argument tokenize = as an assignment operator. \n\n\n\n1.3.5.2 Building an actual body and function\nNow, for this part:\n\nCodeinit = new_function(\n    args = list(), \n    body = call2(\"{\", !!!init_body)\n)\n\n\nDon’t forget to put curly brackets { around the built expression because it becomes necessary in R when composing a function with multiple lines. Still using call2() for that, particularly call2(\"{\", !!!init_body) creates a code block { ... } containing all initialization statements. The !!! operator “splices” the list of expressions into the block, because init_body forms a list of expressions.\nAfter building the expression I want for the body of initialize, let’s take further by utilizing it as a body to create a function with rlang::new_function. I just simply wraps all the layer initialization expressions into a complete function for initialize method for nn_module().\n\n\n\n\n\n\nNoteInputs in initialize\n\n\n\nNotice that the argument for initialize is empty? I could’ve place input_size and output_size if I wanted to, but it seems unnecessary since I already placed the sizes of the input and output within the expression I built. To make a function expression with empty arguments, place the args argument of new_function with empty list().\n\n\nHere’s the result:\nfunction() {\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}\nStore this expression into init because we still have to finalize the expression we want to create. \n\n1.3.6 Step 6: Building Layer Calls for Forward Pass\nThe same process as initialize, except we are not building multiple lines of expression, just building a chained expression with ‘magrittr’ pipe from the initial value.\n\n1.3.6.1 Creating layer of calls\nTo form this expression is also complex\n\nCodelayer_calls = map(1:n_layers, function(i) {\n    layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n    activation_fn = if (i &lt;= length(activations)) activations[[i]] else NULL\n    \n    result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n    if (!is.null(activation_fn)) {\n        result = append(result, list(call2(activation_fn)))\n    }\n    result\n}) |&gt; \n    unlist() |&gt; \n    compact()\n\n\nWhat it does is it builds a sequence of operations for the forward pass: layer calls and their activation functions. I stored the output into layer_calls so that we can keep track of it.\nThe process:\n\n\nFor each layer, create a list containing:\n\nThe layer call: self$fc1()\n\nThe activation call (if exists): nnf_relu()\n\n\n\nFlatten all lists into a single sequence with unlist().\nFilter the list we created away from any NULL values with purrr::compact().\n\nThus, we form a list of expressions:\n\nCodelist(\n    self$fc1(), nnf_relu(),\n    self$fc2(), nnf_relu(),\n    self$fc3(), nnf_relu(),\n    self$fc4(), nnf_relu(),\n    self$fc5(), nnf_relu(),\n    self$out()\n)\n\n\nNote: The last layer (out) has no activation because we set it to NA.\n\n1.3.6.2 Building an actual body and function\nI choose to chain them, x or the input as the initial value, and choose not to break lines and forms multiple assignments. This is what I preferred, and besides, it’s so easy to form chained expression when the output is a defused call with reduce().\n\nCodeforward_body = reduce(layer_calls, function(acc, call) {\n    expr(!!acc %&gt;% !!call)\n}, .init = expr(x))\n\n\nI choose to chain all operations together with pipe operator %&gt;% from ‘magrittr’.\nThen, with reduce() works:\n\n\nStarting with x, it progressively adds each operation:\n\nStep 1: x %&gt;% self$fc1()\n\nStep 2: (x %&gt;% self$fc1()) %&gt;% nnf_relu()\n\nStep 3: (x %&gt;% self$fc1() %&gt;% nnf_relu()) %&gt;% self$fc2()\n\n…and so on\n\n\n\nAs for the final output:\nx %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% \n    self$fc2() %&gt;% nnf_relu() %&gt;% \n    self$fc3() %&gt;% nnf_relu() %&gt;% \n    self$fc4() %&gt;% nnf_relu() %&gt;% \n    self$fc5() %&gt;% nnf_relu() %&gt;% \n    self$out()\n\n\n\n\n\n\n\n\nTipWhy pipes?\n\n\n\nThe pipe operator makes the forward pass logic read like a natural sequence: “take input x, pass through fc1, apply nnf_relu to invoke ReLU activation function, then pass through fc2, apply nnf_relu to invoke ReLU activation function, …, it kepts repeating until we reach to out”\n\n\nAfter that, I stored it into forward_body, then make use of it to build the function for forward method with rlang::new_function():\n\nCodeforward = new_function(\n    args = list(x = expr()), \n    body = call2(\"{\", forward_body)\n)\n\n\nThe args for forward method has x with empty value. Then, wrap the piped forward pass into a function that accepts input x.\nAnd here’s the result:\nfunction(x) {\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% \n        self$fc2() %&gt;% nnf_relu() %&gt;% \n        self$fc3() %&gt;% nnf_relu() %&gt;% \n        self$fc4() %&gt;% nnf_relu() %&gt;% \n        self$fc5() %&gt;% nnf_relu() %&gt;% \n        self$out()\n}\nStore this expression into forward because we still have to finalize the expression we want to create. \n\n1.3.7 Step 7: Finalizing the nn_module Expression generation\nHere we are for the final part: generating the nn_module expression, by puzzling each part: initialize and forward.\nThe final part is built from this:\ncall2(\"nn_module\", nn_name, !!!set_names(list(init, forward), c(\"initialize\", \"forward\")))\nI mean, you still have to use call2() to build a call. The inputs should be:\n\n.fn = \"nn_module\" -&gt;\n\nThe rest of the arguments:\n\n\nnn_name which is equivalent to “DeepFFN”. You can set any names whatever you want, though.\ninitialize = init\nforward = forward\nOriginally, I formed this in this expression: !!!set_names(list(init, forward), c(\"initialize\", \"forward\")). But then, I realized that we only need initialize and forward, and besides, this is a bit overkill.\n\n\n\nThus, the final expression that defines the neural network module.\nAnd hence, I form a function that generates a, perhaps, template:\n\nhd_nodes = c(20, 30, 20, 15, 20)\nact_fns = c(\"nnf_relu\", \"nnf_relu\", \"nnf_relu\", \"nnf_relu\")\ncreate_nn_module(\n    hd_neurons = hd_nodes, \n    activations = act_fns\n)\n\nnn_module(\"DeepFFN\", initialize = function () \n{\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}, forward = function (x) \n{\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% self$fc2() %&gt;% nnf_relu() %&gt;% \n        self$fc3() %&gt;% nnf_relu() %&gt;% self$fc4() %&gt;% nnf_relu() %&gt;% \n        self$fc5() %&gt;% self$out()\n})"
  },
  {
    "objectID": "posts/01-meta-nn/index.html#disclaimer",
    "href": "posts/01-meta-nn/index.html#disclaimer",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "\n1.4 Disclaimer",
    "text": "1.4 Disclaimer\nThis is an advanced example of metaprogramming in R, demonstrating how to leverage functional programming and rlang for code generation. I don’t mind you to replicate what I did, but sometimes this technique should be used judiciously—sometimes simpler, more explicit code is better.\nThis example showcases:\n\nDeep understanding of R’s evaluation model\nFunctional programming with purrr\n\nExpression manipulation with rlang\n\nPractical application to deep learning workflows\n\nAnd also, I am aware to the fact that the function I made is ugly if you said so."
  },
  {
    "objectID": "contacts.html#lets-connect",
    "href": "contacts.html#lets-connect",
    "title": "Joshua Marie",
    "section": "Let’s Connect",
    "text": "Let’s Connect\n\nQuick call: Book 15-min discovery call\nEmail: joshua.marie.k@gmail.com\nLinkedIn: linkedin.com/in/joshuamarie/\nGitHub: github.com/joshuamarie"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My name is Joshua",
    "section": "",
    "text": "Computer Scientist • Statistician\n\n\nI am a computer scientist, package creator / maintainer / contributor (to R and Python), a blogger, and a statistician. I write plenty of blogs and articles in statistics, mathematics, programming, and data science, as well as I studied multiple layers of programming paradigms and designs, and still actively learning. I write packages for statistics, data science, and numerical analysis, as my specialties found in those fields.\nI work in a consulting firm and I also independently consulting (feel free to reach me)."
  },
  {
    "objectID": "index.html#joshua-marie",
    "href": "index.html#joshua-marie",
    "title": "My name is Joshua",
    "section": "",
    "text": "Computer Scientist • Statistician\n\n\nI am a computer scientist, package creator / maintainer / contributor (to R and Python), a blogger, and a statistician. I write plenty of blogs and articles in statistics, mathematics, programming, and data science, as well as I studied multiple layers of programming paradigms and designs, and still actively learning. I write packages for statistics, data science, and numerical analysis, as my specialties found in those fields.\nI work in a consulting firm and I also independently consulting (feel free to reach me)."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "My name is Joshua",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\nA ‘careful’ and small intro guide to data science with ‘tidyverse’\n\n\n\nR\n\ndata-science\n\nanalytics\n\ntidyverse\n\n\n\nTeaching you the things you can take advantage of, and things weren’t taught by some tutorials you (potentially) know\n\n\n\n\n\nDec 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBother yourself writing reusable codes in R, not throwaway codes\n\n\n\nR\n\nbox\n\nprogramming\n\n\n\nI can also show you a proper way to write reusable R codes\n\n\n\n\n\nDec 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThree levels to compose R functions\n\n\n\nR\n\ndata-science\n\ntidyverse\n\npurrr\n\nfunctional-programming\n\nfp\n\n\n\nIntroducing the 3 ways to compose functions in R.\n\n\n\n\n\nDec 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s so special about formulas in R\n\n\n\nR\n\nformulas\n\nrlang\n\nmachine-learning\n\nstatistics\n\n\n\nLet’s discover how in the world it works\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThings you may or you may not know in ggplot2\n\n\n\nR\n\nggplot2\n\ndata-science\n\nvisualization\n\ntidyverse\n\nviz\n\nprogramming\n\n\n\nHere are some things I enumerated — things you may or may not know already\n\n\n\n\n\nDec 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhy SQL + R is an affable combo when I start learning SQL?\n\n\n\nR\n\nSQL\n\ndata-science\n\nanalytics\n\ntidyverse\n\n\n\nLearn mastering both — and how to make them work together seamlessly.\n\n\n\n\n\nNov 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTutorial on Generalized Linear Model from scratch\n\n\n\nR\n\nPython\n\nstatistics\n\nregression\n\nmachine-learning\n\n\n\nFinally understand what a link function actually does instead of just copy-pasting glm()\n\n\n\n\n\nNov 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWays to load / attach packages in R\n\n\n\nR\n\npackages\n\n\n\nA completely objective, totally scientific ranking (2025 edition)\n\n\n\n\n\nNov 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow much do you know about pipes?\n\n\n\nR\n\npipes\n\ntidyverse\n\nprogramming\n\n\n\n\n\n\n\n\n\nNov 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hidden Magic of Tidy-Select: R’s Universal Column Selection Language\n\n\n\nR\n\ntidyselect\n\ntidyverse\n\n\n\n\n\n\n\n\n\nNov 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBox: Placing module system into R\n\n\n\nR\n\nbook\n\n\n\n\n\n\n\n\n\nOct 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFirst level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R\n\n\n\nR\n\ntime-series\n\nmachine-learning\n\ngrid-search\n\n\n\n\n\n\n\n\n\nSep 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression\n\n\n\nR\n\nmachine-learning\n\ntorch\n\npointless-code\n\n\n\n\n\n\n\n\n\nSep 23, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#introduction",
    "href": "posts/02-arima-grid-search/index.html#introduction",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R",
    "section": "\n1.1 Introduction",
    "text": "1.1 Introduction\nThe ARIMA (AutoRegressive Integrated Moving Average) model is defined by three parameters:\n\n\np: Autoregressive order that counts the past lagged terms (This is AR in ARIMA context)\n\nd: Differencing order that counts the number of differencing to achieve stationarity (This is ‘I’ or “integrate” in ARIMA context)\n\nq: Moving average order that counts the past error lagged terms (MA)\n\nChoosing the right combination of (p, d, q) is…not that easy, right when you want to achieve the best fit, even with Hyndman and Khandaka (2008) methodology with their forecast::auto.arima().\nThis is how it’s done:\n\nPrepare a time series data. I generate a time series data from this in order for you to replicate this.\n\nThen fit every possible ARIMA models across a grid of (p, d, q) values.\n\nThen evaluate the models performance by calculating the maximum log-likelihood then weight them with AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).\n\nThen visualize the fitted values from every possible models, alongside the actual data.\n\nFrom the visual, highlight the best model in red.\n\nOptionally, you can make it interactive, using ‘ggiraph’, and I prepare it so that you can hover and explore model fits.\n\nThe packages used:\n\nbox (v1.2.0)\nggplot2 (v4.0.0)\nggiraph (v0.9.1)\npurrr (v1.0.2)\ndplyr (v1.1.4)\nforecast (v8.23.0)\nglue (v1.7.0)\ntidyr (v1.3.1)\nrlang (v1.1.4)\nscales (v1.4.0)"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#simulating-data",
    "href": "posts/02-arima-grid-search/index.html#simulating-data",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R",
    "section": "\n1.2 Simulating Data",
    "text": "1.2 Simulating Data\nWe generate a synthetic dataset with some trend and randomness:\n\nset.seed(123)\nts_sim = runif(365, 5, 10) + seq(-140, 224)^2 / 10000\nday = as.Date(\"2025-06-14\") - 0:364\n\nThis produces 365 daily observations with both trend and noise, which is a good test case for ARIMA."
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#fitting-multiple-arima-models",
    "href": "posts/02-arima-grid-search/index.html#fitting-multiple-arima-models",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R",
    "section": "\n1.3 Fitting Multiple ARIMA Models",
    "text": "1.3 Fitting Multiple ARIMA Models\nWe test a grid of ARIMA parameters:\n\\[\n\\begin{aligned}\np &\\in \\{0,1,2\\} \\\\\nd &\\in \\{0,1\\} \\\\\nq &\\in \\{0,1,2\\}\n\\end{aligned}\n\\]\nWe exclude overly complex models where p + q &gt; 3.\n\nCodemodels = local({\n    box::use(\n        purrr[pmap, pmap_chr, possibly, map, map_dbl], \n        dplyr[transmute, mutate, filter, slice_min, slice, case_when], \n        forecast[Arima], \n        glue[glue], \n        tidyr[expand_grid], \n        rlang[exec]\n    )\n    \n    expand_grid(p = 0:2, d = 0:1, q = 0:2) |&gt; \n        transmute(\n            models = pmap_chr(\n                pick(1:3), \n                \\(p, d, q) glue(\"ARIMA({p},{d},{q})\")\n            ), \n            res = pmap(\n                pick(1:3),\n                possibly(\n                    function (p, d, q) {\n                        if (p + q &gt; 3) return(NULL)\n                        exec(Arima, as.ts(ts_sim), order = c(p, d, q))\n                    },\n                    otherwise = NULL\n                )\n            ), \n            fits = map(res, ~ if(is.null(.x)) NULL else fitted(.x)),\n            aic = map_dbl(res, ~ if(is.null(.x)) NA_real_ else AIC(.x)),\n            bic = map_dbl(res, ~ if(is.null(.x)) NA_real_ else BIC(.x))\n        ) |&gt; \n        filter(!is.na(aic)) |&gt;  # Remove failed models\n        mutate(\n            day = list(day),\n            is_lowest_aic = aic == min(aic, na.rm = TRUE),\n            is_lowest_bic = bic == min(bic, na.rm = TRUE)\n        )\n})\nmodels\n\n\n  \n\n\n\nThis gives us a nested data frame of fitted models with their AIC, BIC, and fitted values."
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#visualizing-models-with-ggplot2",
    "href": "posts/02-arima-grid-search/index.html#visualizing-models-with-ggplot2",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R",
    "section": "\n1.4 Visualizing Models with ggplot2",
    "text": "1.4 Visualizing Models with ggplot2\nDo you want to visualize everything better, including the actual, fitted values, and highlight the fitted values made by the best fit?\nFrom models data, we just need the is_lowest_aic and is_lowest_bic. We just need to tweak the data a little bit here by expanding the fitted values with its corresponding data value. Then, set the model_type to condition the plotting data with dplyr::case_when().\n\nplot_data = local({\n    box::use(\n        tidyr[unnest], \n        dplyr[mutate, case_when]\n    )\n    models |&gt; \n        unnest(cols = c(day, fits)) |&gt;\n        mutate(\n            model_type = case_when(\n                is_lowest_aic ~ \"Best AIC\",\n                is_lowest_bic ~ \"Best BIC\", \n                TRUE ~ \"Other Models\"\n            )\n        )\n})\n\nOptionally:\n\n\nYou can put the information about the model which had the lowest AIC and BIC in an annotated box text in the plot.\n\nbest_model_1 = models |&gt; dplyr::filter(is_lowest_aic)\nbest_model_2 = models |&gt; dplyr::filter(is_lowest_bic)\n\n\n\nPointing out the maximum value of the time series data\n\nmax_val = max(ts_sim)\nmax_idx = which.max(ts_sim)\nmax_day = day[max_idx]\n\n\n\nThen, visualize:\n\nCodelocal({\n    box::use(\n        ggplot2[...],\n        scales[comma],\n        dplyr[filter], \n        glue[glue], \n    )\n    \n    p = ggplot() + \n        # Original data\n        geom_line(\n            aes(x = day, y = as.numeric(ts_sim)), \n            linewidth = 1.2, \n            color = \"#2C3E50\",\n            alpha = 0.8\n        ) + \n        geom_point(\n            aes(x = day, y = as.numeric(ts_sim)), \n            size = 0.8, \n            color = \"#2C3E50\", \n            alpha = 0.6\n        ) + \n        \n        # Other fitted models (light gray)\n        geom_line(\n            data = filter(plot_data, model_type == \"Other Models\"), \n            aes(x = day, y = fits, group = models), \n            color = '#BDC3C7',\n            alpha = 0.6,\n            linewidth = 0.5\n        ) + \n        \n        # Best BIC model (blue)\n        geom_line(\n            data = filter(plot_data, model_type == \"Best BIC\"), \n            aes(x = day, y = fits, group = models), \n            color = '#3498DB',\n            linewidth = 1.2,\n            alpha = 0.9\n        ) + \n        \n        # Best AIC model (red, on top)\n        geom_line(\n            data = filter(plot_data, model_type == \"Best AIC\"), \n            aes(x = day, y = fits, group = models), \n            color = '#E74C3C',\n            linewidth = 1.5,\n            alpha = 0.9\n        ) + \n        \n        # Peak annotation\n        annotate(\n            \"point\",\n            x = max_day, y = max_val,\n            size = 4, colour = \"#E74C3C\", shape = 21, stroke = 2, fill = \"white\"\n        ) + \n        annotate(\n            \"text\", \n            x = max_day + 25,\n            y = max_val + 0.5,\n            label = paste0(\"Peak: \", round(max_val, 1), \" sec\"), \n            color = \"#E74C3C\",\n            fontface = \"bold\",\n            size = 3.5\n        ) +\n        annotate(\n            \"curve\", \n            x = max_day + 20, xend = max_day + 0.1, \n            y = max_val + 0.3, yend = max_val, \n            linewidth = 0.8,\n            color = \"#E74C3C\", \n            curvature = -0.2,\n            arrow = arrow(length = unit(0.15, \"cm\"), type = \"closed\")\n        ) +\n        \n        # Model performance text box\n        annotate(\n            \"rect\",\n            xmin = min(day) + 20, xmax = min(day) + 100,\n            ymin = max(ts_sim) - 1.5, ymax = max(ts_sim) - 0.2,\n            fill = \"white\", color = \"#34495E\", alpha = 0.9\n        ) +\n        annotate(\n            \"text\",\n            x = min(day) + 60, y = max(ts_sim) - 0.5,\n            label = paste0(\"Best AIC: \", best_model_1$models, \n                           \"\\nAIC = \", round(best_model_1$aic, 1)),\n            color = \"#E74C3C\", fontface = \"bold\", size = 3.2\n        ) +\n        annotate(\n            \"text\",\n            x = min(day) + 60, y = max(ts_sim) - 1.1,\n            label = paste0(\"Best BIC: \", best_model_2$models,\n                           \"\\nBIC = \", round(best_model_2$bic, 1)),\n            color = \"#3498DB\", fontface = \"bold\", size = 3.2\n        ) +\n        \n        # Styling\n        scale_x_date(date_labels = \"%b %Y\", date_breaks = \"2 months\") +\n        scale_y_continuous(labels = comma) +\n        \n        labs(\n            x = \"Time Index\", \n            y = \"Simulated Response Values\", \n            title = \"ARIMA Model Grid Search: Simulated Time Series Analysis\",\n            subtitle = glue(\"Comparing {nrow(models)} successful ARIMA models • Best performing models highlighted\"),\n            caption = paste0(\"Models tested: p∈[0,2], d∈[0,1], q∈[0,2] • \",\n                             \"Original data shown in dark gray • \",\n                             \"Total observations: \", length(ts_sim))\n        ) + \n        \n        theme_minimal(base_size = 11, base_family = \"serif\") +\n        theme(\n            plot.title = element_text(\n                family = \"serif\", \n                colour = \"#2C3E50\",\n                size = 14,\n                face = \"bold\",\n                margin = margin(b = 5)\n            ),\n            plot.subtitle = element_text(\n                family = \"serif\",\n                colour = \"#7F8C8D\",\n                size = 10,\n                margin = margin(b = 15)\n            ),\n            plot.caption = element_text(\n                family = \"serif\",\n                colour = \"#95A5A6\",\n                size = 8,\n                margin = margin(t = 10)\n            ),\n            axis.text.x = element_text(\n                angle = 45, \n                hjust = 1,\n                margin = margin(t = 8),\n                color = \"#34495E\"\n            ), \n            axis.text.y = element_text(\n                margin = margin(r = 8),\n                color = \"#34495E\"\n            ), \n            axis.title = element_text(\n                color = \"#2C3E50\",\n                face = \"bold\"\n            ),\n            panel.grid.minor = element_blank(),\n            panel.grid.major = element_line(\n                color = \"#ECF0F1\", \n                linewidth = 0.3\n            ), \n            panel.background = element_rect(fill = \"#FEFEFE\", color = NA),\n            plot.background = element_rect(fill = \"white\", color = NA),\n            plot.margin = margin(20, 20, 20, 20)\n        ) \n    \n    p\n})\n\n\n\n\n\n\n\nWe visualize:\n\nOriginal data (dark gray)\nAll fitted values from every possible ARIMA model (light gray), except, the fitted values from the best fit is highlighted (red, blue, based on AIC, BIC, respectively)\nAnnotated peak point in the data\nAnnotated best AIC/BIC values"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#optional-interactive-visualization-with-ggiraph",
    "href": "posts/02-arima-grid-search/index.html#optional-interactive-visualization-with-ggiraph",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R",
    "section": "\n1.5 Optional: Interactive Visualization with ggiraph",
    "text": "1.5 Optional: Interactive Visualization with ggiraph\nIf you prefer your plot to be interactive like some figures in the website, use ‘ggiraph’ interactive interface version of ggplot2, then girafe(). The output produced by girafe() is wrapped with HTML, so it can be run in web.\nI recommend ‘ggiraph’ to build web applications in R.\nThis is the interactive version of the plot above:\n\nCodelocal({\n    box::use(\n        ggplot2[...],\n        scales[comma],\n        dplyr[filter, mutate, case_when], \n        glue[glue], \n        ggiraph[geom_line_interactive, geom_point_interactive, girafe, opts_hover, opts_hover_inv, opts_selection], \n        tidyr[unnest]\n    )\n    \n    # Use this for preparation\n    original_data = data.frame(\n        day = day,\n        ts_sim = ts_sim,\n        tooltip_line = \"actual data\",\n        tooltip_point = paste0(format(day, \"%Y-%m-%d\"), \"; readings: \", round(ts_sim, 1), \" secs\")\n    )\n    \n    plot_data = models |&gt; \n        unnest(cols = c(day, fits)) |&gt;\n        mutate(\n            model_type = case_when(\n                is_lowest_aic ~ \"Best AIC\",\n                is_lowest_bic ~ \"Best BIC\", \n                TRUE ~ \"Other Models\"\n            ),\n            # Create tooltip text for model lines\n            tooltip_text = case_when(\n                is_lowest_aic ~ paste0(\"best model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2)),\n                is_lowest_bic ~ paste0(\"best model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2)),\n                TRUE ~ paste0(\"model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2))\n            )\n        )\n  \n    p = ggplot() + \n        geom_line_interactive(\n            data = original_data,\n            aes(x = day, y = ts_sim, tooltip = tooltip_line, data_id = \"original_line\"), \n            linewidth = 1.2, \n            color = \"#2C3E50\",\n            alpha = 0.8\n        ) + \n        geom_point_interactive(\n            data = original_data,\n            aes(x = day, y = ts_sim, tooltip = tooltip_point), \n            size = 0.8, \n            color = \"#2C3E50\", \n            alpha = 0.6\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Other Models\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#BDC3C7',\n            alpha = 0.6,\n            linewidth = 0.5\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Best BIC\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#3498DB',\n            linewidth = 1.2,\n            alpha = 0.9\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Best AIC\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#E74C3C',\n            linewidth = 1.5,\n            alpha = 0.9\n        ) + \n        \n        scale_x_date(date_labels = \"%b %Y\", date_breaks = \"2 months\") +\n        scale_y_continuous(labels = comma) +\n        \n        labs(\n            x = \"Time Index\", \n            y = \"Simulated Response Values\", \n            title = \"ARIMA Model Grid Search: Simulated Time Series Analysis\",\n            subtitle = glue(\"Comparing {nrow(models)} successful ARIMA models • Best performing models highlighted\"),\n            caption = paste0(\"Models tested: p∈[0,2], d∈[0,1], q∈[0,2] • \",\n                             \"Original data shown in dark gray • \",\n                             \"Total observations: \", length(ts_sim))\n        ) + \n        \n        theme_minimal(base_size = 11, base_family = \"serif\") +\n        theme(\n            plot.title = element_text(\n                family = \"serif\", \n                colour = \"#2C3E50\",\n                size = 14,\n                face = \"bold\",\n                margin = margin(b = 5)\n            ),\n            plot.subtitle = element_text(\n                family = \"serif\",\n                colour = \"#7F8C8D\",\n                size = 10,\n                margin = margin(b = 15)\n            ),\n            plot.caption = element_text(\n                family = \"serif\",\n                colour = \"#95A5A6\",\n                size = 8,\n                margin = margin(t = 10)\n            ),\n            axis.text.x = element_text(\n                angle = 45, \n                hjust = 1,\n                margin = margin(t = 8),\n                color = \"#34495E\"\n            ), \n            axis.text.y = element_text(\n                margin = margin(r = 8),\n                color = \"#34495E\"\n            ), \n            axis.title = element_text(\n                color = \"#2C3E50\",\n                face = \"bold\"\n            ),\n            panel.grid.minor = element_blank(),\n            panel.grid.major = element_line(\n                color = \"#ECF0F1\", \n                linewidth = 0.3\n            ), \n            panel.background = element_rect(fill = \"#FEFEFE\", color = NA),\n            plot.background = element_rect(fill = \"white\", color = NA),\n            plot.margin = margin(20, 20, 20, 20)\n        ) \n    \n    interactive_plot = girafe(\n        ggobj = p,\n        options = list(\n            opts_hover(css = \"cursor:pointer;stroke-width:4;stroke-opacity:1;fill-opacity:1;r:4px;\"),\n            opts_hover_inv(css = \"opacity:0.1;\"),\n            opts_selection(type = \"none\")\n        )\n    )\n    \n    interactive_plot\n})"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#disclaimer",
    "href": "posts/02-arima-grid-search/index.html#disclaimer",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R",
    "section": "\n1.6 Disclaimer",
    "text": "1.6 Disclaimer\nThis is just a toy example of leveraging functional programming and basic hyperparameter tuning for time series in R, and some of my learning competencies about data visualization in R and how to get deeper in it.\nIf you are interested to learn more, check out my other gists."
  },
  {
    "objectID": "posts/04-tidyselect-helpers/index.html",
    "href": "posts/04-tidyselect-helpers/index.html",
    "title": "The Hidden Magic of Tidy-Select: R’s Universal Column Selection Language",
    "section": "",
    "text": "1 Introduction\nHave you ever wondered how where(), starts_with(), and other selection helpers work seamlessly across different tidyverse packages? I recently discovered something surprising: you can actually use these functions in dplyr, tidyr, and other packages that invokes &lt;tidy-select&gt; API, without explicitly loading them.\nHere’s how it works:\n\niris |&gt; \n    tidyr::pivot_longer(\n        cols = where(is.numeric), # using `where()` w/out calling dplyr / tidyselect\n        names_to = 'Variable',\n        values_to = 'Measure'\n    )\n\n\n  \n\n\n\nTake note that I never load tidyselect and dplyr (the where() function in dplyr is just one of many re-exports). Yet, where() works perfectly. It doesn’t belong to / re-exported by tidyr, but you can use where(), if and only if the functions is invoking &lt;tidy-select&gt; API.\n\n2 What Are These Functions Called?\nThese are officially called tidyselect helpers (or “selection language”). They’re part of the tidyselect package, which provides a domain-specific language (DSL) for selecting columns in data frames.\nYou might also hear them referred to as:\n\nSelection helper functions\n\n&lt;Tidy-select&gt; helpers\nColumn selection helpers\n\n3 The Complete Family of Selection Helpers\nThe tidyselect package can be divided into 3 categories of helpers.\n\n\nPattern Matching Helpers\nPredicate-Based Helpers\n“Positional” Helpers\n\n\n\n\n\nColumns starting with a prefix\n\niris |&gt; \n    dplyr::select(starts_with(\"Sepal\")) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\nColumns ending with a suffix\n\niris |&gt; \n    dplyr::select(ends_with(\"Width\")) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\nColumns containing a literal string\n\niris |&gt; \n    dplyr::select(contains(\"al\")) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\nColumns matching a regular expression\n\niris |&gt; \n    dplyr::select(matches(\"^Sepal\")) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\nColumns following the number pattern\n\niris |&gt; \n    dplyr::select(num_range('x', 1:4)) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\n\n\nThe where() function is similar to SQL WHERE, except it is functional that (should) returns a Boolean value that satisfies the condition.\n\niris |&gt; \n    dplyr::select(where(is.numeric)) |&gt; \n    head(3)\n\n\n  \n\n\niris |&gt; \n    dplyr::select(where(is.factor)) |&gt; \n    head(3)\n\n\n  \n\n\niris |&gt; \n    dplyr::select(where(\\(col) is.numeric(col) && mean(col) &gt; 3.5))\n\n\n  \n\n\n\n\n\nThese are functions that select columns based on their position in the data frame\n\n\neverything()\n\niris |&gt; \n    dplyr::select(everything()) |&gt; \n    head(3)\n\n\n  \n\n\n\nThis is equivalent to relocate(iris, Species):\n\niris |&gt; \n    dplyr::select(Species, everything()) |&gt; \n    head(3)\n\n\n  \n\n\n\n\nlast_col()\n\n\niris |&gt; \n    dplyr::select(last_col()) |&gt; \n    head(3)\n\n\n  \n\n\n\nOffset: 2nd to the last\n\niris |&gt; \n    dplyr::select(last_col(1)) |&gt; \n    head(3)\n\n\n  \n\n\n\nOffset: Multiple columns from the end\n\niris |&gt; \n    dplyr::select(last_col(2):last_col()) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\n\nNoticed that I invoked most of &lt;tidy-select&gt; helpers, but never loaded dplyr or tidyselect, not once, just to use them.\n\n4 “Data-Masking” Subset\nJust like those &lt;tidy-select&gt; helpers, some functions found in dplyr, but doesn’t in tidyselect. These are all the functions that can be used within “data-masking” functions, such as dplyr::mutate() and dplyr::summarise(). Take a note of the term “within”, which means, you can’t use them outside from “data-masking” functions.\nI call across(), if_any(), and if_all() as projection helpers because they correspond to the SELECT clause in SQL, except they both if_any(), and if_all() map over the selected columns and returns the Boolean vector, while the across() function modifies the selected columns. The pick() function, on the other hand serves as a complement of across() by extracting them as a data frame, however, this only applies to subset a data frame to be invoked within the operations in “data-masking” functions. All of them can make use of the &lt;tidy-select&gt; API, meaning you can apply selectors like starts_with() or everything() to specify which columns to project.\n\n\nUsing pick()\nUsing across()\nUsing if_all() / if_any()\nFunctions you actually need to attach\n\n\n\nHere’s an example: Calculating mean and standard deviation\n\niris |&gt; \n    dplyr::group_by(Species) |&gt; \n    dplyr::summarise(\n        summary = list({\n            num = pick(where(is.numeric))\n            tibble::tibble(\n                vars = colnames(num), \n                mean = colMeans(num),\n                sd = apply(num, 2, sd)\n            )\n        })\n    ) |&gt; \n    tidyr::unnest(summary)\n\n\n  \n\n\n\n\nI am aware there’s a better approach to calculate the mean and standard deviation of each column by group.\n\n\n\nHere’s an example: Apply min-max normalization among numeric columns in iris dataset\n\niris |&gt; \n    dplyr::as_tibble() |&gt; \n    dplyr::mutate(\n        across(\n            where(is.numeric), \n            \\(col) { col - min(col) } / { max(col) - min(col) }\n        )\n    )\n\n\n  \n\n\n\nAnd once again, I never attach dplyr into the search path just to use across() and pick().\nYou can use across() in some dplyr “data-masking” function like filter(), but this is a deprecated behavior and attaching dplyr package is required.\n\n\nExample: Removing all missing values across all columns in airquality data frame\n\nairquality |&gt; \n    dplyr::filter(if_all(everything(), \\(col) !is.na(col))) |&gt; \n    head(5)\n\n\n  \n\n\n\nIf if_all() / if_any() is used outside filter(), those functions need dplyr package to be attached to use them.\n\n\nThough, there are some exceptions: there are helper functions you actually need dplyr to be attached to use them, otherwise they don’t work and R will throw an error.\nHere they are:\n\nlibrary(dplyr)\n\n\n\nn()\n\niris |&gt;\n    group_by(Species) |&gt; \n    slice_max(n = 20, order_by = Sepal.Length) |&gt; \n    summarise(\n        count = n(), # 👈 \n        m_sl = mean(Sepal.Length)\n    )\n\n\n  \n\n\n\n\n\ncur_group()\n\nmtcars |&gt;\n    group_by(cyl) |&gt;\n    reframe({\n        model = lm(mpg ~ wt, data = cur_group()) # 👈 \n        coefs = coef(model)\n\n        tibble(\n            terms = names(coefs), \n            estimate = coefs\n        )\n    })\n\n\n  \n\n\n\n\n\ncur_group_id()\n\nstarwars |&gt;\n    group_by(species) |&gt;\n    reframe(\n        species, \n        name, \n        hierarchical_id = sprintf(\"%02d-%03d\", cur_group_id(), row_number()) # 👈 \n    ) |&gt; \n    slice_min(hierarchical_id, n = 15)\n\n\n  \n\n\n\n\n\ncur_group_rows()\n\niris |&gt; \n    group_by(Species) |&gt; \n    slice_sample(\n        n = 75, replace = TRUE\n    ) |&gt; \n    summarise(\n        m_sl = mean(Sepal.Length),\n        n = {length(cur_group_rows()) + 30} # 👈 \n    )\n\n\n  \n\n\n\n\n\ncur_column()\n\niris |&gt; \n    as_tibble() |&gt; \n    transmute(\n        across(\n            where(is.numeric),\n            \\(col) {\n                if (stringr::str_detect(cur_column(), \"Sepal\")) { # 👈 \n                    col - mean(col)\n                } else if (stringr::str_detect(cur_column(), \"Petal\")) { # 👈 \n                    (col - mean(col)) / sd(col)\n                } else {\n                    col\n                }\n            }\n        )\n    )\n\n\n  \n\n\n\n\n\n\n\n\n\n5 Conclusion\nI hope they don’t change this soon, it is quite a nice feature (definitely not a bug 😋), assembling the DSL strengths across tidyverse APIs. Even if it is subtle. I still suggest you to attach these functions (through e.g. library() and box::use()) for better maintainability."
  },
  {
    "objectID": "posts/06-load-pkg/index.html",
    "href": "posts/06-load-pkg/index.html",
    "title": "Ways to load / attach packages in R",
    "section": "",
    "text": "Isn’t it great that R has more than 1 solution to load packages? Some of them are beautiful. Some of them should be illegal in at least three countries. Let’s rank them from “please never do this” to “finally, some good food.”\nIn this post, I will try enumerate the different ways to load packages in R, and discuss their pros and cons. I will also rank them from worst to best solution in practices."
  },
  {
    "objectID": "posts/06-load-pkg/index.html#base-use",
    "href": "posts/06-load-pkg/index.html#base-use",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.1 The new base::use() function (v4.4.0+)",
    "text": "1.1 The new base::use() function (v4.4.0+)\nUpdate: When I discover the bug, thanks to u/guepier and his comment, this changes my mind. Now, I put this in the worst place amongst the solution I listed here.\nI feel like R Core saw the chaos and said “fine, we’ll do something”. This function is available in R version 4.4.0 and above, by the way.\nIt allows you to load packages in a way that minimizes namespace conflicts by only attaching the functions you explicitly use. Take note that base::use() is a short case of library(), a simple wrapper, where it keeps include.only and set:\n\n\nlib.loc to NULL\n\n\ncharacter.only to TRUE\n\n\nlogical.return to TRUE\n\n\nattach.required to FALSE\n\n\n\nuse('pkg', c('obj1', 'fun1'))\n\nThis is still library(), but granular imports are explicit. Except…\n\nAnother problem occurs: Remember, it is just a simple wrapper of library(), therefore the import still goes to the search path.\nIt’s like putting a fancy new paint job on a 1987 Honda Civic and calling it a Ferrari. It LOOKS different, but under the hood, same old engine, baby.\nFor example:\n\nmean_data = function(.data) {\n    use('dplyr', 'summarise')\n    use('tidyr', 'pivot_longer')\n    \n    summarise(\n        .data, across(\n            where(is.numeric), \n            \\(col) mean(col, na.rm = TRUE)\n        )\n    ) |&gt; \n        pivot_longer(\n            cols = where(is.numeric), \n            names_to = \"Variable\", \n            values_to = \"Ave\"\n        )\n}\n\nmean_data(iris)\n\n# A tibble: 4 × 2\n  Variable       Ave\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Sepal.Length  5.84\n2 Sepal.Width   3.06\n3 Petal.Length  3.76\n4 Petal.Width   1.20\n\n\nAfter I execute mean_data(iris), the imports are accessible everywhere. EVERYWHERE!\nAnd base::use() is still broken even in the latest R versions.\nLike, it is so completely broken:\nuse('dplyr', 'mutate')\niris |&gt; mutate(Petal.Area = Petal.Length * Petal.Width)\n#&gt; Error in mutate(iris, Petal.Area = Petal.Length * Petal.Width) : \n#&gt;   could not find function \"mutate\"\n\nThe issue is that subsequent library() calls for an identical package are ignored, and the same is true for base::use(). Bananas. Completely broken.\n\n\n\n\n\n\n\nNote\n\n\n\nThis is noted by R core team:\n\nThis functionality is still experimental: interfaces may change in future versions."
  },
  {
    "objectID": "posts/06-load-pkg/index.html#require",
    "href": "posts/06-load-pkg/index.html#require",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.2 Worst: Using require()\n",
    "text": "1.2 Worst: Using require()\n\n\n~Just…no. I’ll be making a hot take here that sounds controversial, but this solution is the worst thing ever existed in R to attach the packages. ~\nUpdate: This solution may be bad, but at least not worse than base::use().\nThis function returns a Boolean value:\n\nrequire(pkg) |&gt; \n    suppressMessages() |&gt; \n    suppressWarnings() |&gt; \n    print()\n\n[1] FALSE\n\n\nIt returns TRUE if the package is successfully loaded and FALSE otherwise.\nAnd should only be applicable inside functions to check if a package is available.\n\ncheck_package = function() {\n    if (require(pkg, quietly = TRUE)) {\n        print(\"Package loaded successfully\")\n    } else {\n        print(\"Package not available\")\n    }\n}\ncheck_package()\n\n[1] \"Package not available\"\n\n\nUsing require() at the top of a script is how you get mysterious errors 50 lines later. Only acceptable inside functions when you actually check the return value.\nSeriously, this is just library() where you can place it at the top level of your script, but add another extra steps."
  },
  {
    "objectID": "posts/06-load-pkg/index.html#pacman",
    "href": "posts/06-load-pkg/index.html#pacman",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.3 The pro boxer: {pacman}\n",
    "text": "1.3 The pro boxer: {pacman}\n\nThis guy will punch you to death. Just kidding, Manny Pacquiao is a great boxer :). The pacman package tries to streamline package management with functions like p_load().\nDo you know this?\nif (!require(pkg)) {\n    install.packages(\"pkg\")\n    library(pkg)\n}\nWell, they made a shortcut, with pacman::p_load():\npacman::p_load(pkg)\nYou can do the same as above, except you can do this for multiple packages.\nHere’s how:\npacman::p_load(pkg1, pkg2, pkg3)\nSounds convenient, right?\n\nActually mixes two completely different responsibilities:\n\nInstallation (one-time setup)\nLoading (analysis step)\n\nGreat for interactive playtime. Disastrous in scripts, packages, CI/CD, or any environment without internet. Also:\n\nIt violates the single responsibility principle, harder than a toddler with a drum kit.\n\nThis is like a pineapple pizza"
  },
  {
    "objectID": "posts/06-load-pkg/index.html#library-classic",
    "href": "posts/06-load-pkg/index.html#library-classic",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.4 The classic library()\n",
    "text": "1.4 The classic library()\n\nSuch a classic function, isn’t it? After all, this is the most used function to attach R package namespace. It is a standard practice that most R users use, and it is safe: It will throw an error if pkg is not installed. This function is traditional and simple:\nlibrary(pkg)\nThat’s it, right?\nI hope it was that simple, but it has some serious downsides:\n\nIt attaches the entire package namespace to the search path,\nIt can lead to namespace clash, particularly if multiple packages have functions with the same name. This can make debugging difficult and lead to unexpected behaviors in your code.\nIt makes the imports unclear which functions come from which packages\nAll exported functions are available, even if you only need one or two\n\nTo detach the attached package namespace in the search path, use detach() function with package : keyword:\ndetach(package : pkg)\n\n\n\n\n\n\nWarning\n\n\n\nBe minded that library() function still potentially silently fails, even though it will throw an error, unlike require() where silent fails are always prominent."
  },
  {
    "objectID": "posts/06-load-pkg/index.html#combo-pack",
    "href": "posts/06-load-pkg/index.html#combo-pack",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.5 library() and {conflicted} package combo",
    "text": "1.5 library() and {conflicted} package combo\nHow about forcing the search path to select / deselect the imports? Introducing conflicted package.\n\nIn this approach, I combine traditional library() with the conflicted package to explicitly handle namespace conflicts.\nHow good? For example, I prefer using dplyr::filter() over stats::filter(), but a bit later on, I want to use stats::filter() when I want to run time series. The conflicted::conflict_prefer() handles which you want to declare “winners” of conflicts.\nI’ll make a scenario to make you understand:\n\n\nI have no use with stats::filter() because I only want to keep the data frame based on the condition using dplyr::filter(), and I want to load the entire dplyr namespace. Here, I declare dplyr::filter() as “winner” of the conflict:\n\nlibrary(dplyr)\n\nconflicted::conflict_prefer('filter', 'dplyr', 'stats')\nfilter(mtcars, cyl == 8)\n\n\n\nThen, I stopped using dplyr::filter() because I want to perform time series modelling with linear filtering using stats::filter(). Re-state stats::filter() as the “winner” of the conflict:\n\nconflicted::conflict_prefer('filter', 'stats', 'dplyr')\nfilter(1:10, rep(1, 3))\n\n\n\nStill loads everything. Still global. Still manual work. In my standard, this is actually good, but still not enough because it never allows granular imports and import aliasing, and besides, I’ve had better."
  },
  {
    "objectID": "posts/06-load-pkg/index.html#double-colon",
    "href": "posts/06-load-pkg/index.html#double-colon",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.6 Tedious but Explicit: The :: Operator",
    "text": "1.6 Tedious but Explicit: The :: Operator\n\nBefore packages like box and import introduced alternative import systems to R, the :: operator was (and still is) R’s built-in way to explicitly reference functions from specific namespaces without loading entire packages.\nThe :: operator is the most explicit base R solution for calling package functions. It’s part of R’s namespace system and requires no external dependencies - just base R.\nHere’s how:\n\nThe syntax is package::function(), which tells R exactly which package to pull the function from without attaching that package to your search path.\n\nMost of us using R are definitely using this (I am reusing an example from base::use):\n\nmean_data = function(.data) {\n    dplyr::summarise(\n        .data, across(\n            where(is.numeric), \n            \\(col) mean(col, na.rm = TRUE)\n        )\n    ) |&gt; \n        tidyr::pivot_longer(\n            cols = where(is.numeric), \n            names_to = \"Variable\", \n            values_to = \"Ave\"\n        )\n}\n\nmean_data(iris)\n\n# A tibble: 4 × 2\n  Variable       Ave\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Sepal.Length  5.84\n2 Sepal.Width   3.06\n3 Petal.Length  3.76\n4 Petal.Width   1.20\n\n\n\n\n\n\n\n\nNoteNotice\n\n\n\n\nNoticed that I don’t call dplyr:: for across() and where()? I have a blog talking about this.\n\n\n\nThis is great, compared to the previous solutions, no external packages needed and works mostly in any R version. The problem is this is way too verbose and repetitive, especially with many function calls:\nggplot2::ggplot(data, ggplot2::aes(date, y)) +\n    ggplot2::geom_point() + \n    ggplot2::geom_line() + \n    ggplot2::theme_minimal() + \n    ggplot2::labs(\n        x = \"Date (by month)\",\n        y = \"Value (in dollars)\", \n        title = \"Monthly Value in Dollar\"\n    )\nBeing typing-intensive is why I called this solution “tedious”.\n\nRespectable, but I bet nobody wants to type that many in 2025."
  },
  {
    "objectID": "posts/06-load-pkg/index.html#import-pack",
    "href": "posts/06-load-pkg/index.html#import-pack",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.7 Second to best: {import} package",
    "text": "1.7 Second to best: {import} package\nIt is so close!\n\nThis package is made before box. So, before box, the import package is the best solution ever made, arrived to fix library()’s most egregious issues. Created by Stefan Milton Bache (of pipe fame), it brings selective imports to R without requiring a complete paradigm shift.\n\n\nFirst Example\nSymbol binding\n\n\n\nThe first example is simple: Normal imports with aliases.\n\nimport::from(\n    dplyr, \n    select, rename, keep_when = filter, mutate, summarise, n\n)\nimport::from(tidyr, long = pivot_longer, wide = pivot_wider, drop_na)\nimport::from(ggplot2, diamonds, cut_width)\n\ndiamonds |&gt; \n    keep_when(\n        cut %in% c(\"Ideal\", \"Premium\"), \n        carat &gt; 1\n    ) |&gt; \n    drop_na() |&gt; \n    mutate(\n        price_per_carat = price / carat,\n        size_category = cut_width(carat, 0.5)\n    ) |&gt; \n    select(carat, cut, color, price, price_per_carat, size_category) |&gt; \n    wide(\n        names_from = cut,\n        values_from = price_per_carat,\n        values_fn = median\n    ) |&gt; \n    summarise(\n        across(c(Ideal, Premium), \\(col) mean(col, na.rm = TRUE)),\n        n = n()\n    )\n\n# A tibble: 1 × 3\n  Ideal Premium     n\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1 6494.   5978.  9951\n\n\n\n\nUse backticks around %&gt;% since it is under non-syntactic names.\n\nimport::from(dplyr, select, filter, mutate, summarise, n, relocate)\nimport::from(magrittr, `%&gt;%`) \nimport::from(tidyr, long = pivot_longer, wide = pivot_wider, drop_na)\n\nmtcars %&gt;% \n    filter(cyl == 6) %&gt;% \n    mutate(\n        hp_per_cyl = hp / cyl,\n        efficiency = mpg / disp\n    ) %&gt;% \n    select(mpg, disp, hp, hp_per_cyl, efficiency, everything()) %&gt;% \n    summarise(\n        across(\n            c(mpg, hp_per_cyl, efficiency), \n            list(\n                mu = \\(x) mean(x, na.rm = TRUE), \n                sigma = \\(x) sd(x, na.rm = TRUE)\n            ), \n            .names = \"{.col}..{.fn}\"\n        ),\n        n = n()\n    ) %&gt;% \n    long(\n        cols = contains(c(\"mu\", \"sigma\")), \n        names_sep = \"\\\\..\", \n        names_to = c(\"Variable\", \"Stat\"), \n        values_to = \"Est\"\n    ) %&gt;% \n    wide(\n        names_from = Stat, \n        values_from = Est\n    ) %&gt;% \n    relocate(n, .after = last_col()) %&gt;%\n    mutate(\n        se = sigma / sqrt(n), \n        cv = sigma / mu\n    )\n\n# A tibble: 3 × 6\n  Variable       mu  sigma     n      se     cv\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 mpg        19.7   1.45       7 0.549   0.0736\n2 hp_per_cyl 20.4   4.04       7 1.53    0.198 \n3 efficiency  0.112 0.0231     7 0.00872 0.206 \n\n\n\n\n\nIt’s so awesome. Why?\n\nNo masking\nExplicit at the top\nWorks with roxygen2 (@importFrom)\nImports the pipe like any other function\n\nThere’s still some limitations. Even though import provide necessities that solves my problem in R’s import system -\n\n\nIt has no unifying solution to attach the imports in the current environment. In fact, import functions still attach imported functions to the parent environment (usually global). What I mean is that they’re not truly scoped to a module or function. Thus, the use of import::here():\n\nwith(iris, {\n    import::here(stats, corr = cor)\n\n    corr(Sepal.Length, Petal.Length)\n})\n\n[1] 0.8717538\n\n\nThe expression we made, with(iris, { ... }) creates a temporary environment that disappears immediately. So corr() is placed exactly there, inside that temporary environment, and you cannot reuse corr() somewhere in the environment, even in the global environment\ncorr(1:10, 2:11)\n#&gt; Error in corr(1:10, 2:11) : could not find function \"corr\"\nThis is better than loading entire packages, but not as clean as lexical scoping.\n\nThe package was designed primarily for CRAN packages. File-based modules feel like an afterthought rather than a first-class feature.\nIt lacks support for nested module hierarchies. You can import from files with this package, but you can’t organize modules into sophisticated directory structures with their own internal dependencies.\nUnlike box, there’s no way to import a whole package as an object without attaching names"
  },
  {
    "objectID": "posts/06-load-pkg/index.html#box",
    "href": "posts/06-load-pkg/index.html#box",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.8 The ergonomically superior {box} package",
    "text": "1.8 The ergonomically superior {box} package\n\nFinally, some good food. \n\n\n\n\n\n\nNoteMy impression\n\n\n\nIn 2021, Konrad Rudolph looked at R’s prehistoric import system, said:\n\n“This is rubbish”\n\nDisclaimer: I don’t know if he said it, I said this just for fun ;)\nI strongly agree. And then dropped one of the magnum opus: box — he dropped it like Gordon Ramsay dropping a perfectly seared Wellington on the pass.\n\n\nThis isn’t “slightly nicer imports” — it’s a complete rethinking of how R package should be loaded, and R code should be organized and namespaced. It brings true module systems (like Python, JavaScript, or Ruby) to R.\nThere’s 4 of a kind to import like a sane person:\nbox::use(\n    purrr,                          # 1\n    tbl = tibble,                   # 2\n    dplyr = dplyr[filter, select],  # 3\n    stats[st_filter = filter, ...]  # 4\n)\n\nSource: https://github.com/klmr/box\n\n\n\nAttached the names? Nah, even better:\n\nImports the entire purrr package as an object.\n\nNothing goes into the search path.\n\nYou use it as purrr$map(), purrr$keep(), etc.\n\nZero risk of masking, zero pollution. Pure bliss.\n\n\n\nWhole package? But make it short\n\nSame vibe as 1, but you give the package a cute little nickname.\nNow you write tbl$tbl_df(), tbl$as_tibble(), etc.\n\nPerfect when you hate typing tibble:: but also hate global mess.\n\n\n\nI want the whole namespace… but only some names in my face.\n\nA killer move, actually: import the whole package as an object, and selectively attach only the functions you actually want to write naked.\nSo your pipelines stay clean: filter(), select(), mutate() — all smooth, drama-free.\n\nBut when you need the weird stuff, you still have the entire namespace sitting there like:\ndplyr$reconstruct_tibble_from_who_knows_what()\n\n\n\n\n“I refuse to be gaslit by stats::filter() ever again.”\n\n“I want everything from {stats} (because base R is already everywhere), but stats::filter() is a war criminal that keeps fighting with dplyr::filter().”\nSo basically, everything from {stats} is attached, but rename that one cursed function to st_filter() so it never bites me again.\nThe ... means “everything else, with their original names”.\n\n\n\nBut wait, there’s more!\nHere, watch the madness of how I apply box to load package deps:\nbox::use(\n    dplyr[\n        select, rename, \n        keep_when = filter,   # rename because we want to avoid needless fighting\n        mutate, summarise, \n        across, everything\n    ],\n    tidyr[pivot_longer, pivot_wider, drop_na],\n    magrittr[`%&gt;%`],          # yes, don't forget that the pipe is just another import\n    ggplot2[ggplot, aes, geom_point, theme_minimal, labs, ggsave],\n    lubridate[ymd, year, month, floor_date],\n    data.table[fread]         # because sometimes you need speed, not dignity\n)\nLess :: spam. No package::function() that makes your code look like it’s been hit by shrapnel. Zero library() / require().\nAnd then, the part that makes grown R programmers cry tears of joy — You are also allowed to reuse exported namespace from an R script or a folder as a module.\nbox::use(\n    app/models/glm_fit[...],           # brings everything exported\n    app/plots/theme_pub[theme_pub],    # only the theme\n    app/utils/cleaning[clean_names, fix_dates],\n    ./secret_sauce                     # local folder / script = module\n)\nWith box, you can create modules that encapsulate your code and its dependencies — another revolutionary and W move in R community. This package is making my life easier in managing and reuse code across different projects.\nThis approach aligns well with modern programming practices and helps to keep your codebase clean and maintainable.\n\n1.8.1 Little resources\nOther resources to learn more about this package:\n\nCRAN Index\nBox README\nMy book"
  },
  {
    "objectID": "posts/08-sql-r/index.html#introduction",
    "href": "posts/08-sql-r/index.html#introduction",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "1 Introduction",
    "text": "1 Introduction\nI am already using R since 2018, and uses SQL since around 2022-2023. Way back in 2023, I am learning one of the most valuable feature in R, and that’s the ability to integrate R into other software. That’s because I only use softwares independently, i.e. R only, Python only, etc. This is how I first learn SQL, and I learn few frameworks that integrates R and SQL databases.\nIf you’ve spent any time in data science, I am sure you encountered language wars and such debates — there’s like hundreds or maybe thousands of blogs spread in the community comparing which languages is better or worse. I am not talking about that in this blog post, however, here’s the thing — it’s not really a versus situation. SQL and R are like peanut butter and jelly. Each is good on its own, but why not both? Flavorful.\n\nRegardless, SQL excels at what databases do best: storing, organizing, and retrieving massive amounts of data with lightning speed. R, on the other hand, shines where creativity and complexity matter: statistical modeling, advanced visualizations, and transforming raw data into insights that actually mean something.\nIn this post, I’ll show you why combo-ing R and SQL isn’t just nice to have — it’s my stack. And more importantly, I’ll show you what I know how to make them work together so seamlessly you’ll wonder how you ever worked any other way."
  },
  {
    "objectID": "posts/08-sql-r/index.html#why-learn-sql-through-r",
    "href": "posts/08-sql-r/index.html#why-learn-sql-through-r",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "2 Why Learn SQL Through R?",
    "text": "2 Why Learn SQL Through R?\nBefore we dive into the technical details, let me explain why R is actually a fantastic environment for learning SQL:\n\nImmediate Feedback Loop\nWhen you’re learning SQL in a traditional database environment, you often need to set up servers, configure connections, and deal with authentication. With R, you can start writing queries in seconds and see results immediately in your familiar R environment.\nBest of Both Worlds\nYou can write pure SQL when you want to practice, or use {dplyr} syntax and see the generated SQL. This dual approach accelerates learning because you can:\n\nWrite {dplyr} code and inspect the SQL it produces\nCompare your hand-written SQL with {dbplyr}’s output\nGradually transition from {dplyr} comfort to SQL mastery\n\nVisualization Integration\nThe moment you query data, you can pipe it directly into {ggplot2} or other R visualization tools. No export/import cycles, no switching between applications—just seamless analysis.\nReproducible Workflows\nEverything lives in a script or R Markdown document. Your queries, analysis, and visualizations are all version-controlled and reproducible."
  },
  {
    "objectID": "posts/08-sql-r/index.html#tools-and-packages",
    "href": "posts/08-sql-r/index.html#tools-and-packages",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "3 Tools and Packages",
    "text": "3 Tools and Packages\nI can name few tools and packages on working with SQL databases in R, most especially when you just started. I don’t have any database in my own device, but did you know you can simulate databases? These are the tools and packages to start:\n\n{tidyverse} — Why this? This is a package that holds the complete set of tools in data science, and that includes working with databases. Speaking of which, this is a meta-package that also contains what we need: {dbplyr}, which also contains {DBI} package dependency.\n{box} — I already talked about this package in my previous blog posts. Please, take a look at them if you have some time:\n\nBox: Placing module system into R\nIn my “Ways to load / attach packages in R” blog post\n\n{dbplyr} — This is the magic translator. It converts your familiar {dplyr} code into SQL queries behind the scenes. You write R, it speaks SQL to the database. The best part? You can inspect the SQL it generates, which makes it a fantastic learning tool.\n{DBI} — Think of this as the universal adapter for database connections. It provides a consistent interface whether you’re connecting to SQLite, PostgreSQL, MySQL, or other databases. It handles the connection, sending queries, and fetching results.\n{RSQLite} — This is the R interface to SQLite databases. SQLite is perfect for learning because it’s lightweight, requires no server setup, and the entire database is just a single file on your computer.\n\nInstall them through this:\n\nNative RUsing {pak}\n\n\ninstall.packages(c('tidyverse', 'box', 'RSQLite'))\n\n\nInstall them directly\npak::pak(c(\n    'tidyverse', \n    'box', \n    'RSQLite'\n))\nWhen you preferred the development version\npak::pak(c(\n    \"tidyverse/tidyverse\", \n    \"klmr/box\", \n    \"r-dbi/RSQLite\"\n))"
  },
  {
    "objectID": "posts/08-sql-r/index.html#with-existing-database",
    "href": "posts/08-sql-r/index.html#with-existing-database",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "4 With existing database",
    "text": "4 With existing database\nI learn SQL thanks to SQLite. This is a language-agnostic library, written in C, that acts like a database while being lightweight. You can use it literally everywhere! It is also used to built into everywhere, it could be mobile phones and most computers.\nThanks to SQLite, I made a first move to learn SQL without installing heavy softwares, such as PostgreSQL and MySQL, just to learn SQL. Additionally, SQLite is an open-source, but not open for contribution (I believe this is designed for good purpose).\n\n4.1 Why SQLite is Perfect for Learning\nHere’s why SQLite is the ideal training ground:\n\nNo server required — It’s just a file on your computer\nZero configuration — No ports, users, or permissions to set up\nLightweight — Databases can be megabytes instead of gigabytes\nProduction-ready — Despite being “lite,” it’s used in production by major applications\nSQL standard — You learn real SQL that transfers to other databases\n\nIn a positive sense, R and SQL is a great combo. Maybe R and SQL is not a great combo for software development as Python and SQL combo, R and SQL can make a place in data analysis instead. As long as you have {DBI} and {RSQLite} installed in your R, you can now make a first move on integrating R and SQL, and you’re good to go.\n\n\n4.2 SQLite in R\nOh, you can definitely learn SQL and R at the same time, considering that SQLite is portable and lightweight. The only primary requirements are {DBI} and {RSQLite}. If you know how to write a query, you don’t need a compatible set of packages in {tidyverse} and {dbplyr}, otherwise, as long as you know how to use {tidyverse} packages, namely {dplyr}, {tidyr}, etc., you can use it instead.\nLet me show you how to connect to a SQLite database and work with it:\n\nbox::use(\n    DBI[dbConnect, dbWriteTable, dbDisconnect], \n    RSQLite[SQLite]\n)\n\n1\ncon = dbConnect(SQLite(), \"first_database.sqlite\")\n\n2\ndf = data.frame(\n    id = 1:5,\n    name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"),\n    age = c(25, 30, 35, 28, 42),\n    city = c(\"New York\", \"London\", \"Tokyo\", \"Paris\", \"Sydney\")\n)\n\n3\ndbWriteTable(\n    con, \n    \"customers\",\n    df, \n    overwrite = TRUE\n)\n\n\n1\n\nCreate a connection (this creates the database file if it doesn’t exist)\n\n2\n\nCreate some sample data\n\n3\n\nWrite the data as a table to the database\n\n\n\n\n\n\n\n\n\n\nNoteExplanation\n\n\n\n\n\n\nCreate a connection (this creates the database file if it doesn’t exist)\nCreate some sample data\nWrite the data as a table to the database\n\n\n\n\nWhat just happened?\nWe created a SQLite database file called first_database.sqlite in your working directory. Inside it, we created a table called customers with our sample data. If the file already exists, R simply connects to it.\n\n\n4.3 Two Ways to Query: Pure SQL vs {dplyr}\nNow comes the fun part—you can query this database in two different ways, and each has its benefits for learning.\n\nJust SQL{dplyr} API\n\n\nDid you know that with {knitr}, you can write SQL code chunks directly in your R Markdown or Quarto documents? This is incredibly handy for mixing SQL queries with your data analysis in R.\n```{sql connection=con}\nSELECT name, age, city \nFROM customers \nWHERE age &gt; 30\n```\nAlright, I wrote the SQL code chunk above again here for your reference:\n\nSELECT name, age, city \nFROM customers \nWHERE age &gt; 30\n\n\n2 records\n\n\nname\nage\ncity\n\n\n\n\nCharlie\n35\nTokyo\n\n\nEve\n42\nSydney\n\n\n\n\n\nOr you can store the query into a string, and send it via DBI::dbGetQuery(), placed in statement argument.\n\nDBI::dbGetQuery(\n    con, \n    \"SELECT name, age, city \\nFROM customers \\nWHERE age &gt; 30\"\n)\n\n     name age   city\n1 Charlie  35  Tokyo\n2     Eve  42 Sydney\n\n\nWhy this matters for learning:\nYou’re writing actual SQL. No training wheels. This builds muscle memory for SQL syntax and helps you think in terms of SQL operations: SELECT, FROM, WHERE, JOIN, GROUP BY, etc.\n\n\nNothing can make it so easy to work with databases using a familiar syntax with {dplyr}. Yes, with {dbplyr}, you can use {dplyr} functions to interact with your database tables as if they were regular data frames in R. Here’s how you can perform the same query using {dplyr}:\n\nbox::use(\n    dplyr[filter, select, tbl, collect, show_query]\n)\n\ncustomers_tbl = tbl(con, \"customers\")\n\nout = customers_tbl |&gt; \n    filter(age &gt; 30) |&gt; \n    select(name, age, city)\n\nout |&gt; collect()\n\n# A tibble: 2 × 3\n  name      age city  \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 Charlie    35 Tokyo \n2 Eve        42 Sydney\n\n\nThe magic behind this:\nWhen you use {dplyr} verbs on a database table, {dbplyr} doesn’t immediately execute anything. It’s lazy! It builds up the query and only executes it when you call collect(). This is efficient because:\n\nYou can chain many operations without multiple round-trips to the database\nOnly the final result set gets pulled into R memory\nThe heavy computation happens in the database where it’s optimized\n\nSee the generated SQL:\nHow about getting the SQL query generated by {dbplyr}? You can use the show_query() function to see the SQL that {dbplyr} generates for your {dplyr} code:\n\nout |&gt; \n    show_query()\n\n&lt;SQL&gt;\nSELECT `name`, `age`, `city`\nFROM `customers`\nWHERE (`age` &gt; 30.0)\n\n\nEasy, right? If you know R already, treat it as your SQL teacher! Write familiar {dplyr} code, then check the SQL translation. Over time, you’ll start to intuitively understand how filter() becomes WHERE, how select() becomes SELECT, and how more complex operations translate to SQL.\n\n\n\n\n\n4.4 Working with Query Results\nOnce you have query results, you can treat them like any R data frame:\n\nbox::use(\n    ggplot2[ggplot, aes, geom_col, theme_minimal, labs]\n)\n\n1\nresult_data = collect(out)\n\n2\nresult_data |&gt;  \n    ggplot(aes(x = name, y = age, fill = city)) +\n    geom_col() +\n    theme_minimal() +\n    labs(\n        title = \"Customers Over 30\",\n        x = \"Name\",\n        y = \"Age\"\n    )\n\n\n1\n\nRetrieve the data\n\n2\n\nNow you can analyze it with R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExplanation\n\n\n\n\n\n\nRetrieve the data\nNow you can analyze it with R\n\n\n\n\nThis is where the R + SQL combination really shines. You use SQL’s efficiency to get exactly the data you need, then R’s rich ecosystem for analysis and visualization.\n\n\n4.5 Database Hygiene\nAlways remember to close your database connections once you’re done:\n\ndbDisconnect(con)\n\nThis releases resources and ensures your database file isn’t locked. In practice, connections are also closed automatically when your R session ends, but it’s good practice to do it explicitly."
  },
  {
    "objectID": "posts/08-sql-r/index.html#learning-sql-in-r-without-a-server",
    "href": "posts/08-sql-r/index.html#learning-sql-in-r-without-a-server",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "5 Learning SQL in R without a server",
    "text": "5 Learning SQL in R without a server\nBut I know some of you wants to know what it looks like to use the existing database and then call it in R.\nI literally said in the introduction that you can simulate — I have another different meaning:\n\nUse simulate_* family functions in {dbplyr} package. These functions allow you to create in-memory database tables that mimic real database behavior without needing an actual database server. This is perfect for learning and testing SQL queries in R.\n\n\n5.1 What Are Simulated Connections?\nSimulated connections create an in-memory representation of how different database systems handle SQL. This means you can:\n\nSee how your {dplyr} code translates to different SQL dialects\nLearn SQL without any database installation\nTest queries before running them on production databases\nUnderstand the quirks of different database systems\n\n\n\n5.2 Demonstration: Simulating Microsoft SQL Server\nFirst, let me show you how it looks like to connect to a database, i.e. SQLite in this case, and work with it. Try imagine you have a SQL server, and you want to connect to it using R. Use simulate_mssql() function to simulate a Microsoft SQL Server database connection:\n\nbox::use(\n    dbplyr[simulate_mssql, tbl_lazy]\n)\n\n1con_sim = simulate_mssql()\n2customers_sim = tbl_lazy(df, con_sim)\n\n3customers_sim |&gt;\n    select(name, age, city) |&gt;\n    filter(age &gt; 30)\n\n\n1\n\nCreate a simulated MS SQL Server connection\n\n2\n\nCreate a (local) lazy table from our data frame\n\n3\n\nBuild a query\n\n\n\n\n&lt;SQL&gt;\nSELECT `name`, `age`, `city`\nFROM `df`\nWHERE (`age` &gt; 30.0)\n\n\n\n\n\n\n\n\nNoteExplanation\n\n\n\n\n\n\nCreate a simulated MS SQL Server connection\nCreate a (local) lazy table from our data frame\nBuild a query\n\n\n\n\nWe are performing a pure lazy evaluation in this step. Which means, we are not returning any information in the table and in the query process, we only generate the SQL code.\n\n\n5.3 Comparing SQL Dialects\nThe only issue when learning SQL with R in the past is when I found out that different databases have different SQL dialects. Let’s compare how PostgreSQL and Microsoft SQL Server handle the same query:\n\nLet’s import a bit first:\nbox::use(\n    dplyr[\n        keep_when = filter, arrange, desc, show_query, \n        summarise, mutate, relocate\n    ], \n    tidyr[\n        long = pivot_longer\n    ], \n    dbplyr[simulate_postgres, simulate_mssql], \n1    magrittr[`%&gt;%`]\n)\n\n\n1\n\nThis is totally optional, you still can use |&gt; base R pipes, after all.\n\n\n\n\n\n5.3.1 PostgreSQL version\n\ncon_postgres = simulate_postgres()\nmtcars_postgres = tbl_lazy(mtcars, con_postgres)\n\nmtcars_postgres %&gt;% \n    filter(cyl == 6) %&gt;% \n    mutate(\n        hp_per_cyl = hp / cyl,\n        efficiency = mpg / disp\n    ) %&gt;% \n    select(mpg, disp, hp, hp_per_cyl, efficiency, everything()) %&gt;% \n    summarise(\n        across(\n            c(mpg, hp_per_cyl, efficiency), \n            list(\n                mu = \\(x) mean(x, na.rm = TRUE), \n                sigma = \\(x) sd(x, na.rm = TRUE)\n            ), \n            .names = \"{.col}..{.fn}\"\n        ),\n        n = n()\n    ) %&gt;% \n    long(\n        cols = contains(c(\"mu\", \"sigma\")), \n        names_sep = \"\\\\..\", \n        names_to = c(\"Variable\", \"Stat\"), \n        values_to = \"Est\"\n    )\nClick to view generated SQL code\n\n&lt;SQL&gt;\nSELECT `n`, 'mpg' AS `Variable`, 'mu' AS `Stat`, `mpg..mu` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDDEV_SAMP(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDDEV_SAMP(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDDEV_SAMP(`efficiency`) AS `efficiency..sigma`,\n    COUNT(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'hp_per_cyl' AS `Variable`,\n  'mu' AS `Stat`,\n  `hp_per_cyl..mu` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDDEV_SAMP(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDDEV_SAMP(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDDEV_SAMP(`efficiency`) AS `efficiency..sigma`,\n    COUNT(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'efficiency' AS `Variable`,\n  'mu' AS `Stat`,\n  `efficiency..mu` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDDEV_SAMP(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDDEV_SAMP(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDDEV_SAMP(`efficiency`) AS `efficiency..sigma`,\n    COUNT(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT `n`, 'mpg' AS `Variable`, 'sigma' AS `Stat`, `mpg..sigma` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDDEV_SAMP(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDDEV_SAMP(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDDEV_SAMP(`efficiency`) AS `efficiency..sigma`,\n    COUNT(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'hp_per_cyl' AS `Variable`,\n  'sigma' AS `Stat`,\n  `hp_per_cyl..sigma` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDDEV_SAMP(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDDEV_SAMP(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDDEV_SAMP(`efficiency`) AS `efficiency..sigma`,\n    COUNT(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'efficiency' AS `Variable`,\n  'sigma' AS `Stat`,\n  `efficiency..sigma` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDDEV_SAMP(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDDEV_SAMP(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDDEV_SAMP(`efficiency`) AS `efficiency..sigma`,\n    COUNT(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\n\n\n\n\n\n5.3.2 Microsoft SQL Server version\n\ncon_mssql = simulate_mssql()\nmtcars_mssql = tbl_lazy(mtcars, con_mssql)\n\nmtcars_mssql %&gt;% \n    filter(cyl == 6) %&gt;% \n    mutate(\n        hp_per_cyl = hp / cyl,\n        efficiency = mpg / disp\n    ) %&gt;% \n    select(mpg, disp, hp, hp_per_cyl, efficiency, everything()) %&gt;% \n    summarise(\n        across(\n            c(mpg, hp_per_cyl, efficiency), \n            list(\n                mu = \\(x) mean(x, na.rm = TRUE), \n                sigma = \\(x) sd(x, na.rm = TRUE)\n            ), \n            .names = \"{.col}..{.fn}\"\n        ),\n        n = n()\n    ) %&gt;% \n    long(\n        cols = contains(c(\"mu\", \"sigma\")), \n        names_sep = \"\\\\..\", \n        names_to = c(\"Variable\", \"Stat\"), \n        values_to = \"Est\"\n    )\nClick to view generated SQL code\n\n&lt;SQL&gt;\nSELECT `n`, 'mpg' AS `Variable`, 'mu' AS `Stat`, `mpg..mu` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDEV(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDEV(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDEV(`efficiency`) AS `efficiency..sigma`,\n    COUNT_BIG(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'hp_per_cyl' AS `Variable`,\n  'mu' AS `Stat`,\n  `hp_per_cyl..mu` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDEV(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDEV(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDEV(`efficiency`) AS `efficiency..sigma`,\n    COUNT_BIG(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'efficiency' AS `Variable`,\n  'mu' AS `Stat`,\n  `efficiency..mu` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDEV(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDEV(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDEV(`efficiency`) AS `efficiency..sigma`,\n    COUNT_BIG(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT `n`, 'mpg' AS `Variable`, 'sigma' AS `Stat`, `mpg..sigma` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDEV(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDEV(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDEV(`efficiency`) AS `efficiency..sigma`,\n    COUNT_BIG(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'hp_per_cyl' AS `Variable`,\n  'sigma' AS `Stat`,\n  `hp_per_cyl..sigma` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDEV(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDEV(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDEV(`efficiency`) AS `efficiency..sigma`,\n    COUNT_BIG(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'efficiency' AS `Variable`,\n  'sigma' AS `Stat`,\n  `efficiency..sigma` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDEV(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDEV(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDEV(`efficiency`) AS `efficiency..sigma`,\n    COUNT_BIG(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\n\n\n\n\n\n\n\n5.4 When to Use Simulated Connections\nSimulated connections are perfect for:\n\nLearning: Practice SQL translation without database setup\nDevelopment: Test query logic before connecting to real databases\nDocumentation: Show how queries work across different systems\nTeaching: Demonstrate SQL concepts without infrastructure requirements"
  },
  {
    "objectID": "posts/08-sql-r/index.html#common-pitfalls-and-how-to-avoid-them",
    "href": "posts/08-sql-r/index.html#common-pitfalls-and-how-to-avoid-them",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "6 Common Pitfalls and How to Avoid Them",
    "text": "6 Common Pitfalls and How to Avoid Them\nSome are based on my experience, so some of the list can be opinionated.\n\n6.1 1. Collecting Too Early\nThe problem in R can be overdramatic with large amounts of data when read and processed into memory. Unnecessarily bringing entire tables into R memory creates performance bottlenecks and can even crash your R session when datasets exceed available RAM.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nChain operations and collect only when needed. Let the database do the heavy lifting with filtering, aggregating, and joining before bringing results into R.\n\n\n\n\n\n6.2 2. Not Checking Query Plans\nSlow queries are often the result of poor optimization, but many users never investigate why their queries take so long. Without examining the query plan, you’re flying blind—unable to identify bottlenecks, missing indexes, or inefficient joins.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nUse explain() to understand query execution. This reveals how the database processes your query and highlights optimization opportunities.\n\n\n\n\n\n6.3 3. Forgetting Database Differences\nSQL dialects vary between database systems, and code that runs perfectly in SQLite might fail spectacularly in PostgreSQL or MySQL. These differences range from subtle syntax variations to completely different function names and behaviors.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nTest with simulated connections and understand dialect differences. Use {dbplyr}’s simulate_*() functions to preview how queries translate across databases before deploying to production systems.\n\n\n\n\n\n6.4 4. Ignoring Indexes\nQueries that scan entire tables are a common performance killer, especially as datasets grow. Without proper indexes, your database must examine every single row to find matches, turning what should be millisecond queries into multi-second ordeals.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nLearn about indexes and how to create them for frequently queried columns. Understand which columns benefit from indexing and how composite indexes can optimize complex queries.\n\n\n\n\n\n6.5 5. Not Parameterizing Queries\nBuilding queries with string concatenation isn’t just inelegant—it’s dangerous. This practice opens your code to SQL injection attacks, where malicious input can execute arbitrary database commands, potentially exposing or destroying data.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nUse parameterized queries with DBI::dbBind() or stick with {dplyr} operations, which handle parameterization automatically. Never concatenate user input directly into SQL strings."
  },
  {
    "objectID": "posts/08-sql-r/index.html#conclusion",
    "href": "posts/08-sql-r/index.html#conclusion",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nSQL and R aren’t competitors — they’re collaborators. SQL is your data retrieval expert, getting you exactly the data you need with incredible efficiency. R is your analysis specialist, turning that data into insights, models, and visualizations. I can guarantee you that learning SQL through R is that you’re never starting from zero. Your existing R knowledge accelerates SQL mastery. The familiar {dplyr} syntax becomes your bridge to SQL fluency. And the ability to seamlessly move from database queries to statistical analysis to stunning visualizations—all in one environment—is genuinely powerful.\nThe data-driven professionals who thrive are those who can speak both languages fluently. They use SQL to ask databases the right questions and R to find the answers that matter. They understand when to leverage database optimization and when to bring data into R for complex transformations.\nThe journey from R user to R + SQL expert isn’t just about learning syntax—it’s about becoming someone who can efficiently bridge data storage and data science, who understands both the “how” of data retrieval and the “why” of data analysis.\nSo don’t pick sides. Master both. Your future self (and would be your future employers) will thank you."
  },
  {
    "objectID": "posts/10-formulas-r/index.html",
    "href": "posts/10-formulas-r/index.html",
    "title": "What’s so special about formulas in R",
    "section": "",
    "text": "If you’ve used R for any statistical modeling, I am certainly sure you’ve encountered formulas, like, by a lot.\nHowever, what if I told you they’re generally everywhere? For example:\nThe application is not limited to statistical modelling per se, where it describes the relationship between the variables, whereas the left hand side is the dependent variable, while the right hand side is the independent variable, for example. Also, have you ever wondered what makes them tick?\nFormula ~ in R is just another tool, most particular in metaprogramming, where it captures expressions in runtime without evaluating them.\nCan we dive deep into what makes formulas special and how you can leverage them in your own code?"
  },
  {
    "objectID": "posts/10-formulas-r/index.html#it-captures-both-the-expression-and-the-environment",
    "href": "posts/10-formulas-r/index.html#it-captures-both-the-expression-and-the-environment",
    "title": "What’s so special about formulas in R",
    "section": "\n3.1 It captures both the expression and the environment",
    "text": "3.1 It captures both the expression and the environment\nThe formulas themselves are, in fact, objects, which means you can extract and manipulate the expressions they contain. But they also carry something “hidden”: their environment. This dual nature: code plus context, is exactly what inspired quosures in rlang.\n\nf = y ~ x + z\nenvironment(f)\n#&gt;&gt; &lt;environment: R_GlobalEnv&gt;\n\nYou can even set a custom environment:\n\nf_custom = y ~ x\nenvironment(f_custom) = rlang::env(x = 10, y = 20)\nenvironment(f_custom)\n#&gt;&gt; &lt;environment: 0x000001db174bd0b0&gt;\n\nUnless needed, don’t try this in your applications\nEnvironment capture enables delayed evaluation. When you create a formula inside a function, it retains a snapshot of where it was born:\n\ncreate_formula = function(multiplier) {\n    ~ .x * multiplier\n}\n\nf_mult = create_formula(5)\nenvironment(f_mult)$multiplier  # The formula remembers!\n#&gt;&gt; [1] 5\n\n\nThe environment(f_mult)$multiplier actually remembers!"
  },
  {
    "objectID": "posts/10-formulas-r/index.html#difference-between-and-quo",
    "href": "posts/10-formulas-r/index.html#difference-between-and-quo",
    "title": "What’s so special about formulas in R",
    "section": "\n3.2 Difference between “~” and quo",
    "text": "3.2 Difference between “~” and quo\nBoth formulas and quosures are language objects at their core, but they differ in their class structure:\n\nclass(y ~ x)\n#&gt;&gt; [1] \"formula\"\n\nMeanwhile, quo() returns an object with dual inheritance — it’s both a quosure and a formula:\n\nclass(rlang::quo(y))\n#&gt;&gt; [1] \"quosure\" \"formula\"\n\nQuosures build on formulas by adding better introspection tools, cleaner printing, and support for quasiquotation (the ability to selectively evaluate parts of an expression).\n\nbox::use(\n    rlang[\n        quo, quo_get_expr, quo_get_env\n    ]\n)\n\nq = quo(mean(x, na.rm = TRUE))\n\nquo_get_expr(q)     # 1               \n#&gt;&gt; mean(x, na.rm = TRUE)\nquo_get_env(q)      # 2              \n#&gt;&gt; &lt;environment: R_GlobalEnv&gt;\nvalue = 10                            \nquo(x + !!value)    # 3         \n#&gt;&gt; &lt;quosure&gt;\n#&gt;&gt; expr: ^x + 10\n#&gt;&gt; env:  global\n\nI can explain each commented lines:\n\nYou can inspect the expression thanks to rlang::quo_get_expr().\nYou can inspect the environment thanks to rlang::quo_get_env().\nThe rlang package provides an good API for quasiquoation, by unquoting the value."
  },
  {
    "objectID": "posts/10-formulas-r/index.html#difference-between-and-enquo",
    "href": "posts/10-formulas-r/index.html#difference-between-and-enquo",
    "title": "What’s so special about formulas in R",
    "section": "\n3.3 Difference between “~” and enquo",
    "text": "3.3 Difference between “~” and enquo\nThere’s some noted difference between quo() and enquo() when capturing expressions.\n\n\nquo() takes the quosure of the expression you write.\n\nenquo() takes the quosure of the expression taken by the user input.\n\nBoth are still quosures and inspired by formulas in R, as they return both the same classes.\nNow, the distinction between using raw formulas (~) and enquo() is subtle but important when building functions that accept user expressions.\nFor instance:\n\n\nUsing ~ captures literally in the current scope\n\nmy_formula = function(x) {\n    f = ~ x\n    f\n}\n\nmy_formula(a + b)\n#&gt;&gt; ~x\n#&gt;&gt; &lt;environment: 0x000001db15d5db20&gt;\n\n\n\nrlang::enquo() captures what the caller passed\n\nmy_enquo = function(x) {\n    rlang::enquo(x)\n}\n\nmy_enquo(a + b)\n#&gt;&gt; &lt;quosure&gt;\n#&gt;&gt; expr: ^a + b\n#&gt;&gt; env:  global\n\n\n\n\n\n\n\n\n\nNoteGeneral rule of thumb between quo() and enquo()\n\n\n\nUse enquo() when building functions that accept user-supplied names or expressions. Use quo() when you’re programmatically constructing expressions yourself."
  },
  {
    "objectID": "posts/12-reusable-r-code/index.html",
    "href": "posts/12-reusable-r-code/index.html",
    "title": "Bother yourself writing reusable codes in R, not throwaway codes",
    "section": "",
    "text": "Why bother yourself writing reusable codes? There are piles of garbage codes in the wild"
  },
  {
    "objectID": "resources.html#main-website",
    "href": "resources.html#main-website",
    "title": "My resources",
    "section": "Main Website",
    "text": "Main Website\n\n\nHome: https://joshuamarie.com/\nServices\nBlog Posts: https://joshuamarie.com/posts/"
  },
  {
    "objectID": "resources.html#books",
    "href": "resources.html#books",
    "title": "My resources",
    "section": "Books",
    "text": "Books\n\n\nBox: Placing module system into R"
  },
  {
    "objectID": "resources.html#slides",
    "href": "resources.html#slides",
    "title": "My resources",
    "section": "Slides",
    "text": "Slides\n\n\nCuration of slides"
  },
  {
    "objectID": "posts/11-composing-r-function/index.html#nested-function-calls",
    "href": "posts/11-composing-r-function/index.html#nested-function-calls",
    "title": "Three levels to compose R functions",
    "section": "\n1.1 Nested Function Calls",
    "text": "1.1 Nested Function Calls\nThe most straightforward way to compose functions is nesting them:\n\nresult = sqrt(mean(log(c(1, 2, 3, 4, 5))))\nresult\n\n[1] 0.9785184\n\n\nWhile this works, it quickly becomes hard to read as complexity grows. You have to read from the inside out, which goes against our natural left-to-right reading pattern."
  },
  {
    "objectID": "posts/11-composing-r-function/index.html#using-the-pipe-operator",
    "href": "posts/11-composing-r-function/index.html#using-the-pipe-operator",
    "title": "Three levels to compose R functions",
    "section": "\n1.2 Using the Pipe Operator",
    "text": "1.2 Using the Pipe Operator\nThe pipe operator |&gt; (or %&gt;% from magrittr) makes composition much more readable:\n\nc(1, 2, 3, 4, 5) |&gt;\n    log() |&gt;\n    mean() |&gt;\n    sqrt()\n\n[1] 0.9785184\n\n\nThis is essentially manual composition with better syntax. Each function is still written out explicitly, but the flow is clearer.\nWanna learn more about pipes? Read this blog."
  },
  {
    "objectID": "posts/11-composing-r-function/index.html#creating-wrapper-functions",
    "href": "posts/11-composing-r-function/index.html#creating-wrapper-functions",
    "title": "Three levels to compose R functions",
    "section": "\n1.1 Creating Wrapper Functions",
    "text": "1.1 Creating Wrapper Functions\nSometimes we want to reuse a composition pattern. We can manually create a wrapper function with function keyword:\n\nnormalize = function(x) {\n    (x - mean(x)) / sd(x)\n}\n\ndata = c(10, 20, 30, 40, 50)\nnormalize(data)\n\n[1] -1.2649111 -0.6324555  0.0000000  0.6324555  1.2649111\n\n\nWhile this level of composition is explicit, straightforward, and easy to understand, it requires writing out each function definition manually, which can become repetitive when you have many similar transformations."
  },
  {
    "objectID": "posts/11-composing-r-function/index.html#application-in-functionals",
    "href": "posts/11-composing-r-function/index.html#application-in-functionals",
    "title": "Three levels to compose R functions",
    "section": "\n1.2 Application in functionals\n",
    "text": "1.2 Application in functionals\n\nMany base R and tidyverse functions are functionals: they take a function as input and apply it across data (e.g., sapply(), purrr::map(), integrate()).\nTo compose functions for functionals, instead of manually writing function from the outside of the call:\n\nsqrt2 = function(x) x^0.5\n\nsapply(1:5, sqrt2)\n\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nYou can compose on the fly with anonymous functions (you can just refer it as an “unassigned function” if you want):\n\nsapply(1:5, function(x) x^0.5)\n\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nOr use the modern shorthand \\() (after 4.1):\n\nsapply(1:5, \\(x) x^0.5)\n\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nEven better — show that \\() is just syntactic sugar for function():\n\nquote(\\(x) x^2)\n\nfunction(x) x^2"
  },
  {
    "objectID": "posts/12-reusable-r-code/index.html#start-with-functions",
    "href": "posts/12-reusable-r-code/index.html#start-with-functions",
    "title": "Bother yourself writing reusable codes in R, not throwaway codes",
    "section": "\n4.1 Start with Functions",
    "text": "4.1 Start with Functions\nEven if you think you’ll only use code once, wrap it in a function. Future you will thank present you.\nImagine this code is repeatable:\nlibrary(dplyr)\n\ndata = data %&gt;% \n    filter(!is.na(value), value &gt; 0)\nInstead, minimize a bit by writing a function that does the similar, but for any data frame:\n\n\n\nfilter.R\n\nlibrary(dplyr)\n\nretain_positive_value = function(data, var) {\n    data %&gt;%\n        filter(!is.na({{ var }}), {{ var }} &gt; 0)\n}\n\nretain_positive_value(data, value)\n\n\nTo know more what I did, please learn more about tidy evaluation.\nYou gotta have to store this function in some R script, R (and programming in general) cannot remember the codes you wrote and you execute, unless you saved the .Rdata, which is a big no-no. So, let’s go to another step."
  },
  {
    "objectID": "posts/12-reusable-r-code/index.html#sourcing-a-script",
    "href": "posts/12-reusable-r-code/index.html#sourcing-a-script",
    "title": "Bother yourself writing reusable codes in R, not throwaway codes",
    "section": "\n4.2 Sourcing a script",
    "text": "4.2 Sourcing a script\nAs you know and if you read my previous blog, I have some beefs with package import system, but I have personal beefs with code reusability in R in general. This includes “sourcing a script” using source() function.\n\nsource(\"./filter.R\")\n\nWhat’s the big matter about sourcing a script with source()?\n\nEverything from the sourced file goes into your global environment, resulting to a namespace clash.\n\nNo explicit imports: You don’t know what functions you’re actually using.\nYou need to source files in the right order.\n\nNo encapsulation: Functions can conflict with each other."
  },
  {
    "objectID": "posts/12-reusable-r-code/index.html#creating-an-r-package",
    "href": "posts/12-reusable-r-code/index.html#creating-an-r-package",
    "title": "Bother yourself writing reusable codes in R, not throwaway codes",
    "section": "\n4.3 Creating an R Package",
    "text": "4.3 Creating an R Package\nIf reusability is the problem, I mean, you could turn every project into an R package. But it is too heavy (even the implication of R package being “lightweight”), sometimes overkill, and unnecessary.\nThat’s because it:\n\nRequires understanding package structure\nNeeds DESCRIPTION, NAMESPACE, and other boilerplates\nMust follow CRAN conventions even for internal code (sometimes this is not necessary, but it is when publishing an R package to CRAN)\nOverhead of package development for simple projects\n\nAnd besides, the structure of R/ in your R package is ALWAYS flat. You can’t organize modules into subdirectories naturally."
  },
  {
    "objectID": "posts/12-reusable-r-code/index.html#organize-your-code-into-modules",
    "href": "posts/12-reusable-r-code/index.html#organize-your-code-into-modules",
    "title": "Bother yourself writing reusable codes in R, not throwaway codes",
    "section": "\n5.1 Organize Your Code into Modules",
    "text": "5.1 Organize Your Code into Modules\nInstead of one giant script, break your code into bunch of R scripts as logical modules:\n\n\ndata_loading.R - Functions for reading and importing data\n\ndata_cleaning.R - Functions for cleaning and validation\n\nanalysis.R - Core analytical functions\n\nvisualization.R - Plotting functions"
  },
  {
    "objectID": "posts/12-reusable-r-code/index.html#use-a-consistent-structure",
    "href": "posts/12-reusable-r-code/index.html#use-a-consistent-structure",
    "title": "Bother yourself writing reusable codes in R, not throwaway codes",
    "section": "\n5.2 Use a Consistent Structure",
    "text": "5.2 Use a Consistent Structure\nEvery project should follow a similar structure so you (and others) know where to find things. Just imagine you have a particular project:\nproject/\n├── R/\n│   ├── __init__.R       # &lt;------ This will mark `{./R}` folder as a module\n│   ├── data_loading.R\n│   ├── data_cleaning.R\n│   ├── analysis.R\n│   └── visualization.R\n├── data/\n├── output/\n└── main.R"
  },
  {
    "objectID": "posts/12-reusable-r-code/index.html#writing-a-module",
    "href": "posts/12-reusable-r-code/index.html#writing-a-module",
    "title": "Bother yourself writing reusable codes in R, not throwaway codes",
    "section": "\n5.3 Writing a module",
    "text": "5.3 Writing a module\nUnder R/analysis.R file, place this practical example code for the module that provides summary statistics:\n\n\nCode\n./R/analysis.R\n\nbox::use(\n    dplyr[\n        summarise, across, n, relocate, pick,\n        cur_group_id, matches\n    ],\n    tidyr[pivot_longer, pivot_wider]\n)\n\n#' @export\nsummary_data = function(data, vars, .by = NULL) {\n    mtcars |&gt;\n        summarise(\n            grp_id = cur_group_id(), \n            n = n(),\n            across(\n                {{ vars }},\n                list(\n                    mean = \\(x) mean(x, na.rm = TRUE),\n                    median = \\(x) median(x, na.rm = TRUE), \n                    q25 = \\(x) quantile(x, 0.25, na.rm = TRUE),\n                    q75 = \\(x) quantile(x, 0.75, na.rm = TRUE), \n                    sd = \\(x) sd(x, na.rm = TRUE),\n                    cv = \\(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE),\n                    iqr = \\(x) IQR(x, na.rm = TRUE),\n                    mad = \\(x) mad(x, na.rm = TRUE)  \n                ), \n                .names = \"{.col}..{.fn}\"\n            ),\n            \n            .by = {{ .by }}\n        ) |&gt; \n        pivot_longer(\n            cols = matches(\"\\\\.\\\\.\"), \n            names_pattern = \"(.+)\\\\.\\\\.(.+)\",  \n            names_to = c(\"variable\", \"statistic\"),\n            values_to = \"est\"\n        ) |&gt;\n        pivot_wider(\n            names_from = statistic,\n            values_from = est\n        ) |&gt;\n        relocate(n, .after = variable)\n}\n\n\n\n\nbox::use(pander[ptb = pandoc.table])\n\nmtcars |&gt; \n    summary_data(vars = c(mpg, hp, wt), .by = cyl) |&gt; \n    ptb()\nYes, it (just) works\n\n------------------------------------------------------------------------\n cyl   grp_id   variable   n    mean    median    q25     q75      sd   \n----- -------- ---------- ---- ------- -------- ------- ------- --------\n  6      1        mpg      7    19.74    19.7    18.65    21     1.454  \n\n  6      1         hp      7    122.3    110      110     123    24.26  \n\n  6      1         wt      7    3.117   3.215    2.822   3.44    0.3563 \n\n  4      2        mpg      11   26.66     26     22.8    30.4     4.51  \n\n  4      2         hp      11   82.64     91     65.5     96     20.93  \n\n  4      2         wt      11   2.286    2.2     1.885   2.622   0.5696 \n\n  8      3        mpg      14   15.1     15.2    14.4    16.25    2.56  \n\n  8      3         hp      14   209.2   192.5    176.2   241.2   50.98  \n\n  8      3         wt      14   3.999   3.755    3.533   4.014   0.7594 \n------------------------------------------------------------------------\n\nTable: Table continues below\n\n \n---------------------------\n   cv       iqr      mad   \n--------- -------- --------\n 0.07362    2.35    1.927  \n\n 0.1984      13     7.413  \n\n 0.1143    0.6175   0.3632 \n\n 0.1691     7.6     6.523  \n\n 0.2533     30.5    32.62  \n\n 0.2492    0.7375   0.5411 \n\n 0.1695     1.85    1.557  \n\n 0.2437      65     44.48  \n\n 0.1899    0.4812   0.4077 \n---------------------------\n\n\n\nNotice a few key things here:\n\nWe only import the specific dplyr and tidyr functions we need\n\n#' @export annotation: This marks the function as public (available when the module is imported)\nThis module has a function that does one thing: provide summary statistics\n\nThen reuse it by:\n\nbox::use(./R/analysis[summary_data])\n\nmtcars |&gt; \n    summary_data(vars = c(mpg, hp, wt))\n\nYou are also allowed to import multiple functions or even the entire module:\n\nbox::use(\n    # Import the module itself without attaching the names (access functions with summary$function_name)\n    ./analysis, \n    # Import specific names\n    ./analysis[summary_data, another_function], \n    # Attach all exported functions\n    ./analysis[...]\n)"
  },
  {
    "objectID": "posts/12-reusable-r-code/index.html#document-your-functions",
    "href": "posts/12-reusable-r-code/index.html#document-your-functions",
    "title": "Bother yourself writing reusable codes in R, not throwaway codes",
    "section": "\n5.4 Document Your Functions",
    "text": "5.4 Document Your Functions\nUse roxygen2-style comments even if you’re not building a package:\n\n\nCode\n./R/analysis.R\n\nbox::use(\n    dplyr[\n        summarise, across, n, relocate, pick,\n        cur_group_id, matches\n    ],\n    tidyr[pivot_longer, pivot_wider]\n)\n\n#' Get summary data from numeric column\n#' \n#' Calculate comprehensive summary statistics for numeric variables,\n#' including measures of central tendency, dispersion, and spread.\n#' \n#' @param data A data frame\n#' @param vars Vector of columns\n#' @param .by Optional grouping variable(s)\n#' \n#' @return A data frame with summary statistics in long format\n#' \n#' @examples \n#' mtcars |&gt; summary_data(vars = c(mpg, hp, wt))\n#' mtcars |&gt; summary_data(vars = c(mpg, hp), .by = cyl)\n#' \n#' @export\nsummary_data = function(data, vars, .by = NULL) {\n    mtcars |&gt;\n        summarise(\n            grp_id = cur_group_id(), \n            n = n(),\n            across(\n                {{ vars }},\n                list(\n                    mean = \\(x) mean(x, na.rm = TRUE),\n                    median = \\(x) median(x, na.rm = TRUE), \n                    q25 = \\(x) quantile(x, 0.25, na.rm = TRUE),\n                    q75 = \\(x) quantile(x, 0.75, na.rm = TRUE), \n                    sd = \\(x) sd(x, na.rm = TRUE),\n                    cv = \\(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE),\n                    iqr = \\(x) IQR(x, na.rm = TRUE),\n                    mad = \\(x) mad(x, na.rm = TRUE)  \n                ), \n                .names = \"{.col}..{.fn}\"\n            ),\n            \n            .by = {{ .by }}\n        ) |&gt; \n        pivot_longer(\n            cols = matches(\"\\\\.\\\\.\"), \n            names_pattern = \"(.+)\\\\.\\\\.(.+)\",  \n            names_to = c(\"variable\", \"statistic\"),\n            values_to = \"est\"\n        ) |&gt;\n        pivot_wider(\n            names_from = statistic,\n            values_from = est\n        ) |&gt;\n        relocate(n, .after = variable)\n}\n\n\n\nAnd you can access the documentation through box::help():\n\nbox::use(./R/analysis)\n\nbox::help(analysis$summary_data)"
  },
  {
    "objectID": "posts/13-careful-data/index.html#synthetic-example-dealing-with-dates",
    "href": "posts/13-careful-data/index.html#synthetic-example-dealing-with-dates",
    "title": "A ‘careful’ and small intro guide to data science with ‘tidyverse’",
    "section": "\n3.1 Synthetic example: Dealing with dates",
    "text": "3.1 Synthetic example: Dealing with dates\nHere, imagine you have this kind of data in a certain CSV, where it contains filedate/datetime columns that come:\n\nsample_csv = \"\ndate, readings, x, y, z\\n\n\\\"Mar.23,2005\\\",0.02,3.3,1.1,5.7\\n\n\\\"Feb.26,2005\\\",0.15,4.5,5.0,1.9\\n\n\\\"Apr.5,2005\\\",0.2,7.2,2.8,5.2\\n\n\\\"May.12,2005\\\", 0.5,4.9,1.3,6.8\n\"\n\nIf you directly import this CSV data:\n\nreadr::read_csv(I(sample_csv))\n\n# A tibble: 4 × 5\n  date        readings     x     y     z\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mar.23,2005     0.02   3.3   1.1   5.7\n2 Feb.26,2005     0.15   4.5   5     1.9\n3 Apr.5,2005      0.2    7.2   2.8   5.2\n4 May.12,2005     0.5    4.9   1.3   6.8\n\n\nThe problem arises when readr::read_csv() fails to parse the \"date\" column as an actual date type as you expect, instead it is parsed as a string type.\nThe readr package provides an API that can explicitly judge the column type.\nHere’s how you do it:\n\nbox::use(\n    readr[read_csv, cols, col_date]\n)\n\nread_csv(\n    I(sample_csv),\n    col_types = cols(\n        date = col_date(\"%b.%d,%Y\")\n    )\n)\n\n# A tibble: 4 × 5\n  date       readings     x     y     z\n  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2005-03-23     0.02   3.3   1.1   5.7\n2 2005-02-26     0.15   4.5   5     1.9\n3 2005-04-05     0.2    7.2   2.8   5.2\n4 2005-05-12     0.5    4.9   1.3   6.8"
  },
  {
    "objectID": "posts/13-careful-data/index.html#synthetic-example-multiple-messy-date-formats-in-one-column",
    "href": "posts/13-careful-data/index.html#synthetic-example-multiple-messy-date-formats-in-one-column",
    "title": "A ‘careful’ and small intro guide to data science with ‘tidyverse’",
    "section": "\n3.2 Synthetic example: Multiple messy date formats in one column",
    "text": "3.2 Synthetic example: Multiple messy date formats in one column\nBut, what if the 'date' column doesn’t follow 1 pattern?\n\nbox::use(\n    readr[read_csv, cols, col_character],\n    dplyr[mutate], \n    lubridate[parse_date_time, as_date]\n)\n\nsample_csv = \"\ndate, readings, x, y, z\\n\n\\\"Mar.23,2005\\\",0.02,3.3,1.1,5.7\\n\n\\\"Feb 26, 2005\\\",0.15,4.5,5.0,1.9\\n\n\\\"04-5-2005\\\",0.2,7.2,2.8,5.2\\n\n\\\"May12,05\\\", 0.5,4.9,1.3,6.8\\n\n\\\"Aug.17,2005\\\", 0.17,9.5,0.8,4.2\n\"\n\nread_csv(\n    I(sample_csv),\n    col_types = cols(\n        date = col_character()\n    )\n) |&gt; \n    mutate(\n        date = \n            parse_date_time(\n                date,\n                orders = c(\n                    \"b.d,Y\",\n                    \"b d, Y\",\n                    \"m-d-Y\",\n                    \"bd,y\"\n                )\n            ) |&gt; \n            as_date()\n    )\n\n# A tibble: 5 × 5\n  date       readings     x     y     z\n  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2005-03-23     0.02   3.3   1.1   5.7\n2 2005-02-26     0.15   4.5   5     1.9\n3 2005-04-05     0.2    7.2   2.8   5.2\n4 2005-05-12     0.5    4.9   1.3   6.8\n5 2005-08-17     0.17   9.5   0.8   4.2\n\n\nUnfortunately, right now, readr::read_csv() doesn’t know how to parse CSV files with different data format, so I have to take some roundabouts to properly parse the \"date\" column."
  },
  {
    "objectID": "posts/13-careful-data/index.html#inequality-joins-for-binning",
    "href": "posts/13-careful-data/index.html#inequality-joins-for-binning",
    "title": "A ‘careful’ and small intro guide to data science with ‘tidyverse’",
    "section": "\n6.1 Inequality joins for binning",
    "text": "6.1 Inequality joins for binning\nAssign customer segments based on purchase totals:\n\nbox::use(dplyr[left_join, join_by, group_by, slice_max, ungroup])\n\npurchases = tibble::tibble(\n    customer_id = 1:8,\n    total_spent = c(45, 280, 520, 95, 1200, 180, 650, 35)\n)\n\nsegments = tibble::tibble(\n    segment = c(\"Bronze\", \"Silver\", \"Gold\", \"Platinum\"),\n    min_spend = c(0, 100, 500, 1000)\n)\n\npurchases |&gt;\n    left_join(segments, by = join_by(total_spent &gt;= min_spend)) |&gt;\n    group_by(customer_id) |&gt;\n    slice_max(min_spend, n = 1, with_ties = FALSE) |&gt;\n    ungroup()\n\n# A tibble: 8 × 4\n  customer_id total_spent segment  min_spend\n        &lt;int&gt;       &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n1           1          45 Bronze           0\n2           2         280 Silver         100\n3           3         520 Gold           500\n4           4          95 Bronze           0\n5           5        1200 Platinum      1000\n6           6         180 Silver         100\n7           7         650 Gold           500\n8           8          35 Bronze           0\n\n\nThe “greater than or equal to (&gt; + =)” operator finds all matching tiers, then we keep the highest one. Much clearer than manual conditional logic."
  },
  {
    "objectID": "posts/13-careful-data/index.html#range-joins-for-time-based-matching",
    "href": "posts/13-careful-data/index.html#range-joins-for-time-based-matching",
    "title": "A ‘careful’ and small intro guide to data science with ‘tidyverse’",
    "section": "\n6.2 Range joins for time-based matching",
    "text": "6.2 Range joins for time-based matching\nMatch transactions to active promotions:\n\nbox::use(\n    dplyr[tbl = tibble, inner_join, join_by, between, transmute],\n    lubridate[as_date]\n)\n\ntransactions = tbl(\n    tx_id = 1:6,\n    tx_date = as_date(c(\n        \"2024-01-20\", \"2024-02-15\", \"2024-03-25\",\n        \"2024-04-10\", \"2024-05-15\", \"2024-06-05\"\n    )),\n    amount = c(150, 200, 180, 220, 175, 190)\n)\n\npromos = tbl(\n    promo = c(\"New Year\", \"Spring Sale\", \"Summer Blast\"),\n    start = as_date(c(\"2024-01-01\", \"2024-03-01\", \"2024-05-01\")),\n    end = as_date(c(\"2024-01-31\", \"2024-04-30\", \"2024-06-30\")),\n    discount = c(0.15, 0.10, 0.20)\n)\n\ntransactions |&gt;\n    inner_join(\n        promos,\n        by = join_by(between(tx_date, start, end))\n    ) |&gt;\n    transmute(\n        tx_id,\n        tx_date,\n        promo, \n        amount,\n        discount, \n        savings = amount * discount\n    )\n\n# A tibble: 5 × 6\n  tx_id tx_date    promo        amount discount savings\n  &lt;int&gt; &lt;date&gt;     &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1 2024-01-20 New Year        150     0.15    22.5\n2     3 2024-03-25 Spring Sale     180     0.1     18  \n3     4 2024-04-10 Spring Sale     220     0.1     22  \n4     5 2024-05-15 Summer Blast    175     0.2     35  \n5     6 2024-06-05 Summer Blast    190     0.2     38  \n\n\n\n\n\n\n\n\nNoteSQL Code Equivalent\n\n\n\n\n\nHere’s the equivalent SQL code:\nSELECT\n    tx_id,\n    tx_date,\n    promo,\n    amount,\n    discount,\n    amount * discount AS savings\nFROM (\n    SELECT t.*, p.*\n    FROM transactions AS t\n    INNER JOIN promos AS p\n        ON (\n            t.tx_date &gt;= p.start AND\n            t.tx_date &lt;= p.end\n        )\n) AS new_table\n\n\n\nThis beats manually checking date ranges with filter(). The join expresses intent directly."
  },
  {
    "objectID": "posts/13-careful-data/index.html#clean-all-text-columns-at-once",
    "href": "posts/13-careful-data/index.html#clean-all-text-columns-at-once",
    "title": "A ‘careful’ and small intro guide to data science with ‘tidyverse’",
    "section": "\n5.1 Clean all text columns at once",
    "text": "5.1 Clean all text columns at once\n\nbox::use(\n    dplyr[mutate, across, where],\n    stringr[str_trim, str_to_upper]\n)\n\nsurvey = tibble::tibble(\n    resp_id = c(1, 2, 3),\n    name = c(\"  alice  \", \"BOB\", \"  Charlie  \"),\n    city = c(\"New York  \", \"  Boston\", \"Chicago  \"),\n    rating = c(4.2, 3.8, 4.5)\n)\n\nsurvey |&gt;\n    mutate(\n        across(where(is.character), str_trim),\n        across(where(is.character) & !resp_id, str_to_upper)\n    )\n\n# A tibble: 3 × 4\n  resp_id name    city     rating\n    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1       1 ALICE   NEW YORK    4.2\n2       2 BOB     BOSTON      3.8\n3       3 CHARLIE CHICAGO     4.5\n\n\nThe second across() shows combining conditions: all character columns except the ID field."
  },
  {
    "objectID": "posts/13-careful-data/index.html#scale-financial-columns",
    "href": "posts/13-careful-data/index.html#scale-financial-columns",
    "title": "A ‘careful’ and small intro guide to data science with ‘tidyverse’",
    "section": "\n5.2 Scale financial columns",
    "text": "5.2 Scale financial columns\n\nbox::use(dplyr[mutate, across, starts_with])\n\nfinancials = tibble::tibble(\n    company = c(\"TechCo\", \"RetailCorp\"),\n    revenue_q1 = c(1200000, 850000),\n    revenue_q2 = c(1350000, 920000),\n    costs_q1 = c(800000, 600000),\n    costs_q2 = c(850000, 640000)\n)\n\nfinancials |&gt;\n    mutate(\n        across(\n            starts_with(\"revenue\"),\n            \\(x) x / 1e6,\n            .names = \"{.col}_M\"\n        )\n    )\n\n# A tibble: 2 × 7\n  company    revenue_q1 revenue_q2 costs_q1 costs_q2 revenue_q1_M revenue_q2_M\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 TechCo        1200000    1350000   800000   850000         1.2          1.35\n2 RetailCorp     850000     920000   600000   640000         0.85         0.92\n\n\nThe .names argument controls output column names. When you add revenue_q3 later, it automatically gets converted too."
  },
  {
    "objectID": "posts/13-careful-data/index.html#conditional-rounding",
    "href": "posts/13-careful-data/index.html#conditional-rounding",
    "title": "A ‘careful’ and small intro guide to data science with ‘tidyverse’",
    "section": "\n5.3 Conditional rounding",
    "text": "5.3 Conditional rounding\n\nbox::use(dplyr[mutate, across, where, any_of])\n\nmeasurements = tibble::tibble(\n    sample_id = 1:4,\n    temperature = c(98.6234, 99.1456, 97.8921, 98.4567),\n    pressure = c(120.456, 118.234, 122.789, 119.123),\n    ph_level = c(7.4123, 7.3987, 7.4256, 7.4089)\n)\n\nmeasurements |&gt;\n    mutate(\n        across(\n            where(is.numeric) & !any_of(\"sample_id\"),\n            round,\n            digits = 2\n        )\n    )\n\n# A tibble: 4 × 4\n  sample_id temperature pressure ph_level\n      &lt;int&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1         1        98.6     120.     7.41\n2         2        99.2     118.     7.4 \n3         3        97.9     123.     7.43\n4         4        98.5     119.     7.41\n\n\nThis pattern scales: add new numeric columns and they’re automatically rounded. No need to update the rounding code."
  }
]