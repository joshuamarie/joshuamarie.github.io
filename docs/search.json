[
  {
    "objectID": "services.html#consultation-firm",
    "href": "services.html#consultation-firm",
    "title": "Work with Me",
    "section": "Consultation Firm",
    "text": "Consultation Firm\n\n\n\nWhat I Offer\nMathematics & statistics foundation\n\nCalculus\nLinear Algebra\nProbability and statistics theory\n\nStatistical modelling & Machine Learning 1\n\nStatistical inference and simulation\nBayesian inference with Stan/CmdStan\nCustom model development and validation\n\nStatistical modelling & Machine Learning 2\n\nExperimental design and causal inference\nLongitudinal study and time series analysis & forecasting\nApplications with Python and R\n\n\n\nResearch & Methods Consultation\n\nStudy design and methodology review\nStatistical analysis plan development\nLongitudinal studies and panel data\nPower analysis and sample size calculation\nManuscript review (methods & results sections)\n\n\n\n\nWhat I Offer\nData Science\n\nData Visualization ({ggplot2}, {altair})\nShiny app development\nData manipulation and relational algebra with SQL and {tidyverse}\nGeospatial data analysis\n\nPackage Development\n\nR/Python package development\nAPI design and documentation\nTesting frameworks\nOpen-source maintenance strategies\n\n\n\n\n\n\n\nRates, Inquiry & Engagement\n\n\nRetainer packages available for ongoing work\nBook the consultation firm in Calendly\nOr email me at: joshua.marie.k@gmail.com"
  },
  {
    "objectID": "services.html#course-tutorial",
    "href": "services.html#course-tutorial",
    "title": "Work with Me",
    "section": "Course Tutorial",
    "text": "Course Tutorial\nThis is a live session with me, prepared with interactive slides.\n\n\n\n\n\n\nTerms and Condition\n\n\n\nRead terms & and conditions first before booking.\n\n\n\nAvailable Now\n\nCore of R Programming\n\nCourse section 1: Functional programming\nCourse section 2: Performance optimization\nCourse section 3: Metaprogramming\n\n\n\n\nComing Soon\n\n\n\n\n\nBayesian Inference with Stan\n\nCourse section 1: Probabilistic programming fundamentals\nCourse section 2: Stan mechanics\nCourse section 3: Model building and diagnostics\n\n\n\nA/B Testing\n\nCourse section 1: Fundamentals of Experimental Design\nCourse section 2: Statistical Methods\nCourse section 3: Advanced Topics & Real-World Implementation\nPrimary softwares: \n  \nBack-ups:"
  },
  {
    "objectID": "services.html#products",
    "href": "services.html#products",
    "title": "Work with Me",
    "section": "Products",
    "text": "Products\n\nNot available for now"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Check out my blogs",
    "section": "",
    "text": "Get notified when I publish new posts. No spam, unsubscribe anytime"
  },
  {
    "objectID": "posts/index.html#subscribe-to-my-newsletter",
    "href": "posts/index.html#subscribe-to-my-newsletter",
    "title": "Check out my blogs",
    "section": "",
    "text": "Get notified when I publish new posts. No spam, unsubscribe anytime"
  },
  {
    "objectID": "posts/06-load-pkg/index.html",
    "href": "posts/06-load-pkg/index.html",
    "title": "Ways to load / attach packages in R",
    "section": "",
    "text": "Isn‚Äôt it great that R has more than 1 solution to load packages? Some of them are beautiful. Some of them should be illegal in at least three countries. Let‚Äôs rank them from ‚Äúplease never do this‚Äù to ‚Äúfinally, some good food.‚Äù\nIn this post, I will try enumerate the different ways to load packages in R, and discuss their pros and cons. I will also rank them from worst to best solution in practices."
  },
  {
    "objectID": "posts/06-load-pkg/index.html#base-use",
    "href": "posts/06-load-pkg/index.html#base-use",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.1 The new base::use() function (v4.4.0+)",
    "text": "1.1 The new base::use() function (v4.4.0+)\nUpdate: When I discover the bug, thanks to u/guepier and his comment, this changes my mind. Now, I put this in the worst place amongst the solution I listed here.\nI feel like R Core saw the chaos and said ‚Äúfine, we‚Äôll do something‚Äù. This function is available in R version 4.4.0 and above, by the way.\nIt allows you to load packages in a way that minimizes namespace conflicts by only attaching the functions you explicitly use. Take note that base::use() is a short case of library(), a simple wrapper, where it keeps include.only and set:\n\n\nlib.loc to NULL\n\n\ncharacter.only to TRUE\n\n\nlogical.return to TRUE\n\n\nattach.required to FALSE\n\n\n\nuse('pkg', c('obj1', 'fun1'))\n\nThis is still library(), but granular imports are explicit. Except‚Ä¶\n\nAnother problem occurs: Remember, it is just a simple wrapper of library(), therefore the import still goes to the search path.\nIt‚Äôs like putting a fancy new paint job on a 1987 Honda Civic and calling it a Ferrari. It LOOKS different, but under the hood, same old engine, baby.\nFor example:\n\nmean_data = function(.data) {\n    use('dplyr', 'summarise')\n    use('tidyr', 'pivot_longer')\n    \n    summarise(\n        .data, across(\n            where(is.numeric), \n            \\(col) mean(col, na.rm = TRUE)\n        )\n    ) |&gt; \n        pivot_longer(\n            cols = where(is.numeric), \n            names_to = \"Variable\", \n            values_to = \"Ave\"\n        )\n}\n\nmean_data(iris)\n\n# A tibble: 4 √ó 2\n  Variable       Ave\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Sepal.Length  5.84\n2 Sepal.Width   3.06\n3 Petal.Length  3.76\n4 Petal.Width   1.20\n\n\nAfter I execute mean_data(iris), the imports are accessible everywhere. EVERYWHERE!\nAnd base::use() is still broken even in the latest R versions.\nLike, it is so completely broken:\nuse('dplyr', 'mutate')\niris |&gt; mutate(Petal.Area = Petal.Length * Petal.Width)\n#&gt; Error in mutate(iris, Petal.Area = Petal.Length * Petal.Width) : \n#&gt;   could not find function \"mutate\"\n\nThe issue is that subsequent library() calls for an identical package are ignored, and the same is true for base::use(). Bananas. Completely broken.\n\n\n\n\n\n\n\nNote\n\n\n\nThis is noted by R core team:\n\nThis functionality is still experimental: interfaces may change in future versions."
  },
  {
    "objectID": "posts/06-load-pkg/index.html#require",
    "href": "posts/06-load-pkg/index.html#require",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.2 Worst: Using require()\n",
    "text": "1.2 Worst: Using require()\n\n\n~Just‚Ä¶no. I‚Äôll be making a hot take here that sounds controversial, but this solution is the worst thing ever existed in R to attach the packages. ~\nUpdate: This solution may be bad, but at least not worse than base::use().\nThis function returns a Boolean value:\n\nrequire(pkg) |&gt; \n    suppressMessages() |&gt; \n    suppressWarnings() |&gt; \n    print()\n\n[1] FALSE\n\n\nIt returns TRUE if the package is successfully loaded and FALSE otherwise.\nAnd should only be applicable inside functions to check if a package is available.\n\ncheck_package = function() {\n    if (require(pkg, quietly = TRUE)) {\n        print(\"Package loaded successfully\")\n    } else {\n        print(\"Package not available\")\n    }\n}\ncheck_package()\n\n[1] \"Package not available\"\n\n\nUsing require() at the top of a script is how you get mysterious errors 50 lines later. Only acceptable inside functions when you actually check the return value.\nSeriously, this is just library() where you can place it at the top level of your script, but add another extra steps."
  },
  {
    "objectID": "posts/06-load-pkg/index.html#pacman",
    "href": "posts/06-load-pkg/index.html#pacman",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.3 The pro boxer: {pacman}\n",
    "text": "1.3 The pro boxer: {pacman}\n\nThis guy will punch you to death. Just kidding, Manny Pacquiao is a great boxer :). The pacman package tries to streamline package management with functions like p_load().\nDo you know this?\nif (!require(pkg)) {\n    install.packages(\"pkg\")\n    library(pkg)\n}\nWell, they made a shortcut, with pacman::p_load():\npacman::p_load(pkg)\nYou can do the same as above, except you can do this for multiple packages.\nHere‚Äôs how:\npacman::p_load(pkg1, pkg2, pkg3)\nSounds convenient, right?\n\nActually mixes two completely different responsibilities:\n\nInstallation (one-time setup)\nLoading (analysis step)\n\nGreat for interactive playtime. Disastrous in scripts, packages, CI/CD, or any environment without internet. Also:\n\nIt violates the single responsibility principle, harder than a toddler with a drum kit.\n\nThis is like a pineapple pizza"
  },
  {
    "objectID": "posts/06-load-pkg/index.html#library-classic",
    "href": "posts/06-load-pkg/index.html#library-classic",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.4 The classic library()\n",
    "text": "1.4 The classic library()\n\nSuch a classic function, isn‚Äôt it? After all, this is the most used function to attach R package namespace. It is a standard practice that most R users use, and it is safe: It will throw an error if pkg is not installed. This function is traditional and simple:\nlibrary(pkg)\nThat‚Äôs it, right?\nI hope it was that simple, but it has some serious downsides:\n\nIt attaches the entire package namespace to the search path,\nIt can lead to namespace clash, particularly if multiple packages have functions with the same name. This can make debugging difficult and lead to unexpected behaviors in your code.\nIt makes the imports unclear which functions come from which packages\nAll exported functions are available, even if you only need one or two\n\nTo detach the attached package namespace in the search path, use detach() function with package : keyword:\ndetach(package : pkg)\n\n\n\n\n\n\nWarning\n\n\n\nBe minded that library() function still potentially silently fails, even though it will throw an error, unlike require() where silent fails are always prominent."
  },
  {
    "objectID": "posts/06-load-pkg/index.html#combo-pack",
    "href": "posts/06-load-pkg/index.html#combo-pack",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.5 library() and {conflicted} package combo",
    "text": "1.5 library() and {conflicted} package combo\nHow about forcing the search path to select / deselect the imports? Introducing conflicted package.\n\nIn this approach, I combine traditional library() with the conflicted package to explicitly handle namespace conflicts.\nHow good? For example, I prefer using dplyr::filter() over stats::filter(), but a bit later on, I want to use stats::filter() when I want to run time series. The conflicted::conflict_prefer() handles which you want to declare ‚Äúwinners‚Äù of conflicts.\nI‚Äôll make a scenario to make you understand:\n\n\nI have no use with stats::filter() because I only want to keep the data frame based on the condition using dplyr::filter(), and I want to load the entire dplyr namespace. Here, I declare dplyr::filter() as ‚Äúwinner‚Äù of the conflict:\n\nlibrary(dplyr)\n\nconflicted::conflict_prefer('filter', 'dplyr', 'stats')\nfilter(mtcars, cyl == 8)\n\n\n\nThen, I stopped using dplyr::filter() because I want to perform time series modelling with linear filtering using stats::filter(). Re-state stats::filter() as the ‚Äúwinner‚Äù of the conflict:\n\nconflicted::conflict_prefer('filter', 'stats', 'dplyr')\nfilter(1:10, rep(1, 3))\n\n\n\nStill loads everything. Still global. Still manual work. In my standard, this is actually good, but still not enough because it never allows granular imports and import aliasing, and besides, I‚Äôve had better."
  },
  {
    "objectID": "posts/06-load-pkg/index.html#double-colon",
    "href": "posts/06-load-pkg/index.html#double-colon",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.6 Tedious but Explicit: The :: Operator",
    "text": "1.6 Tedious but Explicit: The :: Operator\n\nBefore packages like box and import introduced alternative import systems to R, the :: operator was (and still is) R‚Äôs built-in way to explicitly reference functions from specific namespaces without loading entire packages.\nThe :: operator is the most explicit base R solution for calling package functions. It‚Äôs part of R‚Äôs namespace system and requires no external dependencies - just base R.\nHere‚Äôs how:\n\nThe syntax is package::function(), which tells R exactly which package to pull the function from without attaching that package to your search path.\n\nMost of us using R are definitely using this (I am reusing an example from base::use):\n\nmean_data = function(.data) {\n    dplyr::summarise(\n        .data, across(\n            where(is.numeric), \n            \\(col) mean(col, na.rm = TRUE)\n        )\n    ) |&gt; \n        tidyr::pivot_longer(\n            cols = where(is.numeric), \n            names_to = \"Variable\", \n            values_to = \"Ave\"\n        )\n}\n\nmean_data(iris)\n\n# A tibble: 4 √ó 2\n  Variable       Ave\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Sepal.Length  5.84\n2 Sepal.Width   3.06\n3 Petal.Length  3.76\n4 Petal.Width   1.20\n\n\n\n\n\n\n\n\nNoteNotice\n\n\n\n\nNoticed that I don‚Äôt call dplyr:: for across() and where()? I have a blog talking about this.\n\n\n\nThis is great, compared to the previous solutions, no external packages needed and works mostly in any R version. The problem is this is way too verbose and repetitive, especially with many function calls:\nggplot2::ggplot(data, ggplot2::aes(date, y)) +\n    ggplot2::geom_point() + \n    ggplot2::geom_line() + \n    ggplot2::theme_minimal() + \n    ggplot2::labs(\n        x = \"Date (by month)\",\n        y = \"Value (in dollars)\", \n        title = \"Monthly Value in Dollar\"\n    )\nBeing typing-intensive is why I called this solution ‚Äútedious‚Äù.\n\nRespectable, but I bet nobody wants to type that many in 2025."
  },
  {
    "objectID": "posts/06-load-pkg/index.html#import-pack",
    "href": "posts/06-load-pkg/index.html#import-pack",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.7 Second to best: {import} package",
    "text": "1.7 Second to best: {import} package\nIt is so close!\n\nThis package is made before box. So, before box, the import package is the best solution ever made, arrived to fix library()‚Äôs most egregious issues. Created by Stefan Milton Bache (of pipe fame), it brings selective imports to R without requiring a complete paradigm shift.\n\n\nFirst Example\nSymbol binding\n\n\n\nThe first example is simple: Normal imports with aliases.\n\nimport::from(\n    dplyr, \n    select, rename, keep_when = filter, mutate, summarise, n\n)\nimport::from(tidyr, long = pivot_longer, wide = pivot_wider, drop_na)\nimport::from(ggplot2, diamonds, cut_width)\n\ndiamonds |&gt; \n    keep_when(\n        cut %in% c(\"Ideal\", \"Premium\"), \n        carat &gt; 1\n    ) |&gt; \n    drop_na() |&gt; \n    mutate(\n        price_per_carat = price / carat,\n        size_category = cut_width(carat, 0.5)\n    ) |&gt; \n    select(carat, cut, color, price, price_per_carat, size_category) |&gt; \n    wide(\n        names_from = cut,\n        values_from = price_per_carat,\n        values_fn = median\n    ) |&gt; \n    summarise(\n        across(c(Ideal, Premium), \\(col) mean(col, na.rm = TRUE)),\n        n = n()\n    )\n\n# A tibble: 1 √ó 3\n  Ideal Premium     n\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1 6494.   5978.  9951\n\n\n\n\nUse backticks around %&gt;% since it is under non-syntactic names.\n\nimport::from(dplyr, select, filter, mutate, summarise, n, relocate)\nimport::from(magrittr, `%&gt;%`) \nimport::from(tidyr, long = pivot_longer, wide = pivot_wider, drop_na)\n\nmtcars %&gt;% \n    filter(cyl == 6) %&gt;% \n    mutate(\n        hp_per_cyl = hp / cyl,\n        efficiency = mpg / disp\n    ) %&gt;% \n    select(mpg, disp, hp, hp_per_cyl, efficiency, everything()) %&gt;% \n    summarise(\n        across(\n            c(mpg, hp_per_cyl, efficiency), \n            list(\n                mu = \\(x) mean(x, na.rm = TRUE), \n                sigma = \\(x) sd(x, na.rm = TRUE)\n            ), \n            .names = \"{.col}..{.fn}\"\n        ),\n        n = n()\n    ) %&gt;% \n    long(\n        cols = contains(c(\"mu\", \"sigma\")), \n        names_sep = \"\\\\..\", \n        names_to = c(\"Variable\", \"Stat\"), \n        values_to = \"Est\"\n    ) %&gt;% \n    wide(\n        names_from = Stat, \n        values_from = Est\n    ) %&gt;% \n    relocate(n, .after = last_col()) %&gt;%\n    mutate(\n        se = sigma / sqrt(n), \n        cv = sigma / mu\n    )\n\n# A tibble: 3 √ó 6\n  Variable       mu  sigma     n      se     cv\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 mpg        19.7   1.45       7 0.549   0.0736\n2 hp_per_cyl 20.4   4.04       7 1.53    0.198 \n3 efficiency  0.112 0.0231     7 0.00872 0.206 \n\n\n\n\n\nIt‚Äôs so awesome. Why?\n\nNo masking\nExplicit at the top\nWorks with roxygen2 (@importFrom)\nImports the pipe like any other function\n\nThere‚Äôs still some limitations. Even though import provide necessities that solves my problem in R‚Äôs import system -\n\n\nIt has no unifying solution to attach the imports in the current environment. In fact, import functions still attach imported functions to the parent environment (usually global). What I mean is that they‚Äôre not truly scoped to a module or function. Thus, the use of import::here():\n\nwith(iris, {\n    import::here(stats, corr = cor)\n\n    corr(Sepal.Length, Petal.Length)\n})\n\n[1] 0.8717538\n\n\nThe expression we made, with(iris, { ... }) creates a temporary environment that disappears immediately. So corr() is placed exactly there, inside that temporary environment, and you cannot reuse corr() somewhere in the environment, even in the global environment\ncorr(1:10, 2:11)\n#&gt; Error in corr(1:10, 2:11) : could not find function \"corr\"\nThis is better than loading entire packages, but not as clean as lexical scoping.\n\nThe package was designed primarily for CRAN packages. File-based modules feel like an afterthought rather than a first-class feature.\nIt lacks support for nested module hierarchies. You can import from files with this package, but you can‚Äôt organize modules into sophisticated directory structures with their own internal dependencies.\nUnlike box, there‚Äôs no way to import a whole package as an object without attaching names"
  },
  {
    "objectID": "posts/06-load-pkg/index.html#box",
    "href": "posts/06-load-pkg/index.html#box",
    "title": "Ways to load / attach packages in R",
    "section": "\n1.8 The ergonomically superior {box} package",
    "text": "1.8 The ergonomically superior {box} package\n\nFinally, some good food. \n\n\n\n\n\n\nNoteMy impression\n\n\n\nIn 2021, Konrad Rudolph looked at R‚Äôs prehistoric import system, said:\n\n‚ÄúThis is rubbish‚Äù\n\nDisclaimer: I don‚Äôt know if he said it, I said this just for fun ;)\nI strongly agree. And then dropped one of the magnum opus: box ‚Äî he dropped it like Gordon Ramsay dropping a perfectly seared Wellington on the pass.\n\n\nThis isn‚Äôt ‚Äúslightly nicer imports‚Äù ‚Äî it‚Äôs a complete rethinking of how R package should be loaded, and R code should be organized and namespaced. It brings true module systems (like Python, JavaScript, or Ruby) to R.\nThere‚Äôs 4 of a kind to import like a sane person:\nbox::use(\n    purrr,                          # 1\n    tbl = tibble,                   # 2\n    dplyr = dplyr[filter, select],  # 3\n    stats[st_filter = filter, ...]  # 4\n)\n\nSource: https://github.com/klmr/box\n\n\n\nAttached the names? Nah, even better:\n\nImports the entire purrr package as an object.\n\nNothing goes into the search path.\n\nYou use it as purrr$map(), purrr$keep(), etc.\n\nZero risk of masking, zero pollution. Pure bliss.\n\n\n\nWhole package? But make it short\n\nSame vibe as 1, but you give the package a cute little nickname.\nNow you write tbl$tbl_df(), tbl$as_tibble(), etc.\n\nPerfect when you hate typing tibble:: but also hate global mess.\n\n\n\nI want the whole namespace‚Ä¶ but only some names in my face.\n\nA killer move, actually: import the whole package as an object, and selectively attach only the functions you actually want to write naked.\nSo your pipelines stay clean: filter(), select(), mutate() ‚Äî all smooth, drama-free.\n\nBut when you need the weird stuff, you still have the entire namespace sitting there like:\ndplyr$reconstruct_tibble_from_who_knows_what()\n\n\n\n\n‚ÄúI refuse to be gaslit by stats::filter() ever again.‚Äù\n\n‚ÄúI want everything from {stats} (because base R is already everywhere), but stats::filter() is a war criminal that keeps fighting with dplyr::filter().‚Äù\nSo basically, everything from {stats} is attached, but rename that one cursed function to st_filter() so it never bites me again.\nThe ... means ‚Äúeverything else, with their original names‚Äù.\n\n\n\nBut wait, there‚Äôs more!\nHere, watch the madness of how I apply box to load package deps:\nbox::use(\n    dplyr[\n        select, rename, \n        keep_when = filter,   # rename because we want to avoid needless fighting\n        mutate, summarise, \n        across, everything\n    ],\n    tidyr[pivot_longer, pivot_wider, drop_na],\n    magrittr[`%&gt;%`],          # yes, don't forget that the pipe is just another import\n    ggplot2[ggplot, aes, geom_point, theme_minimal, labs, ggsave],\n    lubridate[ymd, year, month, floor_date],\n    data.table[fread]         # because sometimes you need speed, not dignity\n)\nLess :: spam. No package::function() that makes your code look like it‚Äôs been hit by shrapnel. Zero library() / require().\nAnd then, the part that makes grown R programmers cry tears of joy ‚Äî You are also allowed to reuse exported namespace from an R script or a folder as a module.\nbox::use(\n    app/models/glm_fit[...],           # brings everything exported\n    app/plots/theme_pub[theme_pub],    # only the theme\n    app/utils/cleaning[clean_names, fix_dates],\n    ./secret_sauce                     # local folder / script = module\n)\nWith box, you can create modules that encapsulate your code and its dependencies ‚Äî another revolutionary and W move in R community. This package is making my life easier in managing and reuse code across different projects.\nThis approach aligns well with modern programming practices and helps to keep your codebase clean and maintainable.\n\n1.8.1 Little resources\nOther resources to learn more about this package:\n\nCRAN Index\nBox README\nMy book"
  },
  {
    "objectID": "posts/04-tidyselect-helpers/index.html",
    "href": "posts/04-tidyselect-helpers/index.html",
    "title": "The Hidden Magic of Tidy-Select: R‚Äôs Universal Column Selection Language",
    "section": "",
    "text": "1 Introduction\nHave you ever wondered how where(), starts_with(), and other selection helpers work seamlessly across different tidyverse packages? I recently discovered something surprising: you can actually use these functions in dplyr, tidyr, and other packages that invokes &lt;tidy-select&gt; API, without explicitly loading them.\nHere‚Äôs how it works:\n\niris |&gt; \n    tidyr::pivot_longer(\n        cols = where(is.numeric), # using `where()` w/out calling dplyr / tidyselect\n        names_to = 'Variable',\n        values_to = 'Measure'\n    )\n\n\n  \n\n\n\nTake note that I never load tidyselect and dplyr (the where() function in dplyr is just one of many re-exports). Yet, where() works perfectly. It doesn‚Äôt belong to / re-exported by tidyr, but you can use where(), if and only if the functions is invoking &lt;tidy-select&gt; API.\n\n2 What Are These Functions Called?\nThese are officially called tidyselect helpers (or ‚Äúselection language‚Äù). They‚Äôre part of the tidyselect package, which provides a domain-specific language (DSL) for selecting columns in data frames.\nYou might also hear them referred to as:\n\nSelection helper functions\n\n&lt;Tidy-select&gt; helpers\nColumn selection helpers\n\n3 The Complete Family of Selection Helpers\nThe tidyselect package can be divided into 3 categories of helpers.\n\n\nPattern Matching Helpers\nPredicate-Based Helpers\n‚ÄúPositional‚Äù Helpers\n\n\n\n\n\nColumns starting with a prefix\n\niris |&gt; \n    dplyr::select(starts_with(\"Sepal\")) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\nColumns ending with a suffix\n\niris |&gt; \n    dplyr::select(ends_with(\"Width\")) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\nColumns containing a literal string\n\niris |&gt; \n    dplyr::select(contains(\"al\")) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\nColumns matching a regular expression\n\niris |&gt; \n    dplyr::select(matches(\"^Sepal\")) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\nColumns following the number pattern\n\niris |&gt; \n    dplyr::select(num_range('x', 1:4)) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\n\n\nThe where() function is similar to SQL WHERE, except it is functional that (should) returns a Boolean value that satisfies the condition.\n\niris |&gt; \n    dplyr::select(where(is.numeric)) |&gt; \n    head(3)\n\n\n  \n\n\niris |&gt; \n    dplyr::select(where(is.factor)) |&gt; \n    head(3)\n\n\n  \n\n\niris |&gt; \n    dplyr::select(where(\\(col) is.numeric(col) && mean(col) &gt; 3.5))\n\n\n  \n\n\n\n\n\nThese are functions that select columns based on their position in the data frame\n\n\neverything()\n\niris |&gt; \n    dplyr::select(everything()) |&gt; \n    head(3)\n\n\n  \n\n\n\nThis is equivalent to relocate(iris, Species):\n\niris |&gt; \n    dplyr::select(Species, everything()) |&gt; \n    head(3)\n\n\n  \n\n\n\n\nlast_col()\n\n\niris |&gt; \n    dplyr::select(last_col()) |&gt; \n    head(3)\n\n\n  \n\n\n\nOffset: 2nd to the last\n\niris |&gt; \n    dplyr::select(last_col(1)) |&gt; \n    head(3)\n\n\n  \n\n\n\nOffset: Multiple columns from the end\n\niris |&gt; \n    dplyr::select(last_col(2):last_col()) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\n\nNoticed that I invoked most of &lt;tidy-select&gt; helpers, but never loaded dplyr or tidyselect, not once, just to use them.\n\n4 ‚ÄúData-Masking‚Äù Subset\nJust like those &lt;tidy-select&gt; helpers, some functions found in dplyr, but doesn‚Äôt in tidyselect. These are all the functions that can be used within ‚Äúdata-masking‚Äù functions, such as dplyr::mutate() and dplyr::summarise(). Take a note of the term ‚Äúwithin‚Äù, which means, you can‚Äôt use them outside from ‚Äúdata-masking‚Äù functions.\nI call across(), if_any(), and if_all() as projection helpers because they correspond to the SELECT clause in SQL, except they both if_any(), and if_all() map over the selected columns and returns the Boolean vector, while the across() function modifies the selected columns. The pick() function, on the other hand serves as a complement of across() by extracting them as a data frame, however, this only applies to subset a data frame to be invoked within the operations in ‚Äúdata-masking‚Äù functions. All of them can make use of the &lt;tidy-select&gt; API, meaning you can apply selectors like starts_with() or everything() to specify which columns to project.\n\n\nUsing pick()\nUsing across()\nUsing if_all() / if_any()\nFunctions you actually need to attach\n\n\n\nHere‚Äôs an example: Calculating mean and standard deviation\n\niris |&gt; \n    dplyr::group_by(Species) |&gt; \n    dplyr::summarise(\n        summary = list({\n            num = pick(where(is.numeric))\n            tibble::tibble(\n                vars = colnames(num), \n                mean = colMeans(num),\n                sd = apply(num, 2, sd)\n            )\n        })\n    ) |&gt; \n    tidyr::unnest(summary)\n\n\n  \n\n\n\n\nI am aware there‚Äôs a better approach to calculate the mean and standard deviation of each column by group.\n\n\n\nHere‚Äôs an example: Apply min-max normalization among numeric columns in iris dataset\n\niris |&gt; \n    dplyr::as_tibble() |&gt; \n    dplyr::mutate(\n        across(\n            where(is.numeric), \n            \\(col) { col - min(col) } / { max(col) - min(col) }\n        )\n    )\n\n\n  \n\n\n\nAnd once again, I never attach dplyr into the search path just to use across() and pick().\nYou can use across() in some dplyr ‚Äúdata-masking‚Äù function like filter(), but this is a deprecated behavior and attaching dplyr package is required.\n\n\nExample: Removing all missing values across all columns in airquality data frame\n\nairquality |&gt; \n    dplyr::filter(if_all(everything(), \\(col) !is.na(col))) |&gt; \n    head(5)\n\n\n  \n\n\n\nIf if_all() / if_any() is used outside filter(), those functions need dplyr package to be attached to use them.\n\n\nThough, there are some exceptions: there are helper functions you actually need dplyr to be attached to use them, otherwise they don‚Äôt work and R will throw an error.\nHere they are:\n\nlibrary(dplyr)\n\n\n\nn()\n\niris |&gt;\n    group_by(Species) |&gt; \n    slice_max(n = 20, order_by = Sepal.Length) |&gt; \n    summarise(\n        count = n(), # üëà \n        m_sl = mean(Sepal.Length)\n    )\n\n\n  \n\n\n\n\n\ncur_group()\n\nmtcars |&gt;\n    group_by(cyl) |&gt;\n    reframe({\n        model = lm(mpg ~ wt, data = cur_group()) # üëà \n        coefs = coef(model)\n\n        tibble(\n            terms = names(coefs), \n            estimate = coefs\n        )\n    })\n\n\n  \n\n\n\n\n\ncur_group_id()\n\nstarwars |&gt;\n    group_by(species) |&gt;\n    reframe(\n        species, \n        name, \n        hierarchical_id = sprintf(\"%02d-%03d\", cur_group_id(), row_number()) # üëà \n    ) |&gt; \n    slice_min(hierarchical_id, n = 15)\n\n\n  \n\n\n\n\n\ncur_group_rows()\n\niris |&gt; \n    group_by(Species) |&gt; \n    slice_sample(\n        n = 75, replace = TRUE\n    ) |&gt; \n    summarise(\n        m_sl = mean(Sepal.Length),\n        n = {length(cur_group_rows()) + 30} # üëà \n    )\n\n\n  \n\n\n\n\n\ncur_column()\n\niris |&gt; \n    as_tibble() |&gt; \n    transmute(\n        across(\n            where(is.numeric),\n            \\(col) {\n                if (stringr::str_detect(cur_column(), \"Sepal\")) { # üëà \n                    col - mean(col)\n                } else if (stringr::str_detect(cur_column(), \"Petal\")) { # üëà \n                    (col - mean(col)) / sd(col)\n                } else {\n                    col\n                }\n            }\n        )\n    )\n\n\n  \n\n\n\n\n\n\n\n\n\n5 Conclusion\nI hope they don‚Äôt change this soon, it is quite a nice feature (definitely not a bug üòã), assembling the DSL strengths across tidyverse APIs. Even if it is subtle. I still suggest you to attach these functions (through e.g.¬†library() and box::use()) for better maintainability."
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#introduction",
    "href": "posts/02-arima-grid-search/index.html#introduction",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R",
    "section": "\n1.1 Introduction",
    "text": "1.1 Introduction\nThe ARIMA (AutoRegressive Integrated Moving Average) model is defined by three parameters:\n\n\np: Autoregressive order that counts the past lagged terms (This is AR in ARIMA context)\n\nd: Differencing order that counts the number of differencing to achieve stationarity (This is ‚ÄòI‚Äô or ‚Äúintegrate‚Äù in ARIMA context)\n\nq: Moving average order that counts the past error lagged terms (MA)\n\nChoosing the right combination of (p, d, q) is‚Ä¶not that easy, right when you want to achieve the best fit, even with Hyndman and Khandaka (2008) methodology with their forecast::auto.arima().\nThis is how it‚Äôs done:\n\nPrepare a time series data. I generate a time series data from this in order for you to replicate this.\n\nThen fit every possible ARIMA models across a grid of (p, d, q) values.\n\nThen evaluate the models performance by calculating the maximum log-likelihood then weight them with AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).\n\nThen visualize the fitted values from every possible models, alongside the actual data.\n\nFrom the visual, highlight the best model in red.\n\nOptionally, you can make it interactive, using ‚Äòggiraph‚Äô, and I prepare it so that you can hover and explore model fits.\n\nThe packages used:\n\nbox (v1.2.0)\nggplot2 (v4.0.0)\nggiraph (v0.9.1)\npurrr (v1.0.2)\ndplyr (v1.1.4)\nforecast (v8.23.0)\nglue (v1.7.0)\ntidyr (v1.3.1)\nrlang (v1.1.4)\nscales (v1.4.0)"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#simulating-data",
    "href": "posts/02-arima-grid-search/index.html#simulating-data",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R",
    "section": "\n1.2 Simulating Data",
    "text": "1.2 Simulating Data\nWe generate a synthetic dataset with some trend and randomness:\n\nset.seed(123)\nts_sim = runif(365, 5, 10) + seq(-140, 224)^2 / 10000\nday = as.Date(\"2025-06-14\") - 0:364\n\nThis produces 365 daily observations with both trend and noise, which is a good test case for ARIMA."
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#fitting-multiple-arima-models",
    "href": "posts/02-arima-grid-search/index.html#fitting-multiple-arima-models",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R",
    "section": "\n1.3 Fitting Multiple ARIMA Models",
    "text": "1.3 Fitting Multiple ARIMA Models\nWe test a grid of ARIMA parameters:\n\\[\n\\begin{aligned}\np &\\in \\{0,1,2\\} \\\\\nd &\\in \\{0,1\\} \\\\\nq &\\in \\{0,1,2\\}\n\\end{aligned}\n\\]\nWe exclude overly complex models where p + q &gt; 3.\n\nCodemodels = local({\n    box::use(\n        purrr[pmap, pmap_chr, possibly, map, map_dbl], \n        dplyr[transmute, mutate, filter, slice_min, slice, case_when], \n        forecast[Arima], \n        glue[glue], \n        tidyr[expand_grid], \n        rlang[exec]\n    )\n    \n    expand_grid(p = 0:2, d = 0:1, q = 0:2) |&gt; \n        transmute(\n            models = pmap_chr(\n                pick(1:3), \n                \\(p, d, q) glue(\"ARIMA({p},{d},{q})\")\n            ), \n            res = pmap(\n                pick(1:3),\n                possibly(\n                    function (p, d, q) {\n                        if (p + q &gt; 3) return(NULL)\n                        exec(Arima, as.ts(ts_sim), order = c(p, d, q))\n                    },\n                    otherwise = NULL\n                )\n            ), \n            fits = map(res, ~ if(is.null(.x)) NULL else fitted(.x)),\n            aic = map_dbl(res, ~ if(is.null(.x)) NA_real_ else AIC(.x)),\n            bic = map_dbl(res, ~ if(is.null(.x)) NA_real_ else BIC(.x))\n        ) |&gt; \n        filter(!is.na(aic)) |&gt;  # Remove failed models\n        mutate(\n            day = list(day),\n            is_lowest_aic = aic == min(aic, na.rm = TRUE),\n            is_lowest_bic = bic == min(bic, na.rm = TRUE)\n        )\n})\nmodels\n\n\n  \n\n\n\nThis gives us a nested data frame of fitted models with their AIC, BIC, and fitted values."
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#visualizing-models-with-ggplot2",
    "href": "posts/02-arima-grid-search/index.html#visualizing-models-with-ggplot2",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R",
    "section": "\n1.4 Visualizing Models with ggplot2",
    "text": "1.4 Visualizing Models with ggplot2\nDo you want to visualize everything better, including the actual, fitted values, and highlight the fitted values made by the best fit?\nFrom models data, we just need the is_lowest_aic and is_lowest_bic. We just need to tweak the data a little bit here by expanding the fitted values with its corresponding data value. Then, set the model_type to condition the plotting data with dplyr::case_when().\n\nplot_data = local({\n    box::use(\n        tidyr[unnest], \n        dplyr[mutate, case_when]\n    )\n    models |&gt; \n        unnest(cols = c(day, fits)) |&gt;\n        mutate(\n            model_type = case_when(\n                is_lowest_aic ~ \"Best AIC\",\n                is_lowest_bic ~ \"Best BIC\", \n                TRUE ~ \"Other Models\"\n            )\n        )\n})\n\nOptionally:\n\n\nYou can put the information about the model which had the lowest AIC and BIC in an annotated box text in the plot.\n\nbest_model_1 = models |&gt; dplyr::filter(is_lowest_aic)\nbest_model_2 = models |&gt; dplyr::filter(is_lowest_bic)\n\n\n\nPointing out the maximum value of the time series data\n\nmax_val = max(ts_sim)\nmax_idx = which.max(ts_sim)\nmax_day = day[max_idx]\n\n\n\nThen, visualize:\n\nCodelocal({\n    box::use(\n        ggplot2[...],\n        scales[comma],\n        dplyr[filter], \n        glue[glue], \n    )\n    \n    p = ggplot() + \n        # Original data\n        geom_line(\n            aes(x = day, y = as.numeric(ts_sim)), \n            linewidth = 1.2, \n            color = \"#2C3E50\",\n            alpha = 0.8\n        ) + \n        geom_point(\n            aes(x = day, y = as.numeric(ts_sim)), \n            size = 0.8, \n            color = \"#2C3E50\", \n            alpha = 0.6\n        ) + \n        \n        # Other fitted models (light gray)\n        geom_line(\n            data = filter(plot_data, model_type == \"Other Models\"), \n            aes(x = day, y = fits, group = models), \n            color = '#BDC3C7',\n            alpha = 0.6,\n            linewidth = 0.5\n        ) + \n        \n        # Best BIC model (blue)\n        geom_line(\n            data = filter(plot_data, model_type == \"Best BIC\"), \n            aes(x = day, y = fits, group = models), \n            color = '#3498DB',\n            linewidth = 1.2,\n            alpha = 0.9\n        ) + \n        \n        # Best AIC model (red, on top)\n        geom_line(\n            data = filter(plot_data, model_type == \"Best AIC\"), \n            aes(x = day, y = fits, group = models), \n            color = '#E74C3C',\n            linewidth = 1.5,\n            alpha = 0.9\n        ) + \n        \n        # Peak annotation\n        annotate(\n            \"point\",\n            x = max_day, y = max_val,\n            size = 4, colour = \"#E74C3C\", shape = 21, stroke = 2, fill = \"white\"\n        ) + \n        annotate(\n            \"text\", \n            x = max_day + 25,\n            y = max_val + 0.5,\n            label = paste0(\"Peak: \", round(max_val, 1), \" sec\"), \n            color = \"#E74C3C\",\n            fontface = \"bold\",\n            size = 3.5\n        ) +\n        annotate(\n            \"curve\", \n            x = max_day + 20, xend = max_day + 0.1, \n            y = max_val + 0.3, yend = max_val, \n            linewidth = 0.8,\n            color = \"#E74C3C\", \n            curvature = -0.2,\n            arrow = arrow(length = unit(0.15, \"cm\"), type = \"closed\")\n        ) +\n        \n        # Model performance text box\n        annotate(\n            \"rect\",\n            xmin = min(day) + 20, xmax = min(day) + 100,\n            ymin = max(ts_sim) - 1.5, ymax = max(ts_sim) - 0.2,\n            fill = \"white\", color = \"#34495E\", alpha = 0.9\n        ) +\n        annotate(\n            \"text\",\n            x = min(day) + 60, y = max(ts_sim) - 0.5,\n            label = paste0(\"Best AIC: \", best_model_1$models, \n                           \"\\nAIC = \", round(best_model_1$aic, 1)),\n            color = \"#E74C3C\", fontface = \"bold\", size = 3.2\n        ) +\n        annotate(\n            \"text\",\n            x = min(day) + 60, y = max(ts_sim) - 1.1,\n            label = paste0(\"Best BIC: \", best_model_2$models,\n                           \"\\nBIC = \", round(best_model_2$bic, 1)),\n            color = \"#3498DB\", fontface = \"bold\", size = 3.2\n        ) +\n        \n        # Styling\n        scale_x_date(date_labels = \"%b %Y\", date_breaks = \"2 months\") +\n        scale_y_continuous(labels = comma) +\n        \n        labs(\n            x = \"Time Index\", \n            y = \"Simulated Response Values\", \n            title = \"ARIMA Model Grid Search: Simulated Time Series Analysis\",\n            subtitle = glue(\"Comparing {nrow(models)} successful ARIMA models ‚Ä¢ Best performing models highlighted\"),\n            caption = paste0(\"Models tested: p‚àà[0,2], d‚àà[0,1], q‚àà[0,2] ‚Ä¢ \",\n                             \"Original data shown in dark gray ‚Ä¢ \",\n                             \"Total observations: \", length(ts_sim))\n        ) + \n        \n        theme_minimal(base_size = 11, base_family = \"serif\") +\n        theme(\n            plot.title = element_text(\n                family = \"serif\", \n                colour = \"#2C3E50\",\n                size = 14,\n                face = \"bold\",\n                margin = margin(b = 5)\n            ),\n            plot.subtitle = element_text(\n                family = \"serif\",\n                colour = \"#7F8C8D\",\n                size = 10,\n                margin = margin(b = 15)\n            ),\n            plot.caption = element_text(\n                family = \"serif\",\n                colour = \"#95A5A6\",\n                size = 8,\n                margin = margin(t = 10)\n            ),\n            axis.text.x = element_text(\n                angle = 45, \n                hjust = 1,\n                margin = margin(t = 8),\n                color = \"#34495E\"\n            ), \n            axis.text.y = element_text(\n                margin = margin(r = 8),\n                color = \"#34495E\"\n            ), \n            axis.title = element_text(\n                color = \"#2C3E50\",\n                face = \"bold\"\n            ),\n            panel.grid.minor = element_blank(),\n            panel.grid.major = element_line(\n                color = \"#ECF0F1\", \n                linewidth = 0.3\n            ), \n            panel.background = element_rect(fill = \"#FEFEFE\", color = NA),\n            plot.background = element_rect(fill = \"white\", color = NA),\n            plot.margin = margin(20, 20, 20, 20)\n        ) \n    \n    p\n})\n\n\n\n\n\n\n\nWe visualize:\n\nOriginal data (dark gray)\nAll fitted values from every possible ARIMA model (light gray), except, the fitted values from the best fit is highlighted (red, blue, based on AIC, BIC, respectively)\nAnnotated peak point in the data\nAnnotated best AIC/BIC values"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#optional-interactive-visualization-with-ggiraph",
    "href": "posts/02-arima-grid-search/index.html#optional-interactive-visualization-with-ggiraph",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R",
    "section": "\n1.5 Optional: Interactive Visualization with ggiraph",
    "text": "1.5 Optional: Interactive Visualization with ggiraph\nIf you prefer your plot to be interactive like some figures in the website, use ‚Äòggiraph‚Äô interactive interface version of ggplot2, then girafe(). The output produced by girafe() is wrapped with HTML, so it can be run in web.\nI recommend ‚Äòggiraph‚Äô to build web applications in R.\nThis is the interactive version of the plot above:\n\nCodelocal({\n    box::use(\n        ggplot2[...],\n        scales[comma],\n        dplyr[filter, mutate, case_when], \n        glue[glue], \n        ggiraph[geom_line_interactive, geom_point_interactive, girafe, opts_hover, opts_hover_inv, opts_selection], \n        tidyr[unnest]\n    )\n    \n    # Use this for preparation\n    original_data = data.frame(\n        day = day,\n        ts_sim = ts_sim,\n        tooltip_line = \"actual data\",\n        tooltip_point = paste0(format(day, \"%Y-%m-%d\"), \"; readings: \", round(ts_sim, 1), \" secs\")\n    )\n    \n    plot_data = models |&gt; \n        unnest(cols = c(day, fits)) |&gt;\n        mutate(\n            model_type = case_when(\n                is_lowest_aic ~ \"Best AIC\",\n                is_lowest_bic ~ \"Best BIC\", \n                TRUE ~ \"Other Models\"\n            ),\n            # Create tooltip text for model lines\n            tooltip_text = case_when(\n                is_lowest_aic ~ paste0(\"best model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2)),\n                is_lowest_bic ~ paste0(\"best model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2)),\n                TRUE ~ paste0(\"model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2))\n            )\n        )\n  \n    p = ggplot() + \n        geom_line_interactive(\n            data = original_data,\n            aes(x = day, y = ts_sim, tooltip = tooltip_line, data_id = \"original_line\"), \n            linewidth = 1.2, \n            color = \"#2C3E50\",\n            alpha = 0.8\n        ) + \n        geom_point_interactive(\n            data = original_data,\n            aes(x = day, y = ts_sim, tooltip = tooltip_point), \n            size = 0.8, \n            color = \"#2C3E50\", \n            alpha = 0.6\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Other Models\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#BDC3C7',\n            alpha = 0.6,\n            linewidth = 0.5\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Best BIC\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#3498DB',\n            linewidth = 1.2,\n            alpha = 0.9\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Best AIC\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#E74C3C',\n            linewidth = 1.5,\n            alpha = 0.9\n        ) + \n        \n        scale_x_date(date_labels = \"%b %Y\", date_breaks = \"2 months\") +\n        scale_y_continuous(labels = comma) +\n        \n        labs(\n            x = \"Time Index\", \n            y = \"Simulated Response Values\", \n            title = \"ARIMA Model Grid Search: Simulated Time Series Analysis\",\n            subtitle = glue(\"Comparing {nrow(models)} successful ARIMA models ‚Ä¢ Best performing models highlighted\"),\n            caption = paste0(\"Models tested: p‚àà[0,2], d‚àà[0,1], q‚àà[0,2] ‚Ä¢ \",\n                             \"Original data shown in dark gray ‚Ä¢ \",\n                             \"Total observations: \", length(ts_sim))\n        ) + \n        \n        theme_minimal(base_size = 11, base_family = \"serif\") +\n        theme(\n            plot.title = element_text(\n                family = \"serif\", \n                colour = \"#2C3E50\",\n                size = 14,\n                face = \"bold\",\n                margin = margin(b = 5)\n            ),\n            plot.subtitle = element_text(\n                family = \"serif\",\n                colour = \"#7F8C8D\",\n                size = 10,\n                margin = margin(b = 15)\n            ),\n            plot.caption = element_text(\n                family = \"serif\",\n                colour = \"#95A5A6\",\n                size = 8,\n                margin = margin(t = 10)\n            ),\n            axis.text.x = element_text(\n                angle = 45, \n                hjust = 1,\n                margin = margin(t = 8),\n                color = \"#34495E\"\n            ), \n            axis.text.y = element_text(\n                margin = margin(r = 8),\n                color = \"#34495E\"\n            ), \n            axis.title = element_text(\n                color = \"#2C3E50\",\n                face = \"bold\"\n            ),\n            panel.grid.minor = element_blank(),\n            panel.grid.major = element_line(\n                color = \"#ECF0F1\", \n                linewidth = 0.3\n            ), \n            panel.background = element_rect(fill = \"#FEFEFE\", color = NA),\n            plot.background = element_rect(fill = \"white\", color = NA),\n            plot.margin = margin(20, 20, 20, 20)\n        ) \n    \n    interactive_plot = girafe(\n        ggobj = p,\n        options = list(\n            opts_hover(css = \"cursor:pointer;stroke-width:4;stroke-opacity:1;fill-opacity:1;r:4px;\"),\n            opts_hover_inv(css = \"opacity:0.1;\"),\n            opts_selection(type = \"none\")\n        )\n    )\n    \n    interactive_plot\n})"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#disclaimer",
    "href": "posts/02-arima-grid-search/index.html#disclaimer",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R",
    "section": "\n1.6 Disclaimer",
    "text": "1.6 Disclaimer\nThis is just a toy example of leveraging functional programming and basic hyperparameter tuning for time series in R, and some of my learning competencies about data visualization in R and how to get deeper in it.\nIf you are interested to learn more, check out my other gists."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My name is Joshua",
    "section": "",
    "text": "Computer Scientist ‚Ä¢ Statistician\n\n\nI am a computer scientist, package creator / maintainer / contributor (to R and Python), a blogger, and a statistician. I write plenty of blogs and articles in statistics, mathematics, programming, and data science, as well as I studied multiple layers of programming paradigms and designs, and still actively learning. I write packages for statistics, data science, and numerical analysis, as my specialties found in those fields.\nI work in a consulting firm and I also independently consulting (feel free to reach me)."
  },
  {
    "objectID": "index.html#joshua-marie",
    "href": "index.html#joshua-marie",
    "title": "My name is Joshua",
    "section": "",
    "text": "Computer Scientist ‚Ä¢ Statistician\n\n\nI am a computer scientist, package creator / maintainer / contributor (to R and Python), a blogger, and a statistician. I write plenty of blogs and articles in statistics, mathematics, programming, and data science, as well as I studied multiple layers of programming paradigms and designs, and still actively learning. I write packages for statistics, data science, and numerical analysis, as my specialties found in those fields.\nI work in a consulting firm and I also independently consulting (feel free to reach me)."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "My name is Joshua",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\nThings you may or you may not know in ggplot2\n\n\n\nR\n\nggplot2\n\ndata-science\n\nvisualization\n\ntidyverse\n\nviz\n\n\n\n\n\n\n\n\n\nDec 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhy SQL + R is an affable combo when I start learning SQL?\n\n\n\nR\n\nSQL\n\ndata-science\n\nanalytics\n\ntidyverse\n\n\n\nLearn mastering both ‚Äî and how to make them work together seamlessly.\n\n\n\n\n\nNov 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTutorial on Generalized Linear Model from scratch\n\n\n\nR\n\nPython\n\nstatistics\n\nregression\n\nmachine-learning\n\n\n\nFinally understand what a link function actually does instead of just copy-pasting glm()\n\n\n\n\n\nNov 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWays to load / attach packages in R\n\n\n\nR\n\npackages\n\n\n\nA completely objective, totally scientific ranking (2025 edition)\n\n\n\n\n\nNov 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow much do you know about pipes?\n\n\n\nR\n\npipes\n\ntidyverse\n\nprogramming\n\n\n\n\n\n\n\n\n\nNov 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hidden Magic of Tidy-Select: R‚Äôs Universal Column Selection Language\n\n\n\nR\n\ntidyselect\n\ntidyverse\n\n\n\n\n\n\n\n\n\nNov 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBox: Placing module system into R\n\n\n\nR\n\nbook\n\n\n\n\n\n\n\n\n\nOct 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFirst level of time series modelling: Basic ARIMA model hyperparameter tuning and grid search in R\n\n\n\nR\n\ntime-series\n\nmachine-learning\n\ngrid-search\n\n\n\n\n\n\n\n\n\nSep 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression\n\n\n\nR\n\nmachine-learning\n\ntorch\n\npointless-code\n\n\n\n\n\n\n\n\n\nSep 23, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "contacts.html",
    "href": "contacts.html",
    "title": "Contact Me",
    "section": "",
    "text": "Email: joshua.marie.k@gmail.com\nGitHub: github.com/joshuamarie\nLinkedIn: linkedin.com/in/joshuamarie/"
  },
  {
    "objectID": "posts/01-meta-nn/index.html#introduction",
    "href": "posts/01-meta-nn/index.html#introduction",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "\n1.1 Introduction",
    "text": "1.1 Introduction\nThe create_nn_module() function dynamically generates torch neural network module definitions. Instead of manually writing out layer definitions and forward pass logic, this function builds the code expressions for you.\nKey benefits:\n\n\nFlexibility: Change network architecture with a single function call\n\nAutomation: Generate multiple network configurations programmatically\n\nExperimentation: Quickly test different architectures in hyperparameter searches\n\nThis is how it‚Äôs done:\n\nDefine the network architecture (input size, hidden layers, output size)\nSpecify activation functions for each layer\nProgrammatically generate the initialize method (layer definitions)\nProgrammatically generate the forward method (forward pass logic)\nReturn an nn_module expression ready to be evaluated\n\nThe packages used:\n\n\nrlang (v1.1.4) - For metaprogramming tools\n\npurrr (v1.0.2) - For functional programming\n\nglue (v1.7.0) - For string interpolation\n\nmagrittr - For pipe operator\n\nbox (v1.2.0) - For modular code organization"
  },
  {
    "objectID": "posts/01-meta-nn/index.html#the-complete-function",
    "href": "posts/01-meta-nn/index.html#the-complete-function",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "\n1.2 The Complete Function",
    "text": "1.2 The Complete Function\nI created create_nn_module() function a while ago and shared it on GitHub Gist. Here‚Äôs the function we‚Äôll be analyzing:\n\nCodecreate_nn_module = function(nn_name = \"DeepFFN\", hd_neurons = c(20, 30, 20, 15), no_x = 10, no_y = 1, activations = NULL) {\n    box::use(\n        rlang[new_function, call2, caller_env, expr, exprs, sym, is_function, env_get_list],\n        purrr[map, map2, reduce, set_names, compact, map_if, keep, map_lgl], \n        glue[glue], \n        magrittr[`%&gt;%`]\n    )\n    \n    nodes = c(no_x, hd_neurons, no_y)\n    n_layers = length(nodes) - 1\n    \n    call_args = match.call()\n    activation_arg = call_args$activations\n    \n    if (is.null(activations)) {\n        activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n    } else if (length(activations) == 1 || is.function(activations)) {\n        single_activation = activations\n        activations = c(rep(list(single_activation), length(hd_neurons)), list(NA))\n    }\n    \n    activations = map2(activations, seq_along(activations), function(x, i) {\n        if (is.null(x)) {\n            NULL\n        } else if (is.function(x)) {\n            if(!is.null(activation_arg) && is.call(activation_arg) && activation_arg[[1]] == quote(c)) {\n                func_name = as.character(activation_arg[[i + 1]])\n                sym(func_name)\n            } else if(!is.null(activation_arg) && (is.symbol(activation_arg) || is.character(activation_arg))) {\n                func_name = as.character(activation_arg)\n                sym(func_name)\n            } else {\n                parent_env = parent.frame()\n                env_names = ls(envir = parent_env)\n                matching_names = env_names %&gt;%\n                    keep(~ {\n                        obj = env_get_list(parent_env, .x)[[1]]\n                        identical(obj, x)\n                    })\n                \n                if (length(matching_names) &gt; 0) {\n                    sym(matching_names[1])\n                } else {\n                    stop(\"Could not determine function name for activation function\")\n                }\n            }\n        } else if (is.character(x)) {\n            if (length(x) == 1 && is.na(x)) {\n                NULL\n            } else {\n                sym(x)\n            }\n        } else if (is.symbol(x)) {\n            x\n        } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n        }\n    })\n    \n    init_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), call2(\"nn_linear\", !!!dims))\n    })\n    \n    init = new_function(\n        args = list(), \n        body = call2(\"{\", !!!init_body)\n    )\n    \n    layer_calls = map(1:n_layers, function(i) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        activation_fn = if (i &lt;= length(activations)) activations[[i]] else NULL\n        \n        result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n        if (!is.null(activation_fn)) {\n            result = append(result, list(call2(activation_fn)))\n        }\n        result\n    }) |&gt; \n        unlist() |&gt; # recursive = FALSE is also valid\n        compact()\n    \n    forward_body = reduce(layer_calls, function(acc, call) {\n        expr(!!acc %&gt;% !!call)\n    }, .init = expr(x))\n    \n    forward = new_function(\n        args = list(x = expr()), \n        body = call2(\"{\", forward_body)\n    )\n    \n    call2(\"nn_module\", nn_name, initialize = init, forward = forward)\n}\n\n\n\n\n1.2.1 Why box?\nYou‚Äôll notice that I‚Äôve been using another approach to load namespace in R. But, why ‚Äòbox‚Äô? You need to check out my mini book dedicated on modular programming in R.\n\n1.2.2 But why load dependencies using box::use() inside a function?\n\nWell, a function, or a function call, creates an environment, which encloses the objects and operations within it. In other words, we create a closure. This is actually a good practice for several reasons:\n\nNamespace isolation: Dependencies loaded inside the function will not make pollution the global environment, or conflicts with any packages loaded. When you load packages required with library(), inside a function or not, it attaches those packages to your search path, and will mask functions from other packages. With box::use() inside a function, the imports are scoped only to that function‚Äôs or call‚Äôs environment.\nExplicit dependencies: Anyone reading the function immediately knows what external tools it uses. You don‚Äôt have to scroll to the top of a script to see what‚Äôs loaded.\nReproducibility: The function becomes self-contained. If you share just this function, others know exactly what packages they need less hunting through documentation.\nAvoiding conflicts: Different functions can use different versions or implementations without conflicts. For example, one function might use dplyr::filter() while another uses stats::filter(), and they won‚Äôt interfere with each other.\nLazy loading: The packages are only loaded when the function is actually called, not when it‚Äôs defined. This can improve script startup time if you have many functions but only use a few.\n\n\n\n\n\n\n\nNote\n\n\n\nIn a nutshell: The ‚Äòbox‚Äô package provides explicit, granular imports, making it transparent which namespace to be imported from which packages. It‚Äôs like having a well-organized toolbox where each tool is labeled."
  },
  {
    "objectID": "posts/01-meta-nn/index.html#explanations",
    "href": "posts/01-meta-nn/index.html#explanations",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "\n1.3 Explanations",
    "text": "1.3 Explanations\nI‚Äôll be trying to be concise on explaining each layers of the function so that you‚Äôll understand what I did\n\n1.3.1 Step 1: Loading Dependencies\nI use box::use() to load dependencies:\n\n\nrlang: Improvised Core R programming. One of the core R programming, metaprogramming which includes creating expressions and functions programmatically, are less painful than what base R have.\n\npurrr: Improvised Functional programming utilities.\n\nglue: R lacks Python‚Äôs f-string for string interpolation, although we have sprintf() and paste() for that. glue makes string interpolation more readable with glue(\"fc{i}\") instead of paste0(\"fc\", i) or sprintf(\"fc%d\", i).\n\nmagrittr: The pipe operator %&gt;% for chaining operations. This is optional, by the way ‚Äî R 4.1+ has the native pipe |&gt;, but %&gt;% offers better flexibility with the dot placeholder.\n\n1.3.2 Step 2: Defining Network Architecture\nIn DFFNN architecture, it is defined by the input layer, the hidden layer, and the output layer.\n\n\nSource: https://medium.com/data-science/designing-your-neural-networks-a5e4617027ed\n\nThe number of nodes are defined by integers, except for input and output layer nodes which they are fixed and determined by the data you provided, and they are defined by no_x and no_y. The number of hidden layers is defined by the length of input in hd_neurons argument.\nCombine no_x, hd_neurons, no_y in order:\nnodes = c(no_x, hd_neurons, no_y)\nAnd then calculate the length of nodes, which is \\(1 + n_{\\text{hidden layres}} + 1\\), and then subtract it by 1 because the applied activation functions is invoked between each layer.\nn_layers = length(nodes) - 1\n\n1.3.2.1 Example\nWhen you have:\n\n10 predictors\n\n5 hidden layers, and for each layer:\n\n20 nodes\n30 nodes\n20 nodes\n15 nodes\n20 nodes\n\n\n1 response variable\n\nTotal number of layers: 7\nThis means we need 7 - 1 linear transformations, and here is my diagram:\n\\[10_{\\text{inputs}} \\rightarrow20_{\\text{hd1}} \\rightarrow30_{\\text{hd2}} \\rightarrow20_{\\text{hd3}} \\rightarrow15_{\\text{hd4}} \\rightarrow20_{\\text{hd5}} \\rightarrow1_{\\text{ouput}}\\]\n\n1.3.3 Step 3: Setting Activation Functions\nThe activations argument holds the account of the activation function. It could be a string, a literal function, or a mix of it in a vector of inputs.\nThen, set activations = NULL, where NULL is the default value, which leads to set ReLU (nnf_relu) as the activation function for all hidden layers\n\nCodeif (is.null(activations)) {\n    activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n}\n\n\nEvery activations will have NA as the last element because we need to ensure no activation function after the output. The output layer often doesn‚Äôt need an activation (for regression) or needs a specific one based on the task (softmax for classification, sigmoid for binary classification). By defaulting to NA, the user can decide.\n\n\n\n\n\n\nNoteLength of inputs\n\n\n\nTo provide values in activations argument, it needs to be equal to the size of hidden layers, or if you provide only 1 act. function, this will be the activation function across the transformations.\n\n\n\n\n\n\n\n\nNoteDefault\n\n\n\nThe default is NULL. That is, if activations is not provided, the activation function is set to ReLU function.\n\n\n\n\n\n\n\n\nTipInstead of NULL\n\n\n\nNow, if you‚Äôre asking ‚ÄúWhy needs to set activations to \"nnf_relu\" instead of NULL‚Äù? Don‚Äôt worry, I did consider that, but this is just a pure demo.\n\n\n\n1.3.4 Step 4: Processing Activation Functions\nThis part (re)processes the activation function inputs in the activations argument. This kept tracks the argument you are putting, especially when you the input you are writing in activations argument has different types.\n\nCodecall_args = match.call()\nactivation_arg = call_args$activations\n\nactivations = map2(activations, seq_along(activations), function(x, i) {\n    if (is.null(x)) {\n        NULL\n    } else if (is.function(x)) {\n        if(!is.null(activation_arg) && is.call(activation_arg) && \n           activation_arg[[1]] == quote(c)) {\n            func_name = as.character(activation_arg[[i + 1]])\n            sym(func_name)\n        } else {\n            func_name = names(which(sapply(ls(envir = parent.frame()), \n                function(name) {\n                    identical(get(name, envir = parent.frame()), x)\n                })))[1]\n            if (!is.na(func_name)) {\n                sym(func_name)\n            } else {\n                stop(\"Could not determine function name for activation function\")\n            }\n        }\n    } else if (is.character(x)) {\n        if (length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            sym(x)\n        }\n    } else if (is.symbol(x)) {\n        x\n    } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n        NULL\n    } else {\n        stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n    }\n})\n\n\n\n1.3.5 Step 5: Building the initialize Method Body\nThe body I am referring in initialize method is the body of the function for the initialize implemented method. This part is a bit far from trivial. I named it init_body to keep track the expression I am trying to build.\n\n\n\n\n\n\nNoteIn reality\n\n\n\nKeep in mind that there‚Äôs no initialize and forward parameters within nn_module() torch namespace or whatsoever. However, it is expected you to create them to create a module inside nn_module(). These parameters are kept within the ... wildcard parameter.\n\n\n\n1.3.5.1 Creation of the expressions inside the body\nHere is the part I am tracking inside create_nn_module body expression:\n\nCodeinit_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), \n    function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), \n              call2(\"nn_linear\", !!!dims))\n    })\n\n\nWhat it does is it creates assignment expressions for each layer in the network.\nFor instance, c(20, 30, 20, 15, 20) is your argument for the activations:\n\n\nmap2(nodes[-length(nodes)], nodes[-1], c) pairs consecutive layer sizes:\n\nCodelist(\n    c(10, 20), \n    c(20, 30), \n    c(30, 20), \n    c(20, 15), \n    c(15, 20), \n    c(20, 1)\n)\n\n\n\n\nFor each pair, generates a layer assignment expression:\n\nLayer names: fc1, fc2, ‚Ä¶, out (last layer)\nCreates: self$fc1 = nn_linear(10, 20)\n\n\n\n\nThis will be the generated expression:\nself$fc1 = nn_linear(10, 20)\nself$fc2 = nn_linear(20, 30)\nself$fc3 = nn_linear(30, 20)\nself$fc4 = nn_linear(20, 15)\nself$fc5 = nn_linear(15, 20)\nself$out = nn_linear(20, 1)\n\n\n\n\n\n\nNoteHow is it done?\n\n\n\nI need you to understand rlang::call2() a bit:\nThe call2() function is a glorified call() from base R that builds function call expressions.\nFrom what I did within init_body:\n\ncall2(\"$\", expr(self), sym(\"fc1\")) constructs self$fc1\n\ncall2(\"nn_linear\", !!!dims) is a bit complex:\n\nIt splices dims from what I created in map2(nodes[-length(nodes)], nodes[-1], c).\n\ncall2() function accepts rlang‚Äôs quasiquotation API, then splices the dimensions, i.e.¬†call2(\"nn_linear\", !!!c(10, 20)) to call2(\"nn_linear\", 10, 20).\nThen finally constructs nn_linear(10, 20)\n\n\n\ncall2(\"=\", lhs, rhs) parses an expression: lhs = rhs. This part yields an expression I want: self$fc1 = nn_linear(10, 20).\n\nNote: You can use &lt;- if you want, instead of =. After all, = within call2()‚Äôs .fn argument tokenize = as an assignment operator. \n\n\n\n1.3.5.2 Building an actual body and function\nNow, for this part:\n\nCodeinit = new_function(\n    args = list(), \n    body = call2(\"{\", !!!init_body)\n)\n\n\nDon‚Äôt forget to put curly brackets { around the built expression because it becomes necessary in R when composing a function with multiple lines. Still using call2() for that, particularly call2(\"{\", !!!init_body) creates a code block { ... } containing all initialization statements. The !!! operator ‚Äúsplices‚Äù the list of expressions into the block, because init_body forms a list of expressions.\nAfter building the expression I want for the body of initialize, let‚Äôs take further by utilizing it as a body to create a function with rlang::new_function. I just simply wraps all the layer initialization expressions into a complete function for initialize method for nn_module().\n\n\n\n\n\n\nNoteInputs in initialize\n\n\n\nNotice that the argument for initialize is empty? I could‚Äôve place input_size and output_size if I wanted to, but it seems unnecessary since I already placed the sizes of the input and output within the expression I built. To make a function expression with empty arguments, place the args argument of new_function with empty list().\n\n\nHere‚Äôs the result:\nfunction() {\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}\nStore this expression into init because we still have to finalize the expression we want to create. \n\n1.3.6 Step 6: Building Layer Calls for Forward Pass\nThe same process as initialize, except we are not building multiple lines of expression, just building a chained expression with ‚Äòmagrittr‚Äô pipe from the initial value.\n\n1.3.6.1 Creating layer of calls\nTo form this expression is also complex\n\nCodelayer_calls = map(1:n_layers, function(i) {\n    layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n    activation_fn = if (i &lt;= length(activations)) activations[[i]] else NULL\n    \n    result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n    if (!is.null(activation_fn)) {\n        result = append(result, list(call2(activation_fn)))\n    }\n    result\n}) |&gt; \n    unlist() |&gt; \n    compact()\n\n\nWhat it does is it builds a sequence of operations for the forward pass: layer calls and their activation functions. I stored the output into layer_calls so that we can keep track of it.\nThe process:\n\n\nFor each layer, create a list containing:\n\nThe layer call: self$fc1()\n\nThe activation call (if exists): nnf_relu()\n\n\n\nFlatten all lists into a single sequence with unlist().\nFilter the list we created away from any NULL values with purrr::compact().\n\nThus, we form a list of expressions:\n\nCodelist(\n    self$fc1(), nnf_relu(),\n    self$fc2(), nnf_relu(),\n    self$fc3(), nnf_relu(),\n    self$fc4(), nnf_relu(),\n    self$fc5(), nnf_relu(),\n    self$out()\n)\n\n\nNote: The last layer (out) has no activation because we set it to NA.\n\n1.3.6.2 Building an actual body and function\nI choose to chain them, x or the input as the initial value, and choose not to break lines and forms multiple assignments. This is what I preferred, and besides, it‚Äôs so easy to form chained expression when the output is a defused call with reduce().\n\nCodeforward_body = reduce(layer_calls, function(acc, call) {\n    expr(!!acc %&gt;% !!call)\n}, .init = expr(x))\n\n\nI choose to chain all operations together with pipe operator %&gt;% from ‚Äòmagrittr‚Äô.\nThen, with reduce() works:\n\n\nStarting with x, it progressively adds each operation:\n\nStep 1: x %&gt;% self$fc1()\n\nStep 2: (x %&gt;% self$fc1()) %&gt;% nnf_relu()\n\nStep 3: (x %&gt;% self$fc1() %&gt;% nnf_relu()) %&gt;% self$fc2()\n\n‚Ä¶and so on\n\n\n\nAs for the final output:\nx %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% \n    self$fc2() %&gt;% nnf_relu() %&gt;% \n    self$fc3() %&gt;% nnf_relu() %&gt;% \n    self$fc4() %&gt;% nnf_relu() %&gt;% \n    self$fc5() %&gt;% nnf_relu() %&gt;% \n    self$out()\n\n\n\n\n\n\n\n\nTipWhy pipes?\n\n\n\nThe pipe operator makes the forward pass logic read like a natural sequence: ‚Äútake input x, pass through fc1, apply nnf_relu to invoke ReLU activation function, then pass through fc2, apply nnf_relu to invoke ReLU activation function, ‚Ä¶, it kepts repeating until we reach to out‚Äù\n\n\nAfter that, I stored it into forward_body, then make use of it to build the function for forward method with rlang::new_function():\n\nCodeforward = new_function(\n    args = list(x = expr()), \n    body = call2(\"{\", forward_body)\n)\n\n\nThe args for forward method has x with empty value. Then, wrap the piped forward pass into a function that accepts input x.\nAnd here‚Äôs the result:\nfunction(x) {\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% \n        self$fc2() %&gt;% nnf_relu() %&gt;% \n        self$fc3() %&gt;% nnf_relu() %&gt;% \n        self$fc4() %&gt;% nnf_relu() %&gt;% \n        self$fc5() %&gt;% nnf_relu() %&gt;% \n        self$out()\n}\nStore this expression into forward because we still have to finalize the expression we want to create. \n\n1.3.7 Step 7: Finalizing the nn_module Expression generation\nHere we are for the final part: generating the nn_module expression, by puzzling each part: initialize and forward.\nThe final part is built from this:\ncall2(\"nn_module\", nn_name, !!!set_names(list(init, forward), c(\"initialize\", \"forward\")))\nI mean, you still have to use call2() to build a call. The inputs should be:\n\n.fn = \"nn_module\" -&gt;\n\nThe rest of the arguments:\n\n\nnn_name which is equivalent to ‚ÄúDeepFFN‚Äù. You can set any names whatever you want, though.\ninitialize = init\nforward = forward\nOriginally, I formed this in this expression: !!!set_names(list(init, forward), c(\"initialize\", \"forward\")). But then, I realized that we only need initialize and forward, and besides, this is a bit overkill.\n\n\n\nThus, the final expression that defines the neural network module.\nAnd hence, I form a function that generates a, perhaps, template:\n\nhd_nodes = c(20, 30, 20, 15, 20)\nact_fns = c(\"nnf_relu\", \"nnf_relu\", \"nnf_relu\", \"nnf_relu\")\ncreate_nn_module(\n    hd_neurons = hd_nodes, \n    activations = act_fns\n)\n\nnn_module(\"DeepFFN\", initialize = function () \n{\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}, forward = function (x) \n{\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% self$fc2() %&gt;% nnf_relu() %&gt;% \n        self$fc3() %&gt;% nnf_relu() %&gt;% self$fc4() %&gt;% nnf_relu() %&gt;% \n        self$fc5() %&gt;% self$out()\n})"
  },
  {
    "objectID": "posts/01-meta-nn/index.html#disclaimer",
    "href": "posts/01-meta-nn/index.html#disclaimer",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "\n1.4 Disclaimer",
    "text": "1.4 Disclaimer\nThis is an advanced example of metaprogramming in R, demonstrating how to leverage functional programming and rlang for code generation. I don‚Äôt mind you to replicate what I did, but sometimes this technique should be used judiciously‚Äîsometimes simpler, more explicit code is better.\nThis example showcases:\n\nDeep understanding of R‚Äôs evaluation model\nFunctional programming with purrr\n\nExpression manipulation with rlang\n\nPractical application to deep learning workflows\n\nAnd also, I am aware to the fact that the function I made is ugly if you said so."
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#what-youll-find-here",
    "href": "posts/03-modules-in-r/index.html#what-youll-find-here",
    "title": "Box: Placing module system into R",
    "section": "1.1 What You‚Äôll Find Here",
    "text": "1.1 What You‚Äôll Find Here\nA definitive guide that walks you through:\n\nAlternative approach to import R codes (i.e.¬†R packages, modules)\nModern approaches to compose and organize R codes\nStep-by-step tutorials for using the box package\nBest practices for maintainable R code"
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#why-this-book",
    "href": "posts/03-modules-in-r/index.html#why-this-book",
    "title": "Box: Placing module system into R",
    "section": "1.2 Why this book",
    "text": "1.2 Why this book\nOnly little to no books teach you to correctly write reusable, composable, and modular R codes :). Most of the books maybe teaches you about R, particularly application of R in different fields, but little to none explains one of the best practices. Most of them uses library() anyways, so you won‚Äôt certainly find similar book like this.\nConsequently, while R offers various ways to organize code, the box package manages to be superior among them (I am bias) by introducing a fresh, modern approach to module system that may significantly improve your R development workflow, similar to the workflow made by other developers, particularly Python devs. This book bridges the gap between basic R programming and professional-grade code organization."
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#how-to-use-this-guide",
    "href": "posts/03-modules-in-r/index.html#how-to-use-this-guide",
    "title": "Box: Placing module system into R",
    "section": "1.3 How to Use This Guide",
    "text": "1.3 How to Use This Guide\nClick here and it will send you to the actual book. \nThe content is structured progressively, building from foundational concepts to deep applications. For the best learning experience:\n\nStart with the introduction to understand the core concepts\nLearning the fundamentals of import system with {box} package\nLearning the structures and constructions of reusability of modules, treating them like R packages, and supplied with documentation\nLearning unit testing the modules"
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#contributing",
    "href": "posts/03-modules-in-r/index.html#contributing",
    "title": "Box: Placing module system into R",
    "section": "1.4 Contributing",
    "text": "1.4 Contributing\nContributions are welcome! If you have suggestions or improvements, please open an issue or submit a pull request on the GitHub repository."
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#license",
    "href": "posts/03-modules-in-r/index.html#license",
    "title": "Box: Placing module system into R",
    "section": "1.5 License",
    "text": "1.5 License\nThis book is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). See the LICENSE file for details."
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#credits",
    "href": "posts/03-modules-in-r/index.html#credits",
    "title": "Box: Placing module system into R",
    "section": "1.6 Credits",
    "text": "1.6 Credits\nThis book was created by Joshua Marie.\nSpecial thanks to:\n\nKonrad Rudolph for creating and maintaining the box package\nThe R community for their continued support and feedback\n\nThe book is built with Quarto, hosted on GitHub Pages."
  },
  {
    "objectID": "posts/05-pipes/index.html#the-piper-pioneer-2013",
    "href": "posts/05-pipes/index.html#the-piper-pioneer-2013",
    "title": "How much do you know about pipes?",
    "section": "\n4.1 1. The {pipeR} Pioneer (2013)",
    "text": "4.1 1. The {pipeR} Pioneer (2013)\nThe pipeR package by Kun Ren was one of the earliest pipe implementations in R, introducing the %&gt;&gt;% operator.\n\nbox::use(pipeR[`%&gt;&gt;%`])\n\n1:10 %&gt;&gt;%\n    mean() %&gt;&gt;%\n    round(2)\n\n[1] 5.5\n\n\nHere‚Äôs the cool part:\n\nLambda expressions with parentheses:\n\n\n1:10 %&gt;&gt;%\n    (mean(.) * 2) %&gt;&gt;%\n    round(2)\n\n[1] 11\n\n\n\nSide effects with continued piping:\n\n\nset.seed(123)\nrnorm(100) %&gt;&gt;%\n    (~ plot(., main = \"Random Normal Values\")) %&gt;&gt;%  # Side effect\n    mean() %&gt;&gt;%\n    round(2)\n\n\n\n\n\n\n\n[1] 0.09\n\n\n\n\n\n\n\n\nNoteWhy it faded\n\n\n\nIt‚Äôs not like it vanished from the existence, more like it is superseded by magrittr and took over."
  },
  {
    "objectID": "posts/05-pipes/index.html#the-game-changer-magrittr-pipe-2014",
    "href": "posts/05-pipes/index.html#the-game-changer-magrittr-pipe-2014",
    "title": "How much do you know about pipes?",
    "section": "\n4.2 2. The Game Changer: {magrittr} Pipe (2014)",
    "text": "4.2 2. The Game Changer: {magrittr} Pipe (2014)\nThe magrittr package, created by Stefan Milton Bache and later maintained by Lionel Henry at Posit (formerly RStudio), became the most popular pipe implementation. It was inspired by F#‚Äôs pipe-forward operator and Unix pipes.\n\nbox::use(magrittr[`%&gt;%`, `%&lt;&gt;%`, `%T&gt;%`, `%$%`, `%!&gt;%`])\n\nc(1, 2, 3, 4, 5) %&gt;%\n    mean() %&gt;%\n    round(2)\n\n[1] 3\n\n\nDo you know? There are plenty pipe operators in magrittr package, consists of at least 5 operators. Here are the special features:\n\n\nThe classic\nAssignment pipe\nTee pipe\nExposition pipe\nEager pipe\n\n\n\nThe %&gt;% is magrittr‚Äôs standard and ‚Äúlazy‚Äù pipe - it doesn‚Äôt evaluate arguments until needed, which can affect behavior with certain functions. Lazy evaluation means that the RHS is only computed when its value is required, which optimizes performance but can lead to surprises with side-effect-heavy code.\nTo understand better how %&gt;% works, let‚Äôs give a demonstration by applying dot placeholder for non-first arguments:\n\nmtcars %&gt;%\n    lm(mpg ~ cyl, data = .)\n\n\nCall:\nlm(formula = mpg ~ cyl, data = .)\n\nCoefficients:\n(Intercept)          cyl  \n     37.885       -2.876  \n\n\nThe dot (.) acts as a placeholder for the piped value, allowing it to be inserted into any argument position‚Äînot just the first. You can also apply multiple placeholders:\n\nmtcars %&gt;% \n    head(5) %&gt;% \n    split(., .$cyl)\n\n$`4`\n            mpg cyl disp hp drat   wt  qsec vs am gear carb\nDatsun 710 22.8   4  108 93 3.85 2.32 18.61  1  1    4    1\n\n$`6`\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n\n$`8`\n                   mpg cyl disp  hp drat   wt  qsec vs am gear carb\nHornet Sportabout 18.7   8  360 175 3.15 3.44 17.02  0  0    3    2\n\n\n\n\nThe %&lt;&gt;% operator is invoking reference semantics, where it pipes and assigns the result back to the original variable:\n\nx = 1:5\nx %&lt;&gt;% log() %&gt;% sum()\nx\n\n[1] 4.787492\n\n\nThis is equivalent to x = x %&gt;% log() %&gt;% sum() but more concise. What happened here is we created a side-effect of x. Some pointed it out why it is a problem.\n\n\nThe %T&gt;% ‚Äútee‚Äù pipe passes the left-hand side value forward, not the output of the right-hand side. Useful for side effects like plotting or printing, where you want to perform an action but continue with the original data:\n\nset.seed(123)\nrnorm(100) %T&gt;% \n    plot(main = \"Values before mean\") %&gt;% \n    mean() %&gt;%\n    round(2)\n\n\n\n\n\n\n\n[1] 0.09\n\n\nThis should be the equivalent:\n{\n    set.seed(123)\n    plot(rnorm(100), main = \"Values before mean\")\n    round(mean(rnorm(100)), 2)\n}\nSo, if you try the following:\n\n1:5 %T&gt;% \n    mean()\n\n[1] 1 2 3 4 5\n\n\nThe %T&gt;% operator discards the output of mean(1:5), and that‚Äôs because mean() doesn‚Äôt return a side-value effect.\nBy the way, the ‚Äútee‚Äù name comes from Unix‚Äôs tee command, which splits output streams.\n\n\nThe %$% ‚Äúexposition‚Äù pipe exposes the names within the left-hand side object to the right-hand side expression:\n\nmtcars %$%\n    cor(mpg, cyl)\n\n[1] -0.852162\n\n\nThis is equivalent to:\ncor(mtcars$mpg, mtcars$cyl)\nThis is particularly useful with functions that don‚Äôt have a data argument.\n\n\n\n\n\n\nWarning\n\n\n\nDo not use %$% operator when LHS is not a a list or data frame with named elements.\n\n\n\n\nThe %!&gt;% operator is the ‚Äúeager‚Äù version of %&gt;% that evaluates arguments immediately. This can matter for functions with non-standard evaluation:\n\n# Standard (lazy) pipe\niris %&gt;% \n    subset(Species == \"setosa\") %&gt;% \n    head(3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\n# Eager pipe (forces immediate evaluation)\niris %!&gt;% \n    subset(Species == \"setosa\") %!&gt;% \n    head(3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\n\nIn most cases, the difference is subtle, but it can matter for advanced programming.\nTo see the actual difference:\n\n\n%!&gt;%: cat(1) is immediately evaluated (it evaluates from left to right)\n\n\n0 %!&gt;% (\\(x) { cat(1); x }) %!&gt;% (\\(x) cat(2))\n\n12\n\n\n\n\n%&gt;%: Evaluates only cat(2) as the first result is never used\n\n\n0 %&gt;% (\\(x) { cat(1); x }) %&gt;% (\\(x) cat(2))  \n\n2\n\n\nSource: https://stackoverflow.com/questions/76326742/what-are-the-differences-and-use-cases-of-the-five-magrittr-pipes"
  },
  {
    "objectID": "posts/05-pipes/index.html#the-wrapr-dot-arrow-2017",
    "href": "posts/05-pipes/index.html#the-wrapr-dot-arrow-2017",
    "title": "How much do you know about pipes?",
    "section": "\n4.3 3. The {wrapr} Dot Arrow (2017)",
    "text": "4.3 3. The {wrapr} Dot Arrow (2017)\nJohn Mount‚Äôs wrapr package provides the %.&gt;% ‚Äúdot arrow‚Äù pipe, a deliberate and explicit alternative to %&gt;%.\n\nbox::use(wrapr[`%.&gt;%`])\n\n1:10 %.&gt;%\n    mean(.) %.&gt;%\n    round(., 2)\n\n[1] 5.5\n\n\nI don‚Äôt know much about this pipe, to be honest. As what I can see, this pipe requires the dot to always be explicit, which, for me, it‚Äôs so good that it can prevent some subtle bugs and makes code intentions clearer."
  },
  {
    "objectID": "posts/05-pipes/index.html#the-bizarro-pipe-base-r-2017",
    "href": "posts/05-pipes/index.html#the-bizarro-pipe-base-r-2017",
    "title": "How much do you know about pipes?",
    "section": "\n4.4 4. The Bizarro Pipe (Base R, ~2017)",
    "text": "4.4 4. The Bizarro Pipe (Base R, ~2017)\nI am not sure when this operator released, but there‚Äôs a pipe operator (not categorically) in base R: the ‚ÄúBizarro pipe‚Äù (-&gt;.;), that works like %&gt;% and %.&gt;%. It‚Äôs not a formal operator but an emergent behavior from combining existing R syntax.\n\n1:10 -&gt;.; \n    mean(.) -&gt;.; \n    round(., 2)\n\n[1] 5.5\n\n\nThe Bizarro pipe works by:\n\nUsing right assignment -&gt; to assign to . (this is done by typing - + &gt; + .)\nEnding each statement with ; to separate expressions\nThe next line uses . as input\n\nIt‚Äôs called ‚ÄúBizarro‚Äù because it uses right-to-left assignment syntax (-&gt;) to create a left-to-right workflow.\nHowever, it has disadvantages (talked in this Stackoverflow discussion):\n\nCreates hidden side-effects (the persistent . variable)\nGoes against R style guides (right assignment and semicolons are discouraged)\nCan lead to subtle bugs if you forget to assign to . at some step\nThe . variable is hidden from ls() and IDE inspectors\nIt‚Äôs so pesky, it won‚Äôt auto-indent\n\nSeriously, I won‚Äôt recommend Bizarro pipe at all. It is still a nice touch as a temporary replacement of %&gt;% for chained R codes, and will not use it for production code."
  },
  {
    "objectID": "posts/05-pipes/index.html#the-native-pipe-r-v4.1-2021",
    "href": "posts/05-pipes/index.html#the-native-pipe-r-v4.1-2021",
    "title": "How much do you know about pipes?",
    "section": "\n4.5 5. The Native Pipe (R v4.1+, 2021)",
    "text": "4.5 5. The Native Pipe (R v4.1+, 2021)\nIn May 2021, R v4.1 introduced the native pipe operator |&gt; (type | and &gt;), bringing pipe functionality into base R without the need for external packages. This operator is the actual operator that was inspired by the pipe-forward operator in F# and the concept of Unix pipes.\n\nc(1, 2, 3, 4, 5) |&gt;\n    mean() |&gt;\n    round(2)\n\n[1] 3\n\n\nThis is too identical to %&gt;% from magrittr with some obvious differences.\n\n4.5.1 Common differences from {magrittr} pipe\nThe placeholder for |&gt; is now applied in R v4.2 and above. For the syntax, it rather uses _, not ..\n\nmtcars |&gt;\n    lm(mpg ~ cyl, data = _)\n\n\nCall:\nlm(formula = mpg ~ cyl, data = mtcars)\n\nCoefficients:\n(Intercept)          cyl  \n     37.885       -2.876  \n\n\nThe native pipe:\n\nIs slightly faster (negligible in often cases, this matters for some cases like running for-loop)\nDoes not support the tee (%T&gt;%), exposition (%$%), or assignment (%&lt;&gt;%) operators\nCannot be used with compound assignment\nIs more strict about valid syntax\n\n4.5.2 Performance comparison\nThe native pipe is clearly faster than the magrittr pipe because native pipe does not add more function calls within its implementation compared to the magrittr pipe.\n\nbench::mark(\n    \"magrittr pipe\" = replicate(10000, 1:100 %&gt;% sum()), \n    \"native R pipe\" = replicate(10000, 1:100 |&gt; sum())\n)\n\n# A tibble: 2 √ó 6\n  expression         min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;    &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 magrittr pipe   38.7ms   39.9ms      25.1     375KB    113. \n2 native R pipe   10.5ms   12.9ms      76.8     369KB     40.1\n\n\n\n4.5.3 Pipe-bind operator\nAfter R v4.2, the pipe-bind operator =&gt; (type = + &gt;), or a pipe-binding syntax, allows you to bind the result of the left-hand side (LHS) to a name within the right-hand side (RHS) expression.\nThis feature is, however, disabled by default. You may want to enable it by running the following:\n\nSys.setenv(\"_R_USE_PIPEBIND_\" = TRUE)\n\nAnother options:\n\nPlace this command into .Renviron file (Hint: run usethis::edit_r_environ()):\n\n_R_USE_PIPEBIND_=true\n\nRun this in a command prompt or PowerShell\n\nsetx _R_USE_PIPEBIND_ true\nIf you are in Linux / macOS (bash / zsh):\nexport _R_USE_PIPEBIND_=true\nThen restart R.\nHere‚Äôs what it does:\n\nmtcars |&gt; \n    df =&gt; lm(mpg ~ wt, data = df)\n\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nmtcars |&gt; \n    df =&gt; split(df, df$cyl) |&gt; \n    lapply(\\(df) lm(mpg ~ wt, data = df)) |&gt; \n    vapply(\\(mod) summary(mod)$r.squared, numeric(1))\n\n        4         6         8 \n0.5086326 0.4645102 0.4229655 \n\n\nThe df name temporarily exists only inside that RHS expression ‚Äî not in your global environment. I like this because this is more explicit than . in %&gt;% operator. You can name the LHS result and refer to it directly inside the RHS expression anything you like."
  },
  {
    "objectID": "posts/05-pipes/index.html#non-pipe-alternatives",
    "href": "posts/05-pipes/index.html#non-pipe-alternatives",
    "title": "How much do you know about pipes?",
    "section": "\n4.6 Non-Pipe Alternatives",
    "text": "4.6 Non-Pipe Alternatives\nWhile the above are true pipe operators, it‚Äôs worth mentioning that some packages achieve similar left-to-right workflows through different mechanisms.\n\n4.6.1 Chaining in {data.table} (2010)\ndata.table uses method chaining with [][] notation. It is NOT a pipe operator in a sense, but achieves a similar left-to-right flow. It behaves differently from the pipe operator ‚Äî it chains operations within the same [.data.table method, and doesn‚Äôt pass values between functions, i.e.¬†the use of placeholders.\nLet‚Äôs look at the basic data.table example:\n\nbox::use(data.table[as.data.table, `:=`])\n\ndt = as.data.table(mtcars)\ndt[cyl == 8][order(-mpg)][, .(mpg, cyl, hp)][1:5]\n\n     mpg   cyl    hp\n   &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:  19.2     8   175\n2:  18.7     8   175\n3:  17.3     8   180\n4:  16.4     8   180\n5:  15.8     8   264\n\n\nDeeper method chaining in data.table with grouping and aggregation:\n\ndt[, log_mpg := log(mpg)][,\n    .(\n        mpg_mean = mean(mpg, na.rm = TRUE), \n        log_mpg_mean = mean(log_mpg, na.rm = TRUE)\n    ), by = cyl\n][\n    order(-mpg_mean, -log_mpg_mean)\n]\n\n     cyl mpg_mean log_mpg_mean\n   &lt;num&gt;    &lt;num&gt;        &lt;num&gt;\n1:     4 26.66364     3.270454\n2:     6 19.74286     2.980439\n3:     8 15.10000     2.700171\n\n\nThis is method chaining, not piping‚Äîthe key difference is that pipes pass values between different functions, while data.table chains operations within the same [ method.\n\n4.6.2 Multiple Assignment with {zeallot} (2018)\nThis is not exactly an operator that behaves like a pipe, where it passes LHS as an input for RHS, but I would like to point this one out. R lacks destructuring (also called ‚Äúunpacking‚Äù) method, just like what you see in other languages, such as Python:\nx, y = 0, 1\nThe zeallot allows destructuring assignment with %&lt;-%. While not exactly a pipe operator to chain the commands, works well in pipe-like workflows.\n\nbox::use(zeallot[`%&lt;-%`])\n\n# Multiple assignment\nc(a, b) %&lt;-% c(1, 2)\nc(a, b)\n\n[1] 1 2\n\n\nDestructuring with computations:\n\nc(mean_val, sd_val, n) %&lt;-% local ({\n    set.seed(125)\n    x = rnorm(100)\n    c(mean(x), sd(x), length(x))\n})\n\ncat(glue::glue(\"Mean: {mean_val}, SD: {sd_val} , N: {n}\"), \"\\n\")\n\nMean: 0.100208694251594, SD: 1.06105719788861 , N: 100 \n\n\nWorks with pipe workflows:\n\nset.seed(123)\nc(m, s) %&lt;-% (rnorm(100) %&gt;% { c(mean(.), sd(.)) })\nc(m, s)\n\n[1] 0.09040591 0.91281588"
  },
  {
    "objectID": "posts/07-glm/index.html",
    "href": "posts/07-glm/index.html",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "",
    "text": "Stop asking:\nKidding~\nI have no such problem of you guys asking these questions. This type of question, however, is a type of question that I heard many times already. I mean, I understand why there‚Äôs so many misconceptions, and it could be they are not properly taught or‚Ä¶I don‚Äôt know.\nI have no problems with tutorials and blogs online about teaching statistics, like predicting what species of iris is gonna be, or how likely you‚Äôll get ebola. There are ten thousand blog posts teaching you how to predict iris species with glm(), or Python‚Äôs sklearn.linear_model.LogisticRegression()\nToday we‚Äôre going full math gremlin mode and actually understanding why GLMs exist, why linear regression is a lie for most real data, and why the link function is the most genius hack in 20th-century statistics."
  },
  {
    "objectID": "resources.html#main-website",
    "href": "resources.html#main-website",
    "title": "My resources",
    "section": "Main Website",
    "text": "Main Website\n\n\nHome: https://joshuamarie.com/\nServices\nBlog Posts: https://joshuamarie.com/posts/"
  },
  {
    "objectID": "resources.html#books",
    "href": "resources.html#books",
    "title": "My resources",
    "section": "Books",
    "text": "Books\n\n\nBox: Placing module system into R"
  },
  {
    "objectID": "resources.html#slides",
    "href": "resources.html#slides",
    "title": "My resources",
    "section": "Slides",
    "text": "Slides\n\n\nCuration of slides"
  },
  {
    "objectID": "services.html#firm",
    "href": "services.html#firm",
    "title": "Work with Me",
    "section": "Consultation Firm",
    "text": "Consultation Firm\n\n\n\n\n\n\nTerms and Condition\n\n\n\nRead terms & and conditions first before booking.\n\n\n\n\n\nWhat I Offer\nMathematics & statistics foundation\n\nCalculus\nLinear Algebra\nProbability and statistics theory\n\nStatistical modelling & Machine Learning 1\n\nStatistical inference and simulation\nBayesian inference with Stan/CmdStan\nCustom model development and validation\n\nStatistical modelling & Machine Learning 2\n\nExperimental design and causal inference\nLongitudinal study and time series analysis & forecasting\nApplications with Python and R\n\n\n\nResearch & Methods Consultation\n\nStudy design and methodology review\nStatistical analysis plan development\nLongitudinal studies and panel data\nPower analysis and sample size calculation\nManuscript review (methods & results sections)\n\n\n\n\nWhat I Offer\nData Science\n\nData Visualization ({ggplot2}, {altair})\nShiny app development\nData manipulation and relational algebra with SQL and {tidyverse}\nGeospatial data analysis\n\nPackage Development\n\nR/Python package development\nAPI design and documentation\nTesting frameworks\nOpen-source maintenance strategies\n\n\n\n\n\n\n\nRates, Inquiry & Engagement\n\n\nRetainer packages available for ongoing work\nBook the consultation firm in Calendly\nOr email me at: joshua.marie.k@gmail.com"
  },
  {
    "objectID": "services.html#prods",
    "href": "services.html#prods",
    "title": "Work with Me",
    "section": "Products",
    "text": "Products\n\n&lt; Not available for now &gt;"
  },
  {
    "objectID": "services.html#tac",
    "href": "services.html#tac",
    "title": "Work with Me",
    "section": "Terms & Conditions",
    "text": "Terms & Conditions\n\n\n\n\n\n\nEngagement & Pricing\n\n\n\n\n\nAll service fees are determined based on project scope, complexity, and timeline. Rates will be clearly communicated and agreed upon in writing before any work commences. Custom quotes are provided following an initial consultation or discovery call.\nAcademic Discount: Graduate students and non-profit research institutions qualify for a 20% discount on consulting services.\n\n\n\n\n\n\n\n\n\nPayment Terms\n\n\n\n\n\nConsulting & Development: A 50% deposit is required before project initiation, with the remaining balance due upon completion. Hourly consulting can be billed monthly for ongoing retainers.\nCourses: Full payment is required at registration. Course materials include a 14-day satisfaction guarantee from the first session date.\nFreelance Projects: Payment terms are negotiated per project, typically 30-50% upfront with milestone-based payments for larger engagements.\n\n\n\n\n\n\n\n\n\nPayment Methods\n\n\n\n\n\nCalendly Bookings: For consultation sessions and course registrations, payment is processed directly through Calendly at the time of booking.\nPayPal: International clients can pay via PayPal for added convenience. Link provided in invoice.\nWise (TransferWise): Preferred for international transfers with lower fees. Details shared upon request.\nInvoicing: All project-based work is formally invoiced via email with payment instructions and due dates.\n\n\n\n\n\n\n\n\n\nScope & Revisions\n\n\n\n\n\nEach project includes reasonable revisions as outlined in the initial agreement. Minor adjustments and clarifications are provided at no additional charge. Significant scope changes, additional analyses, or new deliverables will require a revised quote and written approval before proceeding.\n\n\n\n\n\n\n\n\n\nConfidentiality & Data Security\n\n\n\n\n\nAll client data, project details, and proprietary information are treated with strict confidentiality. I adhere to industry-standard data protection practices and will not disclose any information without explicit written consent. Upon request, a Non-Disclosure Agreement (NDA) can be executed prior to engagement.\n\n\n\n\n\n\n\n\n\nProject Timelines\n\n\n\n\n\nTimelines are established during the initial consultation and confirmed in the project agreement. I commit to meeting agreed-upon deadlines and will communicate proactively if unforeseen circumstances arise.\nClient Responsibilities: Timely provision of data, feedback, and necessary resources is essential. Delays in client-side deliverables may impact project timelines and will be addressed collaboratively.\n\n\n\n\n\n\n\n\n\nIntellectual Property\n\n\n\n\n\nClient Work: All custom deliverables, analyses, and reports created specifically for a client become the client‚Äôs property upon full payment.\nReusable Components: I retain the right to reuse general methodologies, code frameworks, and non-proprietary techniques developed during projects.\nOpen Source: For package development projects, licensing terms (MIT, GPL, etc.) will be agreed upon in advance.\n\n\n\n\n\n\n\n\n\nCancellation Policy\n\n\n\n\n\nConsulting Sessions: Cancellations with less than 24 hours notice will be charged at 50% of the session fee. No-shows will be charged the full amount.\nCourses: Cancellations made 7+ days before the first session receive a full refund. Cancellations within 7 days are non-refundable but can be transferred to a future cohort.\nProject Work: Cancellation terms are negotiated per project. Completed work up to the cancellation point will be invoiced.\n\n\n\n\n\n\n\n\n\nLimitation of Liability\n\n\n\n\n\nServices are provided with professional care and expertise. However, I cannot guarantee specific outcomes or results. Liability is limited to the total fees paid for the specific engagement. Clients are responsible for validating all analyses and recommendations within their specific context before implementation.\n\n\n\nQuestions about these terms? Contact me at joshua.marie.k@gmail.com"
  },
  {
    "objectID": "contacts.html#lets-connect",
    "href": "contacts.html#lets-connect",
    "title": "Joshua Marie",
    "section": "Let‚Äôs Connect",
    "text": "Let‚Äôs Connect\n\nQuick call: Book 15-min discovery call\nEmail: joshua.marie.k@gmail.com\nLinkedIn: linkedin.com/in/joshuamarie/\nGitHub: github.com/joshuamarie"
  },
  {
    "objectID": "posts/07-glm/index.html#lets-start-with-the-math",
    "href": "posts/07-glm/index.html#lets-start-with-the-math",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n1 Let‚Äôs start with the math",
    "text": "1 Let‚Äôs start with the math\nWhat did you observe on the mathematical model formula of linear regression and generalized linear model (GLM)?\nTake a look:\nLinear Regression:\n\\[Y = \\textbf{X}\\beta\\ +\\ \\epsilon\\]\nwhere \\(\\textbf{X}\\beta\\) expands to: \\[\\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_3\\beta_3 + \\dots + x_n\\beta_n\\]\nHow about on GLM?\n\\[\ng(\\mu_i) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta} = \\eta_i \\quad \\text{(linear predictor)}\n\\]\n\\[g(\\mu)=\\eta=\\textbf{X}\\beta\\]\nboth are taking the account of the linear model, where the response have linear relationship with the predictors, taken account by \\(\\textbf{X}\\beta\\).\nThere are few exceptions:\n\nGLMs can take handle of the range not only lies on the real number, but with some strict intervals, unlike Linear regression who has always a range lies on the real number.\n\nGLMs doesn‚Äôt have an error term. Why is that? GLMs takes considerations on the expected value of the linear model, and when you take the expected value like this:\n\\[E(Y)=E(\\textbf{X}\\beta\\ + \\ \\epsilon)\\]\nThe expected values \\(E(\\textbf{X}\\beta) = \\textbf{X}\\beta\\) (assuming no randomness in both terms here) and \\(E(\\epsilon) = 0\\). Hence:\n\\[E(Y)=\\textbf{X}\\beta\\]\nand GLMs only care about the expected value\n\n\n\n1.1 But wait, what‚Äôs this \\(g(\\mu)\\) doing here?\nYes, it‚Äôs odd, but hey, it just works.\nThere‚Äôs this thing called link function, and it‚Äôs the reason GLMs exist. Let me break down why we need this weird little function:\nIn linear regression, the mean of the model would be: \\[\\mu = E(Y) = \\textbf{X}\\beta\\]\nOkay, this definitely feels like ‚ÄúHey, it works, okay?‚Äù, but that‚Äôs my intuition about GLMs. I hope you understood a little in this part. Anyways, that‚Äôs the mean of the linear regression that we know of, and this works great when \\(y\\) can be any real number. But, I am asking you a question:\nWhat if your outcome isn‚Äôt allowed to be just any real number?\n\nWhat if \\(Y\\) is a count \\((0, 1, 2, 3, ...)\\)? \\(\\textbf{X}\\beta\\) could give you -3.7 customers\nWhat if \\(Y\\) is a probability (0 to 1)? \\(\\textbf{X}\\beta\\) could give you 1.4, which is not a probability\nWhat if \\(Y\\) is binary (0 or 1)? \\(\\textbf{X}\\beta\\) could give you 0.3, and you can‚Äôt be 30% pregnant\n\nThis strange thing-y in GLMs, link function \\(g(\\cdot)\\), solves this by transforming the mean so that the linear predictor \\(\\eta = \\textbf{X}\\beta\\) can be any real number, while \\(\\mu\\) stays in the valid range.\nUnderstand? No? Alright, let‚Äôs say, the output of \\(\\eta\\), which is an equivalent of \\(\\textbf{X}\\beta\\), should be at least close to the value in \\(g(\\mu)\\), which is an equivalent of \\(g(E(y))\\).\nMathematically: \\[g(\\mu) = \\eta = \\textbf{X}\\beta\\]\nSo the inverse is: \\[\\mu = g^{-1}(\\eta) = g^{-1}(\\textbf{X}\\beta)\\]\nAgain, to understand, we fit the model in ‚Äútransformed space‚Äù where everything is linear, then transform back to get predictions in the correct range.\nList of common link functions:\n\n\n\n\n\n\n\n\n\nLink\n\\(g(\\mu)\\)\n\\(\\mu = g^{-1}(\\eta)\\)\nUsed for\nWhy\n\n\n\nIdentity\n\\(\\mu\\)\n\\(\\eta\\)\nNormal/Gaussian\nNo transformation needed\n\n\nLogit\n\\(\\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\)\n\\(\\frac{1}{1+e^{-\\eta}}\\)\nBinomial/Binary\nMaps \\((0,1) \\to (-\\infty, \\infty)\\)\n\n\n\nLog\n\\(\\log(\\mu)\\)\n\\(e^{\\eta}\\)\nPoisson/Counts\nMaps \\((0, \\infty) \\to (-\\infty, \\infty)\\)\n\n\n\nProbit\n\\(\\Phi^{-1}(\\mu)\\)\n\\(\\Phi(\\eta)\\)\nBinomial\nFor masochists\n\n\n\n1.2 Why Can‚Äôt We Just Minimize Squared Errors?\nIn linear regression, we minimize the sum of squared residuals (SSR): \\[SSR = \\sum_{i}(Y_i - \\hat{Y}_i)^2 = \\sum_{i}(Y_i - \\textbf{X}\\beta)^2\\]\nThis has a nice closed-form solution and works because:\n\nThe errors are approximately normally distributed\nThe variance is constant (a.k.a. homoscedasticity)\nMinimizing SSR is equivalent to maximum likelihood estimation\n\nBut in GLMs, these assumptions break down:\n\nBinomial data has variance \\(\\mu(1-\\mu)\\), which depends on the mean\nPoisson data has variance equal to the mean \\(\\mu\\)\n\nThe response isn‚Äôt always in real number or approximately normally distributed (even with the presence of CLT)\n\nSo we need the alternative approach: Maximum Likelihood Estimation ‚Äî this is the alternative estimation method for Linear regression.\nIn GLM, it will be proven difficult to estimate the optimal solution as the use case will become different. You see the table above? There are actually plenty of them.\n\n1.3 The Three Sacred Components of GLM\nEvery GLM has exactly three components. Forget one at your dissertation defense, I dare you:\n1. Random Component (Distribution Family)\nYour response \\(Y\\) has a likelihood function, expressed in an exponential family distribution. The fancy math looks like: \\[f(Y; \\theta, \\phi) = \\exp\\left\\{\\frac{Y\\theta - b(\\theta)}{a(\\phi)} + c(Y, \\phi)\\right\\}\\]\nTranslation: Normal, Binomial, Poisson, Gamma ‚Äî they‚Äôre all in this family because they can be written in this form.\nWhy do we care? Because exponential family distributions have nice properties:\n\n\n\\(E[Y] = b'(\\theta)\\) (the mean is the derivative of that \\(b\\) function)\n\n\\(\\text{Var}(Y) = b''(\\theta) \\cdot a(\\phi)\\) (variance is also derived from \\(b\\))\n\n2. Systematic Component (Linear Predictor)\n\\[\\eta = \\textbf{X}\\beta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots\\]\nThis is just your boring linear combination of predictors. Nothing fancy. We‚Äôre still doing regression.\n3. Link Function\n\\[g(\\mu) = \\eta\\]\nConnects your linear predictor \\(\\eta\\) (which lives in \\((-\\infty, \\infty)\\)) to your mean response \\(\\mu\\) (which might live in \\((0, 1)\\) or \\([0, \\infty)\\) or wherever your response is allowed to be).\n\n1.4 Why Linear Regression is Just a Special Case\nNotice that if you:\n\nUse the Normal distribution (random component)\nUse the identity link \\(g(\\mu) = \\mu\\) (link function)\nKeep the linear predictor \\(\\eta = \\textbf{X}\\beta\\) (systematic component)\n\nYou get just it:\n\\[\\mu = \\textbf{X}\\beta\\]\nwhich itself is equivalent to \\(E(y)\\)\nWhich is just‚Ä¶your good old ordinary regression:\n\n\n\n\n\n\nIn R\n\n\n\n\n\nlm(y ~ x, data = data)\n\n\n\n\n\n\n\n\n\nIn Python\n\n\n\n\n\nimport statsmodels.formula.api as smf\n\nsmf. \\ \n    ols(\"y ~ x\", data = data). \\\n    fit(). \\ \n    summary()\n\n\n\nLinear regression is a GLM with no identity, literally and figuratively."
  },
  {
    "objectID": "posts/07-glm/index.html#stunning-part-in-depth-mathematics-and-computation",
    "href": "posts/07-glm/index.html#stunning-part-in-depth-mathematics-and-computation",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n2 Stunning Part: In-depth mathematics and computation",
    "text": "2 Stunning Part: In-depth mathematics and computation\nWhat mathematics do I need to know more?\nIt takes a lot to optimize the GLM\n\n2.1 Maximum Likelihood Estimation\nBut how do we actually estimate \\(\\beta\\)? We use maximum likelihood estimation (MLE).\nThe likelihood for a GLM is:\n\\[L(\\beta) = \\prod_{i=1}^n f(y_i; \\theta_i, \\phi)=\\prod_{i=1}^n\\exp\\left\\{\\frac{y_i\\theta_i - b(\\theta_i)}{a(\\phi)} + c(y_i, \\phi)\\right\\}\\]\nFancy, isn‚Äôt it? SIKE!! You don‚Äôt want this, trust me. This thing is so hard to calculate, even the computer starts to act crazy if you really direcly executed a program for this. Direct maximization of this product is computationally difficult. Taking logarithms simplifies things enormously (thanks, John Napier!).\nA sane person would only like what‚Äôs the easier to eat ‚Äî The log-likelihood function for the GLM instead:\n\\[\\ell(\\beta) = \\sum_{i=1}^n \\left\\{\\frac{y_i\\theta_i - b(\\theta_i)}{a(\\phi)} + c(y_i, \\phi)\\right\\}\\]\nNow, this is much easier to work with because:\n\nProducts become sums\nIt‚Äôs numerically more stable\nThe maximum is the same (logarithm is monotonic, after all)\n\n2.2 Fisher Scoring / IRLS Algorithm (The Actual Work)\nSince, there‚Äôs no actual closed form to calculate the coefficients, we can‚Äôt solve \\(U(\\beta) = 0\\) directly. There‚Äôs a method we can use called Iteratively Reweighted Least Squares (IRLS), which is just Newton-Raphson but with the Fisher Information Matrix.\nBefore starting, we need to assign a variable that arbitrarily assigns design matrix X, the response vector y, and then determines the number of predictors p, the number of observations n, the maximum iteration max_iter, and the tolerance eps.\n\nWe can use mtcars dataset as an example\n\n\n\nIn R\nIn Python\n\n\n\nn = nrow(X)\np = ncol(X)\nmax_iter = 100\neps = 1e-7\n\n\nn = X.shape[0]\np = X.shape[1]\nmax_iter = 100\neps = 1e-7  \n\n\n\nHere are the steps:\n\n2.2.1 1. The Algorithm:\n\nStart with initial values \\(\\beta^{(0)}\\) (usually from an unweighted regression or just zeros).\nThis is how you program it in R and Python:\n\n\nR\nPython\n\n\n\nbeta = matrix(0, nrow = p, ncol = 1)\n\n\nimport numpy as np\n\nbeta = np.zeros((p, 1))\n\n\n\n\n2.2.2 2. Repeat until convergence:\n\nIn this step, the extra steps, i.e.:\n\nCalculate linear predictor: \\(\\eta^{(t)} = \\textbf{X}\\beta^{(t)}\\)\nCalculate fitted values: \\(\\mu^{(t)} = g^{-1}(\\eta^{(t)})\\)\n\nCalculate weights:\n\\[w_i^{(t)} = \\frac{1}{\\text{Var}(Y_i)} \\left(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\right)^2\\]\n\n\nCalculate working response:\n\\[z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)}) \\frac{\\partial \\eta_i}{\\partial \\mu_i}\\]\n\n\nUpdate coefficients by solving weighted least squares:\n\\[\\beta^{(t+1)} = (\\textbf{X}^T W^{(t)} \\textbf{X})^{-1} \\textbf{X}^T W^{(t)} z^{(t)}\\]\n\nCheck convergence: If \\(|\\beta^{(t+1)} - \\beta^{(t)}| &lt; \\epsilon\\), stop. Else, go back to step 1.\n\n\n\nR\nPython\n\n\n\nfor (i in 1:max_iter) {\n    # 1. Calculate linear predictor\n    eta = X %*% beta\n    \n    # 2. Calculate fitted values\n    mu = family$linkinv(eta)\n    \n    # 3. Calculate weights\n    V = as.vector(family$variance(mu))\n    gradient = as.vector(family$mu.eta(eta))\n    w_vec = (gradient^2) / V\n    \n    # 4. Working response\n    z = as.vector(eta) + (as.vector(y) - mu) / gp\n    \n    # 5. Update coefficients by solving weighted least squares\n    W = diag(as.vector(w_vec), n, n)\n    beta_new = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% z\n    \n    # 6. Check convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n        beta = beta_new\n        converged = TRUE\n        break\n    }\n    \n    # Final beta\n    beta = beta_new\n}\n\n\nfor i in range(max_iter):\n    # 1. Calculate linear predictor\n    eta = X @ beta\n    \n    # 2. Calculate fitted values\n    mu = family.linkinv(eta)\n    \n    # 3. Calculate weights\n    V = family.variance(mu).flatten()\n    gradient = family.mu_eta(eta).flatten()\n    w_vec = (gradient ** 2) / V\n    \n    # 4. Working response\n    z = eta + (y - mu) / gp\n    \n    # 5. Update coefficients by solving weighted least squares\n    W = np.diag(w_vec)\n    beta_new = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ z)\n    \n    # 6. Check convergence\n    if np.max(np.abs(beta_new - beta)) &lt; tol:\n        beta = beta_new\n        converged = True\n        break\n    \n    # Final beta\n    beta = beta_new"
  },
  {
    "objectID": "posts/07-glm/index.html#math",
    "href": "posts/07-glm/index.html#math",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n2 In-depth mathematics and computation",
    "text": "2 In-depth mathematics and computation\n\nWhat mathematics do I need to know more?\nNow, you know it takes a lot to optimize the GLM. What are the things you need to know?\nFirst, in Linear regression, we have two ways to estimate the coefficients: MLE and ordinary least squares (OLS), which also can be derived and closed form from the maximum likelihood of the normal distribution. In GLM, we cannot use OLS, thus no closed form solution and MLE is the only main solution.\nThen for the extra part, which we talk about the deviance. Imagine this as the residual sum of squares in GLMs.\n\n2.1 Maximum Likelihood Estimation\nBut how do we actually estimate \\(\\beta\\)? We use maximum likelihood estimation (MLE), but considering there are many forms of likelihood functions, derived from several probability distributions came from exponential family, and so there‚Äôs several link functions to be derived.\nThe likelihood for a GLM is:\n\\[L(\\beta) = \\prod_{i=1}^n f(y_i; \\theta_i, \\phi)=\\prod_{i=1}^n\\exp\\left\\{\\frac{y_i\\theta_i - b(\\theta_i)}{a(\\phi)} + c(y_i, \\phi)\\right\\}\\]\nFancy, isn‚Äôt it?\nSIKE!!\n\nYou don‚Äôt want this, trust me. This thing is so hard to calculate, even the computer starts to act crazy if you really direcly executed a program for this. Direct maximization of this product is computationally difficult. Taking logarithms simplifies things enormously (thanks, John Napier!).\nA sane person would only like what‚Äôs the easier to eat ‚Äî The log-likelihood function for the GLM instead:\n\\[\\ell(\\beta) = \\sum_{i=1}^n \\left\\{\\frac{y_i\\theta_i - b(\\theta_i)}{a(\\phi)} + c(y_i, \\phi)\\right\\}\\]\nNow, this is much easier to work with because:\n\nProducts become sums\nIt‚Äôs numerically more stable\nThe maximum is the same (logarithm is monotonic, after all)\n\n2.2 Fisher Scoring / IRLS Algorithm (The Actual Work)\nSince, there‚Äôs no actual closed form to calculate the coefficients, we can‚Äôt solve \\(U(\\beta) = 0\\) directly. There‚Äôs a method we can use called Iteratively Reweighted Least Squares (IRLS), which is just Newton-Raphson but with the Fisher Information Matrix, denoted as \\(W\\).\nBefore starting, we need to assign a variable that arbitrarily assigns design matrix X, the response vector y, and then determines the number of predictors p, the number of observations n, the maximum iteration max_iter, and the tolerance eps.\n\n\nIn R\nIn Python\n\n\n\nn = nrow(X)\np = ncol(X)\nmax_iter = 100\neps = 1e-7\n\n\nn = X.shape[0]\np = X.shape[1]\nmax_iter = 100\neps = 1e-7  \n\n\n\nHere are the steps:\n\n2.2.1 1. The Algorithm:\n\nStart with initial values \\(\\beta^{(0)}\\) (usually from an unweighted regression or just zeros).\nThis is how you program it in R and Python:\n\n\nR\nPython\n\n\n\nbeta = matrix(0, nrow = p, ncol = 1)\n\n\nimport numpy as np\n\nbeta = np.zeros((p, 1))\n\n\n\n\n2.2.2 2. Repeat until convergence:\n\nIn this step, the extra steps, i.e.:\n\nCalculate linear predictor: \\(\\eta^{(t)} = \\mathbf{X}\\beta^{(t)}\\)\nCalculate fitted values: \\(\\mu^{(t)} = g^{-1}(\\eta^{(t)})\\)\n\nCalculate weights:\n\\[w_i^{(t)} = \\frac{1}{\\text{Var}(Y_i)} \\left(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\right)^2\\]\n\n\nCalculate working response:\n\\[z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)}) \\frac{\\partial \\eta_i}{\\partial \\mu_i}\\]\n\n\nUpdate coefficients by solving weighted least squares:\n\\[\\beta^{(t+1)} = (\\mathbf{X}^T W^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T W^{(t)} z^{(t)}\\]\n\nCheck convergence: If \\(|\\beta^{(t+1)} - \\beta^{(t)}| &lt; \\epsilon\\), stop. Else, go back to step 1.\n\n\n\nR\nPython\n\n\n\nfor (i in 1:max_iter) {\n    # 1. Calculate linear predictor\n    eta = X %*% beta\n    \n    # 2. Calculate fitted values\n    mu = family$linkinv(eta)\n    \n    # 3. Calculate weights\n    V = as.vector(family$variance(mu))\n    gradient = as.vector(family$mu.eta(eta))\n    w_vec = (gradient^2) / V\n    \n    # 4. Working response\n    z = as.vector(eta) + (as.vector(y) - mu) / gradient\n    \n    # 5. Update coefficients by solving weighted least squares\n    W = diag(as.vector(w_vec), n, n)\n    beta_new = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% z\n    \n    # 6. Check convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n        beta = beta_new\n        converged = TRUE\n        break\n    }\n    \n    # Final beta\n    beta = beta_new\n}\n\n\nfor i in range(max_iter):\n    # 1. Calculate linear predictor\n    eta = X @ beta\n    \n    # 2. Calculate fitted values\n    mu = family.linkinv(eta)\n    \n    # 3. Calculate weights\n    V = family.variance(mu).flatten()\n    gradient = family.mu_eta(eta).flatten()\n    w_vec = (gradient ** 2) / V\n    \n    # 4. Working response\n    z = eta + (y - mu) / gp\n    \n    # 5. Update coefficients by solving weighted least squares\n    W = np.diag(w_vec)\n    beta_new = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ z)\n    \n    # 6. Check convergence\n    if np.max(np.abs(beta_new - beta)) &lt; tol:\n        beta = beta_new\n        converged = True\n        break\n    \n    # Final beta\n    beta = beta_new\n\n\n\n\n2.3 Be deviant: Deviance (GLM‚Äôs Version of Sum of Squares)\nTo be frank, I am not planning to include this, but I was thinking if anyone is curious on how the fitness in GLMs is being measured. In Linear regression, we measure model fit by summarizing the residuals using Residual Sum of Squares (RSS). It is so odd for GLM to use other way to measure (well, the key term is ‚ÄúGeneralized‚Äù, after all) ‚Äî Instead, GLMs use deviance:\n\\[D = 2[\\ell(\\text{saturated model}) - \\ell(\\text{fitted model})]\\]\nThe saturated model has \\(\\hat{\\mu}_i = y_i\\) (perfect fit). Deviance measures how much worse your model is than this.\nFor specific families:\n\n\nNormal / Gaussian: \\(D = \\sum(y_i - \\hat{\\mu}_i)^2\\) (literally just RSS)\n\nPoisson: \\(D = 2\\sum\\left[y_i\\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) - (y_i - \\hat{\\mu}_i)\\right]\\)\n\n\nBinomial: \\(D = 2\\sum\\left[y_i\\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) + (n_i-y_i)\\log\\left(\\frac{n_i-y_i}{n_i-\\hat{\\mu}_i}\\right)\\right]\\)\n\n\nLower deviance = better fit."
  },
  {
    "objectID": "posts/07-glm/index.html#application-for-simpletons",
    "href": "posts/07-glm/index.html#application-for-simpletons",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n4 Application for simpletons",
    "text": "4 Application for simpletons\nOkay, enough theory ‚Äî show me the money!\nNow, we can derive a full function only from In-depth mathematics and computation. That is, if you only need the coefficients. Use any dataset you want, but on our example, we‚Äôll use the mtcars dataset for the simpletons.\nWhy mtcars? Because every R tutorial on earth has used it at least once, and I‚Äôm not about to break tradition, am I? We‚Äôll predict whether a car has an automatic (0) or manual (1) transmission (am) using horsepower (hp), and weight (wt). Classic binary outcome -&gt; logistic regression -&gt; GLM with binomial family and logit link.\nWe‚Äôll do it three ways so you can flex at parties:\n\nThe lazy way (stats::glm() in R / {statsmodels} in Python)\nThe ‚ÄúI swear I‚Äôm not crazy‚Äù way (calling our own IRLS function in this part)\n\n\n4.1 Tier 1: The lazy way (everyone does this)\nThis is the ‚Äúwhy should I bother making myself one if there‚Äôs one existed already‚Äù part. R has built-in glm() from {stats} package, while Python has {statsmodels} package, ideally under statsmodels.api, not under statsmodels.formula.api, but for our example, we can use that instead.\n\n\nR\nPython\n\n\n\n\nfit_r = glm(am ~ hp + wt, data = mtcars, family = binomial(link = \"logit\"))\nsummary(fit_r)\n\n\nCall:\nglm(formula = am ~ hp + wt, family = binomial(link = \"logit\"), \n    data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) 18.86630    7.44356   2.535  0.01126 * \nhp           0.03626    0.01773   2.044  0.04091 * \nwt          -8.08348    3.06868  -2.634  0.00843 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 10.059  on 29  degrees of freedom\nAIC: 16.059\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.genmod.families.links import Logit\nimport statsmodels.formula.api as smf\n\n# mtcars = sm.datasets.get_rdataset(\"mtcars\").data\nmtcars = r.mtcars\n\nfit_py = smf \\\n    .glm(\n        \"am ~ hp + wt\",\n        data=mtcars,\n        family=Binomial(link=Logit())\n    ) \\\n    .fit()\n\nprint(fit_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                     am   No. Observations:                   32\nModel:                            GLM   Df Residuals:                       29\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -5.0296\nDate:                Thu, 27 Nov 2025   Deviance:                       10.059\nTime:                        19:13:52   Pearson chi2:                     15.0\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.6453\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     18.8663      7.444      2.535      0.011       4.277      33.455\nhp             0.0363      0.018      2.044      0.041       0.001       0.071\nwt            -8.0835      3.069     -2.634      0.008     -14.098      -2.069\n==============================================================================\n\n\n\n\n\n\n4.2 Tier 2: The ‚ÄúI read the math section‚Äù way\n\n\nR\nPython\n\n\n\n\nbox::use(\n    ./glm[glm_custom]\n)\n\nglm_custom(am ~ hp + wt, data = mtcars, family = binomial())\n\n$coefficients\n[1] 18.8662987  0.0362556 -8.0834752\n\n$converged\n[1] TRUE\n\n$iterations\n[1] 9\n\n\n\n\n\nView the source code\n\n\nfrom glm import glm_custom\nimport glm\n\nprint(glm_custom('am ~ hp + wt', data = mtcars, family = glm.Binomial()))\n\n{'coefficients': {'Intercept': np.float64(18.866298717203943), 'hp': np.float64(0.03625559608221654), 'wt': np.float64(-8.083475182444579)}, 'converged': True, 'iterations': 9, 'beta_vector': array([[18.86629872],\n       [ 0.0362556 ],\n       [-8.08347518]])}"
  },
  {
    "objectID": "posts/07-glm/index.html#full-function-implementation-program",
    "href": "posts/07-glm/index.html#full-function-implementation-program",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n3 Full Function implementation program",
    "text": "3 Full Function implementation program\nThe main programming languages are still R and Python. The functions I‚Äôll implement below expects to be brittle and not applicable for some cases.\nThe functions are implemented in this source code."
  },
  {
    "objectID": "posts/07-glm/index.html#references",
    "href": "posts/07-glm/index.html#references",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n7 References",
    "text": "7 References\n\nMcCullagh & Nelder (1989) ‚Äì The Bible. Dense. Bring coffee and a therapist.\nDobson & Barnett ‚Äì Actually readable introduction\nFaraway ‚Äì Practical R examples, minimal pain\nNelder & Wedderburn (1972) ‚Äì The original paper. Short, brilliant, life-changing\nThe UCLA stats page on GLMs ‚Äì Still the best free resource in 2025\n\nAnd remember:\n\n‚ÄúAll models are wrong. But GLMs are wrong in the most elegant way possible.‚Äù\n\nNow go write some proper models, you beautiful statistician."
  },
  {
    "objectID": "posts/07-glm/index.html#lets-start-with-an-overview",
    "href": "posts/07-glm/index.html#lets-start-with-an-overview",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n1 Let‚Äôs start with an overview",
    "text": "1 Let‚Äôs start with an overview\n\nWhat did you observe on the mathematical model formula of linear regression and generalized linear model (GLM)?\nTake a look:\nLinear Regression:\n\\[Y = \\mathbf{X}\\beta\\ +\\ \\epsilon\\]\nwhere \\(\\mathbf{X}\\beta\\) expands to: \\[\\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_3\\beta_3 + \\dots + x_n\\beta_n\\]\nHow about on GLM?\n\\[\ng(\\mu_i) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta} = \\eta_i \\quad \\text{(linear predictor)}\n\\]\n\\[g(\\mu)=\\eta=\\mathbf{X}\\beta\\]\nboth are taking the account of the linear model, where the response have linear relationship with the predictors, taken account by \\(\\mathbf{X}\\beta\\).\nThere are few exceptions:\n\nGLMs can take handle of the range not only lies on the real number, but with some strict intervals, unlike Linear regression who has always a range lies on the real number.\n\nGLMs doesn‚Äôt have an error term. Why is that? GLMs takes considerations on the expected value of the linear model, and when you take the expected value like this:\n\\[E(Y)=E(\\mathbf{X}\\beta\\ + \\ \\epsilon)\\]\nThe expected values \\(E(\\mathbf{X}\\beta) = \\mathbf{X}\\beta\\) (assuming no randomness in both terms here) and \\(E(\\epsilon) = 0\\). Hence:\n\\[E(Y)=\\mathbf{X}\\beta\\]\nand GLMs only care about the expected value\n\n\n\n1.1 But wait, what‚Äôs this \\(g(\\mu)\\) doing here?\nYes, it‚Äôs odd, but hey, it just works.\nThere‚Äôs this thing called link function, and it‚Äôs the reason GLMs exist. Let me break down why we need this weird little function:\nIn linear regression, the mean of the model would be: \\[\\mu = E(Y) = \\mathbf{X}\\beta\\]\nOkay, this definitely feels like ‚ÄúHey, it works, okay?‚Äù, but that‚Äôs my intuition about GLMs. I hope you understood a little in this part. Anyways, that‚Äôs the mean of the linear regression that we know of, and this works great when \\(y\\) can be any real number. But, I am asking you a question:\nWhat if your outcome isn‚Äôt allowed to be just any real number?\n\nWhat if \\(Y\\) is a count \\((0, 1, 2, 3, ...)\\)? Let‚Äôs say number of kids in a family ‚Äî \\(\\mathbf{X}\\beta\\) could give you -3.7 customers\nWhat if \\(Y\\) is a probability (0 to 1)? Probability of rain? Between 0 and 1, inclusive, you monster\nWhat if \\(Y\\) is binary (0 or 1)? Let‚Äôs say ‚Äúwhether someone clicks your ad‚Äù ‚Äî \\(\\mathbf{X}\\beta\\) could give you 0.3, and it can‚Äôt be 1.337, you know.\n\nThis strange thing-y in GLMs, link function \\(g(\\cdot)\\), solves this by transforming the mean so that the linear predictor \\(\\eta = \\mathbf{X}\\beta\\) can be any real number, while \\(\\mu\\) stays in the valid range.\nUnderstand? No? Alright, let‚Äôs say, the output of \\(\\eta\\), which is an equivalent of \\(\\mathbf{X}\\beta\\), should be at least close to the value in \\(g(\\mu)\\), which is an equivalent of \\(g(E(y))\\).\nMathematically: \\[g(\\mu) = \\eta = \\mathbf{X}\\beta\\]\nSo the inverse is: \\[\\mu = g^{-1}(\\eta) = g^{-1}(\\mathbf{X}\\beta)\\]\nAgain, to understand, we fit the model in ‚Äútransformed space‚Äù where everything is linear, then transform back to get predictions in the correct range.\nList of common link functions:\n\n\n\n\n\n\n\n\n\nLink\n\\(g(\\mu)\\)\n\\(\\mu = g^{-1}(\\eta)\\)\nUsed for\nWhy\n\n\n\nIdentity\n\\(\\mu\\)\n\\(\\eta\\)\nNormal/Gaussian\nNo transformation needed\n\n\nLogit\n\\(\\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\)\n\\(\\frac{1}{1+e^{-\\eta}}\\)\nBinomial/Binary\nMaps \\((0,1) \\to (-\\infty, \\infty)\\)\n\n\n\nLog\n\\(\\log(\\mu)\\)\n\\(e^{\\eta}\\)\nPoisson/Counts\nMaps \\((0, \\infty) \\to (-\\infty, \\infty)\\)\n\n\n\nProbit\n\\(\\Phi^{-1}(\\mu)\\)\n\\(\\Phi(\\eta)\\)\nBinomial\nFor masochists\n\n\n\n1.2 Why Can‚Äôt We Just Minimize Squared Errors?\nIn linear regression, we minimize the sum of squared residuals (SSR): \\[SSR = \\sum_{i}(Y_i - \\hat{Y}_i)^2 = \\sum_{i}(Y_i - \\mathbf{X}\\beta)^2\\]\nThis has a nice closed-form solution and works because:\n\nThe errors are approximately normally distributed\nThe variance is constant (a.k.a. homoscedasticity)\nMinimizing SSR is equivalent to maximum likelihood estimation\n\nBut in GLMs, these assumptions break down:\n\nBinomial data has variance \\(\\mu(1-\\mu)\\), which depends on the mean\nPoisson data has variance equal to the mean \\(\\mu\\)\n\nThe response isn‚Äôt always in real number or approximately normally distributed (even with the presence of CLT)\n\nSo we need the alternative approach: Maximum Likelihood Estimation ‚Äî this is the alternative estimation method for Linear regression.\nIn GLM, it will be proven difficult to estimate the optimal solution as the use case will become different. You see the table above? There are actually plenty of them.\n\n1.3 The Three Sacred Components of GLM\nEvery GLM has exactly three components. Forget one at your dissertation defense, I dare you:\n\n\nRandom Component (Distribution Family)\nYour response \\(Y\\) has a likelihood function, expressed in an exponential family distribution. The fancy math looks like: \\[f(Y; \\theta, \\phi) = \\exp\\left\\{\\frac{Y\\theta - b(\\theta)}{a(\\phi)} + c(Y, \\phi)\\right\\}\\]\nTranslation: Normal, Binomial, Poisson, Gamma ‚Äî they‚Äôre all in this family because they can be written in this form.\nWhy do we care? Because exponential family distributions have nice properties:\n\n\n\\(E[Y] = b'(\\theta)\\) (the mean is the derivative of that \\(b\\) function)\n\n\\(\\text{Var}(Y) = b''(\\theta) \\cdot a(\\phi)\\) (variance is also derived from \\(b\\))\n\n\n\nSystematic Component (Linear Predictor)\n\\[\\eta = \\mathbf{X}\\beta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots\\]\nThis is just your boring linear combination of predictors. Nothing fancy. We‚Äôre still doing regression.\n\n\nLink Function\n\\[g(\\mu) = \\eta\\]\nConnects your linear predictor \\(\\eta\\) (which lives in \\((-\\infty, \\infty)\\)) to your mean response \\(\\mu\\) (which might live in \\((0, 1)\\) or \\([0, \\infty)\\) or wherever your response is allowed to be).\n\n\n1.4 Why Linear Regression is Just a Special Case\nNotice that if you:\n\nUse the Normal distribution (random component)\nIts identity link \\(g(\\mu) = \\mu\\) (link function)\nKeep the linear predictor \\(\\eta = \\mathbf{X}\\beta\\) (systematic component)\n\nYou get just it:\n\\[\\mu = \\mathbf{X}\\beta\\]\nwhich itself is equivalent to \\(E(y)\\)\nWhich is just‚Ä¶your good old ordinary regression:\n\n\n\n\n\n\nNoteIn R\n\n\n\n\n\nlm(y ~ x, data = data)\n\n\n\n\n\n\n\n\n\nNoteIn Python\n\n\n\n\n\nimport statsmodels.formula.api as smf\n\nsmf. \\ \n    ols(\"y ~ x\", data = data). \\\n    fit(). \\ \n    summary()\n\n\n\nLinear regression is a GLM with no identity, literally and figuratively."
  },
  {
    "objectID": "posts/07-glm/index.html#show-me-the-money-mtcars-logistic-regression-showdown",
    "href": "posts/07-glm/index.html#show-me-the-money-mtcars-logistic-regression-showdown",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n4 Show Me the Money: mtcars Logistic Regression Showdown",
    "text": "4 Show Me the Money: mtcars Logistic Regression Showdown\nOkay, enough theory ‚Äî show me the money!\nNow, we can implement a full function only from In-depth mathematics and computation. That is, if you only need the coefficients. Use any dataset you want, but on our example, we‚Äôll use the mtcars dataset for the simpletons.\nWhy mtcars? Because every R tutorial on earth has used it at least once, and I‚Äôm not about to break tradition, am I? We‚Äôll predict whether a car has an automatic (0) or manual (1) transmission (am) using horsepower (hp), and weight (wt). Classic binary outcome -&gt; logistic regression -&gt; GLM with binomial family and logit link.\nWe‚Äôll do it three ways so you can flex at parties:\n\nThe lazy way (stats::glm() in R / {statsmodels} in Python)\nThe ‚ÄúI Implemented IRLS at 3 AM‚Äù way (calling our own IRLS function in this part)\n\n\n4.1 Tier 1: The lazy way (everyone does this)\nThis is the ‚Äúwhy should I bother making myself one if there‚Äôs one existed already‚Äù part. R has built-in glm() from {stats} package, while Python has {statsmodels} package, ideally under statsmodels.api, not under statsmodels.formula.api, but for our example, we can use that instead.\n\n\nR\nPython\n\n\n\n\nfit_r = glm(am ~ hp + wt, data = mtcars, family = binomial(link = \"logit\"))\nsummary(fit_r)\n\n\nCall:\nglm(formula = am ~ hp + wt, family = binomial(link = \"logit\"), \n    data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) 18.86630    7.44356   2.535  0.01126 * \nhp           0.03626    0.01773   2.044  0.04091 * \nwt          -8.08348    3.06868  -2.634  0.00843 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 10.059  on 29  degrees of freedom\nAIC: 16.059\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.genmod.families.links import Logit\nimport statsmodels.formula.api as smf\n\n# mtcars = sm.datasets.get_rdataset(\"mtcars\").data\nmtcars = r.mtcars\n\nfit_py = smf \\\n    .glm(\n        \"am ~ hp + wt\",\n        data=mtcars,\n        family=Binomial(link=Logit())\n    ) \\\n    .fit()\n\nprint(fit_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                     am   No. Observations:                   32\nModel:                            GLM   Df Residuals:                       29\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -5.0296\nDate:                Tue, 02 Dec 2025   Deviance:                       10.059\nTime:                        13:25:43   Pearson chi2:                     15.0\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.6453\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     18.8663      7.444      2.535      0.011       4.277      33.455\nhp             0.0363      0.018      2.044      0.041       0.001       0.071\nwt            -8.0835      3.069     -2.634      0.008     -14.098      -2.069\n==============================================================================\n\n\n\n\n\n\n4.2 Tier 2: The ‚ÄúI Implemented IRLS at 3 AM‚Äù way\n\n\nR\nPython\n\n\n\n\nView the source code\n\n\nbox::use(\n    ./glm[glm_custom]\n)\n\nglm_custom(am ~ hp + wt, data = mtcars, family = binomial())\n\n$coefficients\n[1] 18.8662987  0.0362556 -8.0834752\n\n$converged\n[1] TRUE\n\n$iterations\n[1] 9\n\n\n\n\n\nView the source code\n\n\nfrom glm import glm_custom\nimport glm\n\nprint(glm_custom('am ~ hp + wt', data = mtcars, family = glm.Binomial()))\n\n{'coefficients': {'Intercept': np.float64(18.866298717203943), 'hp': np.float64(0.03625559608221654), 'wt': np.float64(-8.083475182444579)}, 'converged': True, 'iterations': 9, 'beta_vector': array([[18.86629872],\n       [ 0.0362556 ],\n       [-8.08347518]])}"
  },
  {
    "objectID": "posts/07-glm/index.html#when-things-go-wrong-trust-me-they-will",
    "href": "posts/07-glm/index.html#when-things-go-wrong-trust-me-they-will",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n5 When Things Go Wrong (Trust me, they Will)",
    "text": "5 When Things Go Wrong (Trust me, they Will)\n\n5.1 ‚ÄúMy model won‚Äôt converge!‚Äù\n\nCheck for complete separation in logistic regression\nTry scaling your predictors\nCheck for perfect multicollinearity\nOr just accept that your data is cursed\n\n5.2 ‚ÄúMy coefficients are huge!‚Äù\n\nYou forgot to scale your variables\nOr you have a separation issue\nOr your data really is that dramatic\n\n5.3 ‚ÄúThe deviance is gigantic!‚Äù\n\nMaybe your model actually sucks?\nOr you have outliers\nCheck if you‚Äôre using the right family\nConsider adding interaction terms or polynomial terms\n\n5.4 ‚ÄúI‚Äôm getting warnings about fitted probabilities of 0 or 1‚Äù\n\nThis is actually okay sometimes! It means you have very confident predictions\nBut if you‚Äôre getting tons of these, you might have separation issues\nCheck your data for extreme values"
  },
  {
    "objectID": "posts/07-glm/index.html#final-boss-wisdom",
    "href": "posts/07-glm/index.html#final-boss-wisdom",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n6 Final Boss Wisdom",
    "text": "6 Final Boss Wisdom\n\nLinear regression is a GLM that forgot it was a GLM\nThe link function is the reason you‚Äôre not predicting negative counts\nIRLS is just weighted least squares that updates itself until it stops crying\nDeviance is RSS for adults\n\nGo forth and model responsibly."
  },
  {
    "objectID": "posts/07-glm/index.html#appendix-cheat-sheet",
    "href": "posts/07-glm/index.html#appendix-cheat-sheet",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n8 Appendix ‚Äî Cheat Sheet",
    "text": "8 Appendix ‚Äî Cheat Sheet\nWhich GLM Should I Use?\n\n\n\n\n\n\n\n\n\nYour Y looks like‚Ä¶\nDistribution\nLink\nR Code\nPython Code\n\n\n\n0, 1, 2, 3‚Ä¶ (counts)\nPoisson\nlog\nfamily = poisson()\nsm.families.Poisson()\n\n\nYes/No (binary)\nBinomial\nlogit\nfamily = binomial()\nsm.families.Binomial()\n\n\n0.2, 0.7, 0.4‚Ä¶ (proportions)\nBinomial\nlogit\nfamily = binomial()\nsm.families.Binomial()\n\n\nAlways positive, right-skewed\nGamma\nlog\nfamily = Gamma()\nsm.families.Gamma()\n\n\nAny real number\nGaussian\nidentity\n\nfamily = gaussian() or lm()\n\nsm.families.Gaussian()\n\n\n\nOh wait, the overdispersion in count data? R pre-installs MASS anyways, so bother use glm.nb() yourself. \nPro tip: When in doubt, plot your residuals. If they look like a crime scene, you chose wrong."
  },
  {
    "objectID": "posts/07-glm/index.html#full-function-implementation",
    "href": "posts/07-glm/index.html#full-function-implementation",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n3 Full Function implementation",
    "text": "3 Full Function implementation\nThe main programming languages are still R and Python. The functions I implemented expects to be brittle and not applicable for some cases.\nThe functions are implemented in this source code."
  },
  {
    "objectID": "posts/07-glm/index.html#some-good-references",
    "href": "posts/07-glm/index.html#some-good-references",
    "title": "Tutorial on Generalized Linear Model from scratch",
    "section": "\n7 Some good references",
    "text": "7 Some good references\n\nMcCullagh & Nelder (1989) ‚Äì The Bible. Dense. Bring coffee and a therapist.\nDobson & Barnett ‚Äì Actually readable introduction\nFaraway ‚Äì Practical R examples, minimal pain\nNelder & Wedderburn (1972) ‚Äì The original paper. Short, brilliant, life-changing\nThe UCLA stats page on GLMs ‚Äì Still the best free resource in 2025\n\nAnd remember:\n\n‚ÄúAll models are wrong. But GLMs are wrong in the most elegant way possible.‚Äù\n\nNow go write some proper models, you beautiful statistician."
  },
  {
    "objectID": "posts/08-sql-r/index.html",
    "href": "posts/08-sql-r/index.html",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "",
    "text": "I am already using R since 2018, and uses SQL since 2022-2023. Way back in 2023, I am learning one of the most valuable feature in R, and that‚Äôs the ability to integrate R into other software. That‚Äôs because I only use softwares independently, i.e.¬†R only, Python only, etc. This is how I first learn SQL, and I learn few frameworks that integrates R and SQL databases.\nIf you‚Äôve spent any time in data science, I am sure you encountered language wars and such debates ‚Äî there‚Äôs like hundreds or maybe thousands of blogs spread in the community comparing which languages is better or worse. I am not talking about that in this blog post, however, here‚Äôs the thing ‚Äî it‚Äôs not really a versus situation. SQL and R are like peanut butter and jelly. Each is good on its own, but why not both? Flavorful.\n\nRegardless, SQL excels at what databases do best: storing, organizing, and retrieving massive amounts of data with lightning speed. R, on the other hand, shines where creativity and complexity matter: statistical modeling, advanced visualizations, and transforming raw data into insights that actually mean something.\nIn this post, I‚Äôll show you why combo-ing R and SQL isn‚Äôt just nice to have ‚Äî it‚Äôs my stack. And more importantly, I‚Äôll show you what I know how to make them work together so seamlessly you‚Äôll wonder how you ever worked any other way."
  },
  {
    "objectID": "posts/08-sql-r/index.html#introduction",
    "href": "posts/08-sql-r/index.html#introduction",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "1 Introduction",
    "text": "1 Introduction\nI am already using R since 2018, and uses SQL since around 2022-2023. Way back in 2023, I am learning one of the most valuable feature in R, and that‚Äôs the ability to integrate R into other software. That‚Äôs because I only use softwares independently, i.e.¬†R only, Python only, etc. This is how I first learn SQL, and I learn few frameworks that integrates R and SQL databases.\nIf you‚Äôve spent any time in data science, I am sure you encountered language wars and such debates ‚Äî there‚Äôs like hundreds or maybe thousands of blogs spread in the community comparing which languages is better or worse. I am not talking about that in this blog post, however, here‚Äôs the thing ‚Äî it‚Äôs not really a versus situation. SQL and R are like peanut butter and jelly. Each is good on its own, but why not both? Flavorful.\n\nRegardless, SQL excels at what databases do best: storing, organizing, and retrieving massive amounts of data with lightning speed. R, on the other hand, shines where creativity and complexity matter: statistical modeling, advanced visualizations, and transforming raw data into insights that actually mean something.\nIn this post, I‚Äôll show you why combo-ing R and SQL isn‚Äôt just nice to have ‚Äî it‚Äôs my stack. And more importantly, I‚Äôll show you what I know how to make them work together so seamlessly you‚Äôll wonder how you ever worked any other way."
  },
  {
    "objectID": "posts/08-sql-r/index.html#tools-and-packages",
    "href": "posts/08-sql-r/index.html#tools-and-packages",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "3 Tools and Packages",
    "text": "3 Tools and Packages\nI can name few tools and packages on working with SQL databases in R, most especially when you just started. I don‚Äôt have any database in my own device, but did you know you can simulate databases? These are the tools and packages to start:\n\n{tidyverse} ‚Äî Why this? This is a package that holds the complete set of tools in data science, and that includes working with databases. Speaking of which, this is a meta-package that also contains what we need: {dbplyr}, which also contains {DBI} package dependency.\n{box} ‚Äî I already talked about this package in my previous blog posts. Please, take a look at them if you have some time:\n\nBox: Placing module system into R\nIn my ‚ÄúWays to load / attach packages in R‚Äù blog post\n\n{dbplyr} ‚Äî This is the magic translator. It converts your familiar {dplyr} code into SQL queries behind the scenes. You write R, it speaks SQL to the database. The best part? You can inspect the SQL it generates, which makes it a fantastic learning tool.\n{DBI} ‚Äî Think of this as the universal adapter for database connections. It provides a consistent interface whether you‚Äôre connecting to SQLite, PostgreSQL, MySQL, or other databases. It handles the connection, sending queries, and fetching results.\n{RSQLite} ‚Äî This is the R interface to SQLite databases. SQLite is perfect for learning because it‚Äôs lightweight, requires no server setup, and the entire database is just a single file on your computer.\n\nInstall them through this:\n\nNative RUsing {pak}\n\n\ninstall.packages(c('tidyverse', 'box', 'RSQLite'))\n\n\nInstall them directly\npak::pak(c(\n    'tidyverse', \n    'box', \n    'RSQLite'\n))\nWhen you preferred the development version\npak::pak(c(\n    \"tidyverse/tidyverse\", \n    \"klmr/box\", \n    \"r-dbi/RSQLite\"\n))"
  },
  {
    "objectID": "posts/08-sql-r/index.html#with-existing-database",
    "href": "posts/08-sql-r/index.html#with-existing-database",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "4 With existing database",
    "text": "4 With existing database\nI learn SQL thanks to SQLite. This is a language-agnostic library, written in C, that acts like a database while being lightweight. You can use it literally everywhere! It is also used to built into everywhere, it could be mobile phones and most computers.\nThanks to SQLite, I made a first move to learn SQL without installing heavy softwares, such as PostgreSQL and MySQL, just to learn SQL. Additionally, SQLite is an open-source, but not open for contribution (I believe this is designed for good purpose).\n\n4.1 Why SQLite is Perfect for Learning\nHere‚Äôs why SQLite is the ideal training ground:\n\nNo server required ‚Äî It‚Äôs just a file on your computer\nZero configuration ‚Äî No ports, users, or permissions to set up\nLightweight ‚Äî Databases can be megabytes instead of gigabytes\nProduction-ready ‚Äî Despite being ‚Äúlite,‚Äù it‚Äôs used in production by major applications\nSQL standard ‚Äî You learn real SQL that transfers to other databases\n\nIn a positive sense, R and SQL is a great combo. Maybe R and SQL is not a great combo for software development as Python and SQL combo, R and SQL can make a place in data analysis instead. As long as you have {DBI} and {RSQLite} installed in your R, you can now make a first move on integrating R and SQL, and you‚Äôre good to go.\n\n\n4.2 SQLite in R\nOh, you can definitely learn SQL and R at the same time, considering that SQLite is portable and lightweight. The only primary requirements are {DBI} and {RSQLite}. If you know how to write a query, you don‚Äôt need a compatible set of packages in {tidyverse} and {dbplyr}, otherwise, as long as you know how to use {tidyverse} packages, namely {dplyr}, {tidyr}, etc., you can use it instead.\nLet me show you how to connect to a SQLite database and work with it:\n\nbox::use(\n    DBI[dbConnect, dbWriteTable, dbDisconnect], \n    RSQLite[SQLite]\n)\n\n1\ncon = dbConnect(SQLite(), \"first_database.sqlite\")\n\n2\ndf = data.frame(\n    id = 1:5,\n    name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"),\n    age = c(25, 30, 35, 28, 42),\n    city = c(\"New York\", \"London\", \"Tokyo\", \"Paris\", \"Sydney\")\n)\n\n3\ndbWriteTable(\n    con, \n    \"customers\",\n    df, \n    overwrite = TRUE\n)\n\n\n1\n\nCreate a connection (this creates the database file if it doesn‚Äôt exist)\n\n2\n\nCreate some sample data\n\n3\n\nWrite the data as a table to the database\n\n\n\n\n\n\n\n\n\n\nNoteExplanation\n\n\n\n\n\n\nCreate a connection (this creates the database file if it doesn‚Äôt exist)\nCreate some sample data\nWrite the data as a table to the database\n\n\n\n\nWhat just happened?\nWe created a SQLite database file called first_database.sqlite in your working directory. Inside it, we created a table called customers with our sample data. If the file already exists, R simply connects to it.\n\n\n4.3 Two Ways to Query: Pure SQL vs {dplyr}\nNow comes the fun part‚Äîyou can query this database in two different ways, and each has its benefits for learning.\n\nJust SQL{dplyr} API\n\n\nDid you know that with {knitr}, you can write SQL code chunks directly in your R Markdown or Quarto documents? This is incredibly handy for mixing SQL queries with your data analysis in R.\n```{sql connection=con}\nSELECT name, age, city \nFROM customers \nWHERE age &gt; 30\n```\nAlright, I wrote the SQL code chunk above again here for your reference:\n\nSELECT name, age, city \nFROM customers \nWHERE age &gt; 30\n\n\n2 records\n\n\nname\nage\ncity\n\n\n\n\nCharlie\n35\nTokyo\n\n\nEve\n42\nSydney\n\n\n\n\n\nOr you can store the query into a string, and send it via DBI::dbGetQuery(), placed in statement argument.\n\nDBI::dbGetQuery(\n    con, \n    \"SELECT name, age, city \\nFROM customers \\nWHERE age &gt; 30\"\n)\n\n     name age   city\n1 Charlie  35  Tokyo\n2     Eve  42 Sydney\n\n\nWhy this matters for learning:\nYou‚Äôre writing actual SQL. No training wheels. This builds muscle memory for SQL syntax and helps you think in terms of SQL operations: SELECT, FROM, WHERE, JOIN, GROUP BY, etc.\n\n\nNothing can make it so easy to work with databases using a familiar syntax with {dplyr}. Yes, with {dbplyr}, you can use {dplyr} functions to interact with your database tables as if they were regular data frames in R. Here‚Äôs how you can perform the same query using {dplyr}:\n\nbox::use(\n    dplyr[filter, select, tbl, collect, show_query]\n)\n\ncustomers_tbl = tbl(con, \"customers\")\n\nout = customers_tbl |&gt; \n    filter(age &gt; 30) |&gt; \n    select(name, age, city)\n\nout |&gt; collect()\n\n# A tibble: 2 √ó 3\n  name      age city  \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 Charlie    35 Tokyo \n2 Eve        42 Sydney\n\n\nThe magic behind this:\nWhen you use {dplyr} verbs on a database table, {dbplyr} doesn‚Äôt immediately execute anything. It‚Äôs lazy! It builds up the query and only executes it when you call collect(). This is efficient because:\n\nYou can chain many operations without multiple round-trips to the database\nOnly the final result set gets pulled into R memory\nThe heavy computation happens in the database where it‚Äôs optimized\n\nSee the generated SQL:\nHow about getting the SQL query generated by {dbplyr}? You can use the show_query() function to see the SQL that {dbplyr} generates for your {dplyr} code:\n\nout |&gt; \n    show_query()\n\n&lt;SQL&gt;\nSELECT `name`, `age`, `city`\nFROM `customers`\nWHERE (`age` &gt; 30.0)\n\n\nEasy, right? If you know R already, treat it as your SQL teacher! Write familiar {dplyr} code, then check the SQL translation. Over time, you‚Äôll start to intuitively understand how filter() becomes WHERE, how select() becomes SELECT, and how more complex operations translate to SQL.\n\n\n\n\n\n4.4 Working with Query Results\nOnce you have query results, you can treat them like any R data frame:\n\nbox::use(\n    ggplot2[ggplot, aes, geom_col, theme_minimal, labs]\n)\n\n1\nresult_data = collect(out)\n\n2\nresult_data |&gt;  \n    ggplot(aes(x = name, y = age, fill = city)) +\n    geom_col() +\n    theme_minimal() +\n    labs(\n        title = \"Customers Over 30\",\n        x = \"Name\",\n        y = \"Age\"\n    )\n\n\n1\n\nRetrieve the data\n\n2\n\nNow you can analyze it with R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExplanation\n\n\n\n\n\n\nRetrieve the data\nNow you can analyze it with R\n\n\n\n\nThis is where the R + SQL combination really shines. You use SQL‚Äôs efficiency to get exactly the data you need, then R‚Äôs rich ecosystem for analysis and visualization.\n\n\n4.5 Database Hygiene\nAlways remember to close your database connections once you‚Äôre done:\n\ndbDisconnect(con)\n\nThis releases resources and ensures your database file isn‚Äôt locked. In practice, connections are also closed automatically when your R session ends, but it‚Äôs good practice to do it explicitly."
  },
  {
    "objectID": "posts/08-sql-r/index.html#learning-sql-and-r-combo-without-a-server",
    "href": "posts/08-sql-r/index.html#learning-sql-and-r-combo-without-a-server",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "\n4 Learning SQL and R combo without a server",
    "text": "4 Learning SQL and R combo without a server\nBut I know some of you wants to know what it looks like to use the existing database and then call it in R. I literally said in the introduction that you can simulate ‚Äî I have another different meaning: use simulate_* family functions in dbplyr\nBut first, let me show you how it looks like to connect to a database, i.e.¬†SQLite in this case, and work with it:\n\nbox::use()"
  },
  {
    "objectID": "posts/08-sql-r/index.html#conclusion",
    "href": "posts/08-sql-r/index.html#conclusion",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nSQL and R aren‚Äôt competitors ‚Äî they‚Äôre collaborators. SQL is your data retrieval expert, getting you exactly the data you need with incredible efficiency. R is your analysis specialist, turning that data into insights, models, and visualizations. I can guarantee you that learning SQL through R is that you‚Äôre never starting from zero. Your existing R knowledge accelerates SQL mastery. The familiar {dplyr} syntax becomes your bridge to SQL fluency. And the ability to seamlessly move from database queries to statistical analysis to stunning visualizations‚Äîall in one environment‚Äîis genuinely powerful.\nThe data-driven professionals who thrive are those who can speak both languages fluently. They use SQL to ask databases the right questions and R to find the answers that matter. They understand when to leverage database optimization and when to bring data into R for complex transformations.\nThe journey from R user to R + SQL expert isn‚Äôt just about learning syntax‚Äîit‚Äôs about becoming someone who can efficiently bridge data storage and data science, who understands both the ‚Äúhow‚Äù of data retrieval and the ‚Äúwhy‚Äù of data analysis.\nSo don‚Äôt pick sides. Master both. Your future self (and would be your future employers) will thank you."
  },
  {
    "objectID": "posts/08-sql-r/index.html#learning-sql-in-r-without-a-server",
    "href": "posts/08-sql-r/index.html#learning-sql-in-r-without-a-server",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "5 Learning SQL in R without a server",
    "text": "5 Learning SQL in R without a server\nBut I know some of you wants to know what it looks like to use the existing database and then call it in R.\nI literally said in the introduction that you can simulate ‚Äî I have another different meaning:\n\nUse simulate_* family functions in {dbplyr} package. These functions allow you to create in-memory database tables that mimic real database behavior without needing an actual database server. This is perfect for learning and testing SQL queries in R.\n\n\n5.1 What Are Simulated Connections?\nSimulated connections create an in-memory representation of how different database systems handle SQL. This means you can:\n\nSee how your {dplyr} code translates to different SQL dialects\nLearn SQL without any database installation\nTest queries before running them on production databases\nUnderstand the quirks of different database systems\n\n\n\n5.2 Demonstration: Simulating Microsoft SQL Server\nFirst, let me show you how it looks like to connect to a database, i.e.¬†SQLite in this case, and work with it. Try imagine you have a SQL server, and you want to connect to it using R. Use simulate_mssql() function to simulate a Microsoft SQL Server database connection:\n\nbox::use(\n    dbplyr[simulate_mssql, tbl_lazy]\n)\n\n1con_sim = simulate_mssql()\n2customers_sim = tbl_lazy(df, con_sim)\n\n3customers_sim |&gt;\n    select(name, age, city) |&gt;\n    filter(age &gt; 30)\n\n\n1\n\nCreate a simulated MS SQL Server connection\n\n2\n\nCreate a (local) lazy table from our data frame\n\n3\n\nBuild a query\n\n\n\n\n&lt;SQL&gt;\nSELECT `name`, `age`, `city`\nFROM `df`\nWHERE (`age` &gt; 30.0)\n\n\n\n\n\n\n\n\nNoteExplanation\n\n\n\n\n\n\nCreate a simulated MS SQL Server connection\nCreate a (local) lazy table from our data frame\nBuild a query\n\n\n\n\nWe are performing a pure lazy evaluation in this step. Which means, we are not returning any information in the table and in the query process, we only generate the SQL code.\n\n\n5.3 Comparing SQL Dialects\nThe only issue when learning SQL with R in the past is when I found out that different databases have different SQL dialects. Let‚Äôs compare how PostgreSQL and Microsoft SQL Server handle the same query:\n\nLet‚Äôs import a bit first:\nbox::use(\n    dplyr[\n        keep_when = filter, arrange, desc, show_query, \n        summarise, mutate, relocate\n    ], \n    tidyr[\n        long = pivot_longer\n    ], \n    dbplyr[simulate_postgres, simulate_mssql], \n1    magrittr[`%&gt;%`]\n)\n\n\n1\n\nThis is totally optional, you still can use |&gt; base R pipes, after all.\n\n\n\n\n\n5.3.1 PostgreSQL version\n\ncon_postgres = simulate_postgres()\nmtcars_postgres = tbl_lazy(mtcars, con_postgres)\n\nmtcars_postgres %&gt;% \n    filter(cyl == 6) %&gt;% \n    mutate(\n        hp_per_cyl = hp / cyl,\n        efficiency = mpg / disp\n    ) %&gt;% \n    select(mpg, disp, hp, hp_per_cyl, efficiency, everything()) %&gt;% \n    summarise(\n        across(\n            c(mpg, hp_per_cyl, efficiency), \n            list(\n                mu = \\(x) mean(x, na.rm = TRUE), \n                sigma = \\(x) sd(x, na.rm = TRUE)\n            ), \n            .names = \"{.col}..{.fn}\"\n        ),\n        n = n()\n    ) %&gt;% \n    long(\n        cols = contains(c(\"mu\", \"sigma\")), \n        names_sep = \"\\\\..\", \n        names_to = c(\"Variable\", \"Stat\"), \n        values_to = \"Est\"\n    )\nClick to view generated SQL code\n\n&lt;SQL&gt;\nSELECT `n`, 'mpg' AS `Variable`, 'mu' AS `Stat`, `mpg..mu` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDDEV_SAMP(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDDEV_SAMP(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDDEV_SAMP(`efficiency`) AS `efficiency..sigma`,\n    COUNT(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'hp_per_cyl' AS `Variable`,\n  'mu' AS `Stat`,\n  `hp_per_cyl..mu` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDDEV_SAMP(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDDEV_SAMP(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDDEV_SAMP(`efficiency`) AS `efficiency..sigma`,\n    COUNT(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'efficiency' AS `Variable`,\n  'mu' AS `Stat`,\n  `efficiency..mu` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDDEV_SAMP(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDDEV_SAMP(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDDEV_SAMP(`efficiency`) AS `efficiency..sigma`,\n    COUNT(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT `n`, 'mpg' AS `Variable`, 'sigma' AS `Stat`, `mpg..sigma` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDDEV_SAMP(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDDEV_SAMP(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDDEV_SAMP(`efficiency`) AS `efficiency..sigma`,\n    COUNT(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'hp_per_cyl' AS `Variable`,\n  'sigma' AS `Stat`,\n  `hp_per_cyl..sigma` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDDEV_SAMP(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDDEV_SAMP(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDDEV_SAMP(`efficiency`) AS `efficiency..sigma`,\n    COUNT(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'efficiency' AS `Variable`,\n  'sigma' AS `Stat`,\n  `efficiency..sigma` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDDEV_SAMP(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDDEV_SAMP(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDDEV_SAMP(`efficiency`) AS `efficiency..sigma`,\n    COUNT(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\n\n\n\n\n\n5.3.2 Microsoft SQL Server version\n\ncon_mssql = simulate_mssql()\nmtcars_mssql = tbl_lazy(mtcars, con_mssql)\n\nmtcars_mssql %&gt;% \n    filter(cyl == 6) %&gt;% \n    mutate(\n        hp_per_cyl = hp / cyl,\n        efficiency = mpg / disp\n    ) %&gt;% \n    select(mpg, disp, hp, hp_per_cyl, efficiency, everything()) %&gt;% \n    summarise(\n        across(\n            c(mpg, hp_per_cyl, efficiency), \n            list(\n                mu = \\(x) mean(x, na.rm = TRUE), \n                sigma = \\(x) sd(x, na.rm = TRUE)\n            ), \n            .names = \"{.col}..{.fn}\"\n        ),\n        n = n()\n    ) %&gt;% \n    long(\n        cols = contains(c(\"mu\", \"sigma\")), \n        names_sep = \"\\\\..\", \n        names_to = c(\"Variable\", \"Stat\"), \n        values_to = \"Est\"\n    )\nClick to view generated SQL code\n\n&lt;SQL&gt;\nSELECT `n`, 'mpg' AS `Variable`, 'mu' AS `Stat`, `mpg..mu` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDEV(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDEV(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDEV(`efficiency`) AS `efficiency..sigma`,\n    COUNT_BIG(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'hp_per_cyl' AS `Variable`,\n  'mu' AS `Stat`,\n  `hp_per_cyl..mu` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDEV(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDEV(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDEV(`efficiency`) AS `efficiency..sigma`,\n    COUNT_BIG(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'efficiency' AS `Variable`,\n  'mu' AS `Stat`,\n  `efficiency..mu` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDEV(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDEV(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDEV(`efficiency`) AS `efficiency..sigma`,\n    COUNT_BIG(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT `n`, 'mpg' AS `Variable`, 'sigma' AS `Stat`, `mpg..sigma` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDEV(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDEV(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDEV(`efficiency`) AS `efficiency..sigma`,\n    COUNT_BIG(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'hp_per_cyl' AS `Variable`,\n  'sigma' AS `Stat`,\n  `hp_per_cyl..sigma` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDEV(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDEV(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDEV(`efficiency`) AS `efficiency..sigma`,\n    COUNT_BIG(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\nUNION ALL\n\nSELECT\n  `n`,\n  'efficiency' AS `Variable`,\n  'sigma' AS `Stat`,\n  `efficiency..sigma` AS `Est`\nFROM (\n  SELECT\n    AVG(`mpg`) AS `mpg..mu`,\n    STDEV(`mpg`) AS `mpg..sigma`,\n    AVG(`hp_per_cyl`) AS `hp_per_cyl..mu`,\n    STDEV(`hp_per_cyl`) AS `hp_per_cyl..sigma`,\n    AVG(`efficiency`) AS `efficiency..mu`,\n    STDEV(`efficiency`) AS `efficiency..sigma`,\n    COUNT_BIG(*) AS `n`\n  FROM (\n    SELECT\n      `mpg`,\n      `disp`,\n      `hp`,\n      `hp` / `cyl` AS `hp_per_cyl`,\n      `mpg` / `disp` AS `efficiency`,\n      `cyl`,\n      `drat`,\n      `wt`,\n      `qsec`,\n      `vs`,\n      `am`,\n      `gear`,\n      `carb`\n    FROM `df`\n    WHERE (`cyl` = 6.0)\n  ) AS `q01`\n) AS `q01`\n\n\n\n\n\n\n\n\n5.4 When to Use Simulated Connections\nSimulated connections are perfect for:\n\nLearning: Practice SQL translation without database setup\nDevelopment: Test query logic before connecting to real databases\nDocumentation: Show how queries work across different systems\nTeaching: Demonstrate SQL concepts without infrastructure requirements"
  },
  {
    "objectID": "posts/08-sql-r/index.html#why-learn-sql-through-r",
    "href": "posts/08-sql-r/index.html#why-learn-sql-through-r",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "2 Why Learn SQL Through R?",
    "text": "2 Why Learn SQL Through R?\nBefore we dive into the technical details, let me explain why R is actually a fantastic environment for learning SQL:\n\nImmediate Feedback Loop\nWhen you‚Äôre learning SQL in a traditional database environment, you often need to set up servers, configure connections, and deal with authentication. With R, you can start writing queries in seconds and see results immediately in your familiar R environment.\nBest of Both Worlds\nYou can write pure SQL when you want to practice, or use {dplyr} syntax and see the generated SQL. This dual approach accelerates learning because you can:\n\nWrite {dplyr} code and inspect the SQL it produces\nCompare your hand-written SQL with {dbplyr}‚Äôs output\nGradually transition from {dplyr} comfort to SQL mastery\n\nVisualization Integration\nThe moment you query data, you can pipe it directly into {ggplot2} or other R visualization tools. No export/import cycles, no switching between applications‚Äîjust seamless analysis.\nReproducible Workflows\nEverything lives in a script or R Markdown document. Your queries, analysis, and visualizations are all version-controlled and reproducible."
  },
  {
    "objectID": "posts/08-sql-r/index.html#common-pitfalls-and-how-to-avoid-them",
    "href": "posts/08-sql-r/index.html#common-pitfalls-and-how-to-avoid-them",
    "title": "Why SQL + R is an affable combo when I start learning SQL?",
    "section": "6 Common Pitfalls and How to Avoid Them",
    "text": "6 Common Pitfalls and How to Avoid Them\nSome are based on my experience, so some of the list can be opinionated.\n\n6.1 1. Collecting Too Early\nThe problem in R can be overdramatic with large amounts of data when read and processed into memory. Unnecessarily bringing entire tables into R memory creates performance bottlenecks and can even crash your R session when datasets exceed available RAM.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nChain operations and collect only when needed. Let the database do the heavy lifting with filtering, aggregating, and joining before bringing results into R.\n\n\n\n\n\n6.2 2. Not Checking Query Plans\nSlow queries are often the result of poor optimization, but many users never investigate why their queries take so long. Without examining the query plan, you‚Äôre flying blind‚Äîunable to identify bottlenecks, missing indexes, or inefficient joins.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nUse explain() to understand query execution. This reveals how the database processes your query and highlights optimization opportunities.\n\n\n\n\n\n6.3 3. Forgetting Database Differences\nSQL dialects vary between database systems, and code that runs perfectly in SQLite might fail spectacularly in PostgreSQL or MySQL. These differences range from subtle syntax variations to completely different function names and behaviors.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nTest with simulated connections and understand dialect differences. Use {dbplyr}‚Äôs simulate_*() functions to preview how queries translate across databases before deploying to production systems.\n\n\n\n\n\n6.4 4. Ignoring Indexes\nQueries that scan entire tables are a common performance killer, especially as datasets grow. Without proper indexes, your database must examine every single row to find matches, turning what should be millisecond queries into multi-second ordeals.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nLearn about indexes and how to create them for frequently queried columns. Understand which columns benefit from indexing and how composite indexes can optimize complex queries.\n\n\n\n\n\n6.5 5. Not Parameterizing Queries\nBuilding queries with string concatenation isn‚Äôt just inelegant‚Äîit‚Äôs dangerous. This practice opens your code to SQL injection attacks, where malicious input can execute arbitrary database commands, potentially exposing or destroying data.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nUse parameterized queries with DBI::dbBind() or stick with {dplyr} operations, which handle parameterization automatically. Never concatenate user input directly into SQL strings."
  }
]